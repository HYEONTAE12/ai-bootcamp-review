

---

# 📘 MLOps 엔지니어링 개요 (정리 1\~40p)

---

### 🚩 1. 강의 개요 & 목표

* **목표**: ML 프로젝트 전체 흐름 이해 + MLOps 필요성 학습
* **집중 포인트**

  * 프로젝트 특성 파악
  * 아키텍처 설계 및 서비스 선택
  * MLOps 기본 개념과 도구 이해
* **주의**: 모델 알고리즘 연구(X) → **엔지니어링 관점(O)**

---

### 👥 2. 데이터 직군 & 역할

* **데이터 과학자**: 모델 개발 중심 (예: 추천 모델 설계, 학습)
* **데이터 엔지니어**: 데이터 파이프라인 구축 (예: 데이터 적재·전처리 자동화)
* **ML 엔지니어**: 모델 배포·운영 담당 (예: API 서빙, 모니터링)
* **현실**: 역할이 구분되지만 협업이 중요. 스타트업은 한 사람이 여러 역할 겸하기도 함.

---

### 🤖 3. 머신러닝 기본 개념

* 머신러닝 모델 = 복잡한 함수 `f(x)`
* **규칙 기반**: 사람이 직접 `if-else`로 규칙 정의
* **ML 기반**: 데이터를 학습해 규칙 자동 추출
* **학습 종류**

  * 지도학습 (분류·회귀)
  * 비지도학습 (군집·차원축소)
  * 강화학습 (환경에서 보상 최대화 전략 학습)

---

### 🔄 4. 머신러닝 워크플로우

1. **비즈니스 문제 정의**
2. **데이터 탐색 및 전처리**
3. **모델 학습 및 튜닝**
4. **모델 검증 및 평가**
5. **모델 배포**
6. **모니터링 및 피드백**

👉 경진대회는 보통 4단계(평가)까지만, 실무는 6단계까지 반복되는 **순환 구조**.

---

### 🎯 5. 비즈니스 문제 정의

* **흔한 실수**: 데이터·목표 정의 없이 모델링부터 시작 → 산으로 감.
* **데이터 해석 주의**: WWII 전투기 탄흔 분석 → "탄흔 없는 부분이 치명타"라는 역발상 필요.
* **검토 사항**

  * ML이 꼭 필요한 문제인가?
  * 비용 대비 효과 있는가?
  * 규제, 설명가능성, 투명성 충족하는가?
* **실패 사례**

  * 스마트 팩토리: 데이터 관계 파악 미흡 → 실패
  * 차량 거동 모델: PoC 성공, 양산 실패 → 실제 환경 반영 부족

---

### ⚠️ 6. 비즈니스 목표 & 리스크

* **목표 수립 시 고려**

  * 성능 목표, 인프라 요구사항, 비용 제약, 설명가능성
* **대표 리스크**

  * 모델 다운타임 (서버 장애로 서비스 중단)
  * 특정 표본 잘못된 예측 (데이터 편향)
  * 시간에 따른 성능 저하 (데이터 드리프트, 예: LoL 메타 변화)
  * 유지보수 인력 이탈 (코드·파이프라인 관리 부실)
* **평가 방법**: 발생 확률 × 영향도 → **5×5 매트릭스**로 시각화

---

### 📌 7. 의도와 책무

* **실패 사례**: 이루다 챗봇(윤리 이슈), 아마존 채용 AI(편향 문제)
* **교훈**: 의도와 책무를 지켜야

  * 모델 품질 확보
  * 안정성 보장
  * 신뢰성 향상
  * 규제 준수 달성
* 👉 이는 곧 **비즈니스 성과**와 직결됨.

---

### 📊 8. 데이터 탐색 & 전처리

* **체크리스트**

  * 어떤 데이터셋을 사용할 수 있는가? (권한, 접근성)
  * 데이터는 충분히 정확·신뢰할 수 있는가?
  * 여러 소스를 결합 가능? (예: 게임 로그 + 결제 내역)
  * 수집 주기가 요구사항에 맞는가? (실시간 vs 주 단위)
  * 레이블링 필요? 리소스 충분?
  * 배포 후 데이터 갱신 계획은?
  * KPI 측정 방식은?
  * 법적·윤리적 문제 없는가? (PII, 소수 집단 고려 등)
* **EDA(탐색적 분석)**: 패턴, 분포, 이상치 확인
* **데이터 파이프라인 정의**: 수집 주기, 저장소, 장애 처리 방식

---

### 🛠️ 9. 모델 개발

* **데이터 과학자 역할**: 모델 실험·개발 집중
* **필요 요소**

  * Python, PyTorch, Pandas 등 환경
  * GPU/메모리 같은 컴퓨팅 자원
  * 실험 관리(여러 모델 성능 비교, 로그 기록)
* **엔지니어 지원**: 분산 학습, 자원 관리, 인터페이스 제공

---

### 📚 10. 모델 개발 도구 & 의존성

* **언어**: Python
* **플랫폼**: AWS SageMaker, Alteryx, DataRobot
* **프레임워크**: PyTorch, TensorFlow, Keras
* **의존성 관리**: pip, conda
* **코드 관리**: GitHub, GitLab, BitBucket

👉 도구 많아서 **춘추전국시대**라 불림.

---

### 📥 11. 모델 학습 입출력 요소

* **입력**:

  * 데이터셋 (train/valid/test)
  * 하이퍼파라미터 (학습률, batch size 등)
  * 수행 파라미터 (날짜, 태스크 옵션)
* **출력**:

  * 학습된 모델(가중치 파일)
  * 추론 결과 (예측값)
  * 성능 지표 (정확도, RMSE 등)
  * 로그

👉 이 모든 걸 잘 기록·저장해야 **재현성** 확보.

---

### ✅ 12. 데이터셋 검증

* **Train/Validation/Test** 필수 분할
* **K-Fold 교차 검증**: 데이터를 여러 번 나눠 학습·검증 반복 → 일반화 성능 확인

---

### 🤝 13. 모델 개발 협업

* 데이터 과학자 ↔ ML 엔지니어 ↔ 데이터 엔지니어 협력 구조
* 엔지니어도 ML 프레임워크와 기본 알고리즘은 이해해야 원활한 협업 가능

---

### 🚀 14. 모델 배포 개요

* **흐름**: 학습된 모델 전달 → 서비스 환경에 배포
* **문제**: 연구 환경 vs 운영 환경 차이 (OS, Python, 라이브러리, GPU 등)
* **결과**: 환경 불일치로 배포 실패 가능 → **일관된 실행 환경 필요**

---

### 📋 15. 배포 시 고려사항

* 모델 & 데이터 버전 관리
* 태스크 수행 파라미터 관리
* 서빙 방식 선택 (REST API, gRPC, On-device)
* 실행 시간 & 자원 요구사항 충족
* 확장성 & 탄력성
* 응답 지연(latency) 최소화
* 모니터링 & 로깅 체계 구축

---

### 📦 16. 컨테이너화 (Docker)

* **문제 해결**: 연구 환경 ≠ 배포 환경
* **방법**: Docker 컨테이너로 **환경 + 코드 + 모델**을 하나로 묶음
* **배포 과정**

  1. Dockerfile 작성 (환경 명시)
  2. Docker Image 빌드
  3. 이미지 저장소(Registry)에 Push (예: Docker Hub, AWS ECR)
  4. 서버에서 Pull → 컨테이너 실행
* **효과**: 어디서 실행하든 동일한 환경 보장

---

### 🔧 17. 워크플로우 관리 도구

* 모델 개발/배포 과정 = 여러 Task의 파이프라인
* 컨테이너 기반으로 실행 → 휘발성 문제 해결 위해 결과물은 반드시 외부 저장소에 저장
* **대표 도구**: Airflow, Kubeflow

---

### 🗄️ 18. 모델 저장소

* **결과 데이터**: S3, MinIO, MySQL, MongoDB
* **아티팩트(모델 파일)**: 오브젝트 스토리지(S3, MinIO)
* **모델 지표**: ELK, EFK 같은 모니터링 도구
* **수행 로그**: Airflow, MLflow, W\&B

---

✅ **핵심 요약**

* 문제 정의 → 목표/리스크/책무부터 시작해야 함
* 데이터 탐색 → 신뢰성·권한·주기·레이블링·법적 검토 필수
* 모델 개발 → 과학자 중심, 엔지니어 지원 필요
* 모델 배포 → 환경 차이 극복 위해 컨테이너화
* 워크플로우 & 저장소 → 자동화 + 기록 관리 필수

---

