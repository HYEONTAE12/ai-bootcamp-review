
---

# 📘 1\~80p 복습 문제 정리

---

### 🔹 문제 1

머신러닝 프로젝트의 기본 워크플로우 단계 6가지를 순서대로 말해보라.

**정답**
비즈니스 문제 정의 → 데이터 탐색 및 전처리 → 모델 학습 및 튜닝 → 모델 검증 및 평가 → 모델 배포 → 모니터링 및 피드백

**핵심포인트**

* “검증 및 평가” 단계가 자주 빠뜨리는 부분 → 실제 성능 확인에 반드시 필요

---

### 🔹 문제 2

데이터 탐색 단계에서 “이 데이터는 정확하고 신뢰할 수 있는가?”라는 질문을
LoL 유저 추천 모델에 적용하면 어떤 상황일까?

**정답**
→ 챔피언 승률, 성능 지표가 최신 패치 반영 없이 기록돼 있으면 신뢰할 수 없음.

**핵심포인트**

* 데이터 신뢰성 = 최신성 + 정확성
* 잘못된 데이터 → 모델이 잘못된 추천을 학습함

---

### 🔹 문제 3

리스크 평가에서 “시간이 지남에 따라 성능 저하” 리스크가 생기는 이유를 LoL 예시로 설명하라.

**정답**
→ 패치·메타 변화로 인해 과거 데이터를 반영한 모델이 성능을 잃는다.

**핵심포인트**

* **데이터 드리프트**: 시간 경과로 데이터 분포가 변함 → 모델 성능 저하

---

### 🔹 문제 4

MLOps 레벨 1에서 **자동화된 부분**과 **여전히 수동인 부분**은?

**정답**

* 자동화: 데이터 준비, 학습, 검증, 재학습 트리거
* 수동: 모델 배포, 사용자 요청 처리(API 서빙)

**핵심포인트**

* 레벨 1 = “연구/실험 자동화”에 집중, 운영은 수동

---

### 🔹 문제 5

만약 LoL 추천 모델 성능이 떨어졌다면, 레벨 1에서는 어떻게 재학습이 일어날까?

**정답**
→ 성능이 일정 기준 이하일 때 트리거 발생 → 파이프라인 실행 → Feature Store에서 최신 데이터 받아 모델 재학습

**핵심포인트**

* **트리거 기반 자동화** = 레벨 1의 장점
* 단, 배포까지 자동은 아님

---

### 🔹 문제 6

컨테이너화(Docker)가 중요한 이유 2가지는?

**정답**

1. 환경 일관성 보장 (로컬=서버=클라우드 어디서든 동일 실행)
2. 모델+코드+환경을 묶어서 이식성 확보

**핵심포인트**

* 연구 환경 vs 운영 환경 불일치를 해결하는 핵심 기술

---

### 🔹 문제 7 (심화)

게임 유저 이탈 예측 모델에서 “레이블링이 필요한가?”를 적용하면, 레이블은 어떻게 정의할 수 있을까?

**정답**
→ 예: “30일 동안 접속하지 않으면 이탈”

**핵심포인트**

* **라벨 정의 = 문제 정의의 핵심**
* 기준이 명확해야 학습 가능

---

### 🔹 문제 8 (심화)

유지보수 리스크를 줄이려면 어떤 준비가 필요할까?

**정답**

* 코드 형상 관리(Git)
* 문서화 (README, 파이프라인 설명)
* 모델·실험 관리 도구(MLflow, W\&B)

**핵심포인트**

* 유지보수 리스크 = “사람이 바뀌면 망하는 상황”
* **재현성과 협업 가능성** 확보가 해답

---

### 🔹 문제 9 (심화)

레벨 0 → 레벨 1로 넘어갈 때 팀이 얻는 가장 큰 이점은?

**정답**
→ 새로운 데이터가 들어올 때 사람이 직접 재학습·재배포 안 해도 되므로 시간 절약 + 효율성 상승

**핵심포인트**

* **자동화된 파이프라인** = 반복 작업 감소 + 재현성 확보

---

### 🔹 문제 10 (심화)

컨테이너화를 하지 않고 모델을 배포하면 어떤 문제가 발생할까?

**정답**
→ 로컬에서는 잘 되는데 서버에서는 라이브러리·Python 버전 충돌로 실행 불가.
→ 실행 환경이 다르면 결과가 달라져 재현성 없음.

**핵심포인트**

* 컨테이너화 없으면 **“It works on my machine”** 문제 발생

---

