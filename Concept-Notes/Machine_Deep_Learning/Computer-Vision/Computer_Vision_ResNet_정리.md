
---

# 🧠 ResNet(Residual Network) 정리

## 🔷 1. ResNet이란?

* **Residual Network**의 줄임말로, 매우 깊은 신경망을 안정적으로 학습하기 위해 개발된 구조.
* **2015년 He et al.**에 의해 제안된 모델.
* 기존 CNN은 층이 깊어질수록 **기울기 소실(Vanishing Gradient)** 문제가 생겨 학습이 어려웠음 → 이를 해결!

---

## 🔷 2. 핵심 개념: Residual Block

### 💡 Residual Block 구조

```
입력 x → Conv → ReLU → Conv → 출력
        ↘ (skip connection) ↗
```

* **스킵 커넥션(Skip Connection)** 또는 **숏컷 연결(Shortcut Connection)**이라고 불리는 구조 포함
* 입력을 그대로 출력에 더하는 방식:
  `y = F(x) + x`
  여기서 F(x)는 블록 내부의 합성곱 연산 결과

### ✅ 장점

* 기울기 소실 문제 완화
* 정보 손실 없이 원래 입력을 그대로 다음 층에 전달 가능
* 더 깊은 신경망 구성 가능 (ResNet-18, 34, 50, 101, 152 등)

---

## 🔷 3. Residual Block vs Plain Block

| 항목     | Residual Block         | Plain Block       |
| ------ | ---------------------- | ----------------- |
| 연결 방식  | 스킵 커넥션 포함 (`F(x) + x`) | 단순히 레이어만 순차적으로 연결 |
| 학습 안정성 | 깊은 네트워크에서도 학습이 잘 됨     | 깊어질수록 학습 어려움      |
| 대표 모델  | ResNet 시리즈             | 기존 VGG, AlexNet 등 |

---

## 🔷 4. 1x1 Convolution의 역할

### 🔸 정의:

* 커널 크기가 1x1인 합성곱 연산
* 공간 정보 변화 없이 **채널 간 연산**만 수행

### 🔸 용도:

* **채널 축소 (차원 축소)** → 연산량/파라미터 수 감소
* **비선형 조합 학습** → 복잡한 표현 가능

### 🔸 예시:

* 입력: `[B, 64, H, W]`
* 1x1 Conv → 출력: `[B, 32, H, W]` (채널 수 절반으로 감소)

---

## 🔷 5. 깊이에 따른 ResNet 모델 종류

| 모델 이름      | 레이어 수 | 사용 예시              |
| ---------- | ----- | ------------------ |
| ResNet-18  | 18    | 작은/중간 크기 데이터셋      |
| ResNet-34  | 34    | 일반적인 용도            |
| ResNet-50  | 50    | 대규모 데이터셋, ImageNet |
| ResNet-101 | 101   | 매우 복잡한 태스크         |
| ResNet-152 | 152   | 초대형 모델, 고성능 필요 시   |

* 깊어질수록 **표현력 증가**, 하지만 **계산량 증가** 및 **오버피팅 가능성 증가**

---

## 🔷 6. 추가로 함께 언급된 개념들

### 📌 Batch Normalization (BN)

* 각 층의 출력 분포를 정규화하여 학습 안정화
* ResNet에서도 깊은 네트워크 학습을 위해 필수적으로 사용

### 📌 학습률 스케줄링 전략

* **Cosine Annealing**, **StepLR** 등이 깊은 네트워크 학습 시 주로 사용됨

### 📌 옵티마이저

* Adam, SGD with momentum 등

---

## 🧩 주요 문제 예시 (복습용)

1. ResNet의 스킵 커넥션이 왜 중요한가?
2. Residual Block과 Plain Block의 차이는?
3. 1x1 컨볼루션은 왜 사용되는가?
4. ResNet-50과 ResNet-152는 어떤 차이가 있는가?
5. ResNet의 학습에 도움이 되는 옵티마이저 및 정규화 기법은?

---