
---

# 🧠 VGGNet vs AlexNet 퀴즈 개념 정리

## ✅ 1. AlexNet의 주요 특징

| 항목         | 설명                                                                  |
| ---------- | ------------------------------------------------------------------- |
| 🔸 발표 시기   | 2012년 ImageNet 대회 우승 모델                                             |
| 🔸 주요 기여   | CNN 기반 이미지 분류 성능의 비약적 향상                                            |
| 🔸 사용한 정규화 | **LRN (Local Response Normalization)**<br>→ 뉴런 간 경쟁을 유도하여 일반화 성능 향상 |
| 🔸 활성화 함수  | **ReLU (Rectified Linear Unit)**<br>→ 시그모이드보다 빠른 수렴, 기울기 소실 문제 완화   |
| 🔸 과적합 방지  | **Dropout** 사용 (Fully Connected layer에서)<br>→ 무작위로 일부 뉴런 비활성화       |
| 🔸 데이터 증강  | 이미지 회전, 좌우 반전, 크롭, 색상 변화 등                                          |
| 🔸 GPU 활용  | 당시 NVIDIA GTX 580 두 개를 사용해 병렬 학습                                    |
| 🔸 풀링 기법   | **Max Pooling**<br>→ 중요한 특징 유지하면서 크기 축소                             |

---

## ✅ 2. VGGNet의 주요 특징

| 항목                       | 설명                                                            |
| ------------------------ | ------------------------------------------------------------- |
| 🔸 발표 시기                 | 2014년 (Oxford - Visual Geometry Group)                        |
| 🔸 주요 구조                 | 동일한 크기의 필터 (3×3) 반복 사용                                        |
| 🔸 네트워크 깊이               | **16층 또는 19층** 구조 (VGG-16, VGG-19)                            |
| 🔸 커널 크기                 | **3x3 Convolution 필터**만 사용<br>→ 작은 필터 여러 개를 쌓아 더 깊고 정교한 특징 추출 |
| 🔸 활성화 함수                | **ReLU**                                                      |
| 🔸 구조적 특징                | 단순하고 일관적인 구조<br>→ 구현과 확장이 쉬움                                  |
| 🔸 Fully Connected Layer | 마지막에 2~3개의 FC Layer로 구성                                       |
| 🔸 파라미터 수                | 매우 많음 (메모리와 계산 비용 큼)                                          |

---

## ✅ 3. AlexNet vs VGGNet 비교 요약

| 비교 항목  | AlexNet           | VGGNet           |
| ------ | ----------------- | ---------------- |
| 발표 연도  | 2012              | 2014             |
| 커널 크기  | 다양 (11x11, 5x5 등) | 3x3 고정           |
| 층의 깊이  | 8층                | 16 or 19층        |
| 활성화 함수 | ReLU              | ReLU             |
| 정규화    | LRN 사용            | 사용하지 않음          |
| 과적합 방지 | Dropout 사용        | Dropout 사용       |
| 구조적 특징 | 다양한 필터, 복잡한 구조    | 반복적, 단순하고 일관된 구조 |
| GPU 사용 | Yes (병렬 학습)       | Yes              |

---


---

## ✅ 4. 주요 퀴즈 예시 및 정답 (보충 설명 포함)

| ❓ 질문                                   | ✅ 정답                                   | 💡 보충 설명                                                                                                                      |
| -------------------------------------- | -------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| **Q1. AlexNet에서 사용된 정규화 기법은?**         | **Local Response Normalization (LRN)** | LRN은 신경망이 특정 뉴런의 출력을 국소적으로 정규화하여 **뉴런 간 경쟁을 유도**하고, **과적합을 방지**합니다. 이는 일반화 성능을 높이는 데 도움을 줍니다.                                 |
| **Q2. AlexNet이 ReLU를 선택한 이유는?**        | **비선형성 강화 + 빠른 수렴**                    | 기존의 Sigmoid, Tanh 함수는 입력값이 크면 **기울기가 0에 수렴하는 vanishing gradient 문제**가 있었습니다. ReLU는 이 문제를 피하면서도 계산이 단순해 **학습 속도를 높여줍니다**.      |
| **Q3. VGG에서 사용된 합성곱 커널 크기는?**          | **3x3**                                | 3x3 커널을 여러 번 반복적으로 쌓으면, **넓은 수용 영역(receptive field)**을 확보하면서도 **파라미터 수를 줄일 수 있어** 효율적입니다. 예: 3x3 conv 2번 → 5x5와 유사한 수용 범위     |
| **Q4. VGG가 층이 더 깊은데도 단순한 이유는?**        | **동일한 구조 반복 사용**                       | VGG는 3x3 conv → ReLU → 3x3 conv → ReLU → max pooling 같은 **패턴을 반복**합니다. 다양한 커널을 섞은 AlexNet보다 **구조가 예측 가능하고 일관적**이라 단순하게 느껴집니다. |
| **Q5. Dropout의 역할은?**                  | **과적합 방지**                             | 학습 중 일부 뉴런의 출력을 0으로 만들면서, **특정 뉴런에 의존하는 현상을 줄이고**, 더 **일반화된 표현**을 학습하도록 유도합니다.                                                |
| **Q6. 맥스 풀링(Max Pooling)의 목적은?**       | **중요 특징 유지 + 차원 축소**                   | 필터 내 가장 강한 반응(max)을 선택함으로써 **위치 불변성과 잡음 제거**, 계산량 감소, 과적합 방지 등에 기여합니다.                                                        |
| **Q7. GPU 사용의 장점은?**                   | **대규모 데이터의 빠른 연산 처리**                  | CNN은 수많은 행렬 곱셈을 수행하므로, 병렬 연산에 강한 GPU를 활용하면 **훈련 속도가 수십 배 이상 빨라질 수 있습니다.**                                                     |
| **Q8. 필터 수가 많아질수록 출력 채널 수가 많아지는 이유는?** | **각 필터가 다른 특징을 학습하기 때문**               | 예: 3채널 RGB 이미지에 96개의 필터를 적용하면, 각 필터는 입력 이미지에서 다른 특징을 학습하여 **96개의 출력 채널(Feature Map)**을 생성하게 됩니다. 필터 수가 곧 출력 채널 수입니다.          |

---

### 📌 추가 참고: VGG의 구조가 단순하게 느껴지는 이유

* **반복 구조**:
  `Conv (3x3) → ReLU → Conv (3x3) → ReLU → MaxPool` 이 **블록 구조**로 반복됨

* **레이어 깊이는 증가**하지만,

  * 커널 크기, 패딩, 스트라이드 등이 **고정되어 있어** 파악하기 쉬움
  * 복잡한 커널 구성(11x11, 5x5 등)을 섞은 AlexNet보다 **시각적으로도 깔끔함**

* **비유**:

  * VGG = 정갈하게 층을 반복한 **단정한 건물**
  * AlexNet = 다양한 방과 복도를 가진 **복잡한 구조물**

---


