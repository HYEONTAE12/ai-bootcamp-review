
---

# ğŸš€ Transformer Encoder â€” Shape ì¤‘ì‹¬ ì •ë¦¬ (í‰ìƒ ì°¸ê³ ìš©)

---

# ğŸ”· 0. ê¸°ë³¸ ì¶• ì˜ë¯¸

```
B = batch_size       (ë¬¸ì¥/ì´ë¯¸ì§€ ë¬¶ìŒ ê°œìˆ˜)
S = seq_len          (í† í°/íŒ¨ì¹˜ ê°œìˆ˜)
D = d_model          (í† í° ë²¡í„° ê¸¸ì´)
H = num_heads        (ë©€í‹°í—¤ë“œ ê°œìˆ˜)
d = head_dim = D/H   (ê° headê°€ ë³´ëŠ” ì°¨ì›)
```

---

# ğŸ”· 1. ì…ë ¥ ë‹¨ê³„ Shape

### âœ” í† í°í™”

```
seq_len = S
```

### âœ” ì„ë² ë”©

```
x: (B, S, D)
```

### âœ” í¬ì§€ì…”ë„ ì¸ì½”ë”© ì¶”ê°€

```
(B, S, D)
```

---

# ğŸ”· 2. Multi-Head Self-Attention Shape íë¦„

ì•„ë˜ëŠ” **ì •í™•í•œ shape ë³€í™” ê³¼ì •**ì´ë‹¤.

---

## âœ” (1) Q, K, V ìƒì„±

ì…ë ¥ xì— ëŒ€í•´:

```
Q = xW_Q
K = xW_K
V = xW_V

Q, K, V shape:
(B, S, D)
```

---

## âœ” (2) headë¡œ ë¶„ë¦¬ (reshape + transpose)

```
(B, S, D)
â†’ reshape
(B, S, H, d)
â†’ transpose
(B, H, S, d)
```

ì´ì œ headë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ Attention ê°€ëŠ¥.

---

## âœ” (3) Self-Attention (headë³„)

Attention ê²°ê³¼:

```
(B, H, S, d)
```

ê° headëŠ” **ìê¸° ê´€ì **ì—ì„œ ë¬¸ì¥ì„ ì²˜ë¦¬.

---

## âœ” (4) head concat (ì›ë˜ ì°¨ì› ë³µêµ¬)

```
(B, H, S, d)
â†’ transpose
(B, S, H, d)
â†’ reshape
(B, S, D)
```

headë“¤ì„ ì±„ë„ ë°©í–¥ìœ¼ë¡œ í•©ì¹¨.

---

## âœ” (5) ìµœì¢… ì„ í˜• ë³€í™˜ W_O

```
(B, S, D)
```

MHA ë¸”ë¡ ìµœì¢… output.

---

## âœ” (6) Residual + LayerNorm

```
(B, S, D)
```

ì…ë ¥ê³¼ ì™„ì „íˆ ë™ì¼í•œ shape ìœ ì§€.

---

# ğŸ”· 3. FeedForward Network(FFN) Shape íë¦„

ì…ë ¥ì€ MHA ë¸”ë¡ì˜ ì¶œë ¥ x1:

```
x1: (B, S, D)
```

---

## âœ” (1) 1ì°¨ ì„ í˜• ë³€í™˜ (í™•ì¥)

```
(B, S, D)
â†’ (B, S, d_ff)   # d_ff = 4Dê°€ ì¼ë°˜ì 
```

---

## âœ” (2) GELU/ReLU

```
(B, S, d_ff)
```

---

## âœ” (3) 2ì°¨ ì„ í˜• ë³€í™˜ (ì¶•ì†Œ)

```
(B, S, d_ff)
â†’ (B, S, D)
```

---

## âœ” (4) Residual + LayerNorm

```
(B, S, D)
```

FFN ì¶œë ¥ë„ í•­ìƒ ì…ë ¥ê³¼ ë™ì¼.

---

# ğŸ”· 4. ì¸ì½”ë” ì „ì²´ ìŠ¤íƒ (N ë ˆì´ì–´)

ë ˆì´ì–´ë¥¼ ì—¬ëŸ¬ ê°œ ìŒ“ìœ¼ë©´:

```
Layer 0 ì…ë ¥:        (B, S, D)
â†“
Layer 1 ì¶œë ¥:        (B, S, D)
â†“
Layer 2 ì¶œë ¥:        (B, S, D)
â†“
...
â†“
Layer N ì¶œë ¥:        (B, S, D)
```

í•­ìƒ shape ë³€í™” ì—†ìŒ.

---

# ğŸ”· 5. ì „ì²´ Shape íë¦„ â€” í•œëˆˆì— ìš”ì•½

```
ì…ë ¥ x:                      (B, S, D)
â†“
Q,K,V:                       (B, S, D)
â†“ reshape
Q,K,V:                       (B, H, S, d)
â†“ Attention per head
Attn:                        (B, H, S, d)
â†“ concat
Concat:                      (B, S, D)
â†“ W_O
MHA output:                  (B, S, D)
â†“ Residual + Norm
x1:                          (B, S, D)
â†“ FFN expand
(B, S, d_ff)
â†“ activation
(B, S, d_ff)
â†“ FFN shrink
(B, S, D)
â†“ Residual + Norm
x2 (ë ˆì´ì–´ ì¶œë ¥):            (B, S, D)
```

---

# ğŸ”· 6. 10ì´ˆ ë§Œì— ë³µìŠµ ê°€ëŠ¥í•œ ì ˆëŒ€ í•µì‹¬

* **Q/K/V**: (B, S, D)
* **split heads**: (B, H, S, d)
* **attention**: (B, H, S, d)
* **concat**: (B, S, D)
* **FFN**: (B, S, D) â†’ (B, S, d_ff) â†’ (B, S, D)
* **ëª¨ë“  ë ˆì´ì–´ ì…ë ¥ê³¼ ì¶œë ¥ shape ë™ì¼**: (B, S, D)

---

