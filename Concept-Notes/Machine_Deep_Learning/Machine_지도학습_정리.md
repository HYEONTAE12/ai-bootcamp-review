
--

# 📘 지도학습(Supervised Learning) 정리

## 1. 회귀 모델 (Regression Model)

* **정의**: 종속변수(y)가 **연속형 변수**일 때 사용하는 모델
* **목표**: 입력값이 주어졌을 때, 출력값(연속값)을 예측
* **예시**

  * 부모 키 → 자녀 키
  * 오늘 기온 → 내일 기온

### 선형회귀 (Linear Regression)

* 데이터 점들에 직선을 맞추는 방법
* **직선 방정식**:

  $$
  y = wx + b
  $$

  * w: 기울기(coefficient, weight)
  * b: 절편(intercept, bias)
* 목표: 직선과 데이터 점들 간의 **오차**를 최소화

### 손실함수 (Loss Function)

* **MSE (Mean Squared Error, 평균제곱오차)**

  $$
  MSE = \frac{1}{n}\sum (y - \hat{y})^2
  $$
* 실제값 y와 예측값 ŷ의 차이를 제곱해서 평균낸 값
* 값이 작을수록 모델이 데이터를 잘 설명함

### 최적화 관점

* MSE 손실함수는 **convex 함수(볼록 함수)** → 최소값이 유일하게 존재
* 선형회귀 = “손실함수를 최소화하는 w, b를 찾는 문제” = **컨벡스 최적화 문제**

### 해법

* **해석적 해법 (Analytical Solution)**

  * 정규방정식:

    $$
    w = (X^TX)^{-1}X^Ty
    $$
  * 역행렬을 이용해 w를 직접 계산
* **수치적 해법 (Numerical Solution)**

  * 경사하강법(Gradient Descent)으로 반복적으로 w 업데이트

---

## 2. 상관관계 vs 인과관계

* **상관관계 (Correlation)**

  * 한 변수가 변할 때 다른 변수가 함께 변하는 경향성
  * 하지만 원인-결과 관계는 아님
  * 예: 아이스크림 판매량 ↑ ↔ 물놀이 인명사고 ↑ (공통원인=더운 날씨)
* **인과관계 (Causation)**

  * 한 변수의 변화가 실제로 다른 변수의 변화를 유발
  * 예: 담배 흡연량 ↑ → 폐암 발생 확률 ↑

### 상관분석 기본 가정

1. **선형성**: 두 변수 관계가 직선 형태여야 함
2. **등분산성**: 오차의 분산이 일정해야 함
3. **정규성**: 데이터가 정규분포를 따른다고 가정
4. **독립성**: 각 데이터가 서로 독립적이어야 함

### 피어슨 상관계수

* 두 변수 사이 직선적 상관관계를 -1 \~ 1 사이 값으로 측정
* +1: 완전 양의 상관관계
* -1: 완전 음의 상관관계
* 0: 상관관계 없음

---

## 3. 분류 모델 (Classification Model)

* **정의**: 종속변수(y)가 **범주형/이산형 변수**일 때 사용하는 모델
* **예시**

  * 이메일 → 스팸 / 정상
  * 종양 → 악성 / 양성

### 이진분류 (Binary Classification)

* 출력값이 **0 또는 1 (참/거짓, yes/no)**
* 가장 기본적인 분류 문제

---

## 4. 분류에서 사용하는 손실함수

### MSE의 한계

* 분류 문제에 적용하면 잘 작동하지 않음 → 패널티가 약함

### 크로스엔트로피 (Cross Entropy)

* 분류에서 주로 사용하는 손실함수
* **Binary Cross Entropy (이진분류)**

  $$
  -\big(y \log(\hat{y}) + (1-y)\log(1-\hat{y})\big)
  $$
* 장점:

  1. 잘못된 예측에 큰 패널티 부여
  2. 로지스틱 함수와 결합 시 미분식이 간단해져 학습이 효율적

---

## 5. 다중분류 (Multiclass Classification)

* 출력 클래스가 3개 이상일 때 사용

### 소프트맥스 함수 (Softmax)

* 로짓(logit) 벡터 → 각 클래스에 대한 확률로 변환
* 모든 클래스 확률 합 = 1

---

## 6. 주요 분류 알고리즘

### KNN (K-Nearest Neighbors)

* 새로운 데이터가 들어오면, **가장 가까운 K개의 이웃**을 보고 다수결로 분류
* 직관적이고 간단하지만, 데이터가 많을수록 계산량 커짐

### 결정트리 (Decision Tree)

* 데이터를 조건에 따라 분할하면서 나무 구조 생성
* **불순도(impurity)**: 노드 내 데이터가 섞여 있는 정도

  * 지니지수(Gini Index), 엔트로피(Entropy) 사용
  * 한 클래스만 있으면 불순도 ↓ (좋음)
  * 여러 클래스 섞이면 불순도 ↑

### 랜덤포레스트 (Random Forest)

* 여러 개의 결정트리를 **무작위로 학습**
* 각 트리의 결과를 **투표(앙상블)** → 최종 결과

### 앙상블 (Ensemble)

* 여러 모델의 결과를 결합해 성능 향상
* Bagging, Boosting, Stacking 등 방식 존재

---

## 7. 결정경계 (Decision Boundary)

* 분류 모델이 두 클래스를 나누는 기준선
* 2D에서는 직선/곡선, 고차원에서는 **초평면(hyperplane)**

### 선형 분리 가능 데이터셋

* 하나의 결정경계(직선/초평면)로 두 클래스가 완전히 나뉘는 데이터

---

## 8. 서포트 벡터 머신 (SVM)

* 선형 분리 가능한 데이터에서, **마진을 최대화하는 초평면**을 찾는 방법
* **마진**: 결정경계와 가장 가까운 점(서포트 벡터) 사이 거리
* 서포트 벡터: 마진 경계에 위치해, 결정경계를 고정시키는 데이터

### 커널 트릭 (Kernel Trick)

* 데이터를 고차원으로 매핑해 선형분리 가능하게 만든 뒤 SVM 적용
* 예: 선형으로 안 나뉘는 데이터를 원형/다항식 커널로 변환 → 초평면으로 분리

---

# ✅ 정리

* **회귀**: 연속값 예측 (MSE 손실, 선형회귀, 해석적/수치적 해법)
* **분류**: 범주값 예측 (Cross Entropy 손실, 로지스틱 회귀, Softmax)
* **상관 vs 인과**: 관계 파악 시 유의
* **결정트리 & 랜덤포레스트**: 불순도 기반 분류, 앙상블 기법
* **SVM**: 마진 최대화, 커널 트릭 활용

---

