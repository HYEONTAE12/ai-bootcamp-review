{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64750f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb75e5",
   "metadata": {},
   "source": [
    "## type\n",
    "- torch.tensor : 원본의 데이터 타입을 그대로 따라가서, tensor 내부적으로 사용하는 자료형 변환\n",
    "- .dtype : tensor 안에 있는 원소들의 자료형을 나타내는 속성\n",
    "- torch.set_default_dtype(torch.float64) : 앞으로 새로 만드는 float의 기본 dtype 을 바꾸는 함수\n",
    "\n",
    "\n",
    "## 비트 수의 따른 차이\n",
    "- float32 와 float64의 차이는 **정밀도** 와 메모리 사용량이다.\n",
    "\n",
    "- float32 : 32비트(=4바이트) 부동소수점 -> 약 7자리 십진수 정밀도\n",
    "- float64 : 64비트(=8바이트) 부동소수점 -> 약 15자리 십진수 정밀도\n",
    "\n",
    "즉, float64 : 더 많은 메모리를 쓰지만, 훨씬 정밀한 연산이 가능\n",
    "\n",
    "- 딥러닝에서는 float32(속도-메모리 효율 때문에)\n",
    "- 정밀한 계산(과학/금융)에서는 float64\n",
    "\n",
    "\n",
    "### 일반적인 PyTorch 타입 추론 규칙\n",
    "\n",
    "- 모두 int → int64 (예: [1, 2, 3])\n",
    "\n",
    "- 모두 float → float32 (예: [1.0, 2.0])\n",
    "\n",
    "- int + float 섞임 → float32로 업캐스팅\n",
    "\n",
    "- 실수 + 복소수 섞임 → complex64로 업캐스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2dcc738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.complex64\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1, 2, 3]).dtype)     # int64\n",
    "print(torch.tensor([1.0, 2.0, 3.0]).dtype) # float32\n",
    "print(torch.tensor([1.2, 3]).dtype)       # float32 (int → float 자동 변환)\n",
    "print(torch.tensor([1.2, 3j]).dtype)      # complex64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64) # 앞으로 새로 만드는 float의 기본 dtype을 64로 변경해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float64\n",
      "torch.float64\n",
      "torch.complex128\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1, 2, 3]).dtype)     # int64\n",
    "print(torch.tensor([1.0, 2.0, 3.0]).dtype) # float65\n",
    "print(torch.tensor([1.2, 3]).dtype)       # float64 (int → float 자동 변환)\n",
    "print(torch.tensor([1.2, 3j]).dtype)      # complex128 / 복소수도 실수부/허수부가 같은 precision을 따라감 complex(=float64 + float64) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b702c66b",
   "metadata": {},
   "source": [
    "## dtype 확인 속성 \n",
    "- .is_floating_point() : 데이터 유형이 부동 소수점 데이터 유형인지 여부를 반환하는 속성 / True or False\n",
    "- .is_complex() : 데이터 유형이 복소수 데이터 유형인지 여부를 반환하는 속성 / True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296ab67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_floating_point()\n",
      "False\n",
      "True\n",
      "False\n",
      "\n",
      "\n",
      ".is_complex()\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([1.2,3])\n",
    "c = torch.tensor([1.2,3j])\n",
    "\n",
    "print(\"is_floating_point()\")\n",
    "print(a.is_floating_point())\n",
    "print(b.is_floating_point())\n",
    "print(c.is_floating_point())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\".is_complex()\")\n",
    "print(a.is_complex())\n",
    "print(b.is_complex())\n",
    "print(c.is_complex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1493a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 📌 PyTorch 주요 dtype 정리\n",
    "\n",
    "| dtype (PyTorch 이름)                        | 크기                         | 값 범위 / 정밀도       | 특징 / 사용 예시                                                        |\n",
    "| ----------------------------------------- | -------------------------- | ---------------- | ----------------------------------------------------------------- |\n",
    "| **`torch.float32`**<br>(= `torch.float`)  | 32비트 (4바이트)                | 약 7자리 유효 숫자      | **딥러닝에서 가장 기본 데이터 타입**. 모델 파라미터, 입력 텐서 등 대부분에 사용됨.                |\n",
    "| **`torch.float64`**<br>(= `torch.double`) | 64비트 (8바이트)                | 약 15~16자리 유효 숫자  | 고정밀 계산용. 메모리와 연산 속도가 느리지만 정밀도가 필요할 때 사용. (ex. 과학 계산, 수치해석)        |\n",
    "| **`torch.complex64`**                     | 64비트 (실수부 32비트 + 허수부 32비트) | float32 수준 정밀도   | 복소수 계산 지원. (ex. 신호 처리, FFT, 물리 시뮬레이션)                             |\n",
    "| **`torch.uint8`**                         | 8비트 (1바이트)                 | 0 ~ 255          | 부호 없는 정수. **이미지 픽셀(raw data)** 저장 시 많이 사용. (ex. OpenCV, PIL 불러오기) |\n",
    "| **`torch.bool`**                          | 1비트 (보통 내부적으로 1바이트)        | `True` / `False` | 논리값 표현. 마스크(mask), 조건문, 필터링 등에 사용.                                |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 딥러닝 실전에서의 주 용도\n",
    "\n",
    "* **float32** → 기본 타입 (학습/추론 대부분)\n",
    "* **double(float64)** → 수치 안정성이 꼭 필요할 때만 (거의 안 씀)\n",
    "* **complex64** → 특수 도메인 (음성, 신호처리)\n",
    "* **uint8** → 원본 데이터 저장/불러오기, 시각화\n",
    "* **bool** → 마스킹, 조건 연산\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 한 줄 요약\n",
    "\n",
    "* **학습/모델 연산**: float32\n",
    "* **정밀 계산**: double\n",
    "* **복소수 연산**: complex64\n",
    "* **데이터 저장/로딩**: uint8\n",
    "* **조건/마스크**: bool\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff98946",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_tensor = torch.ones(1, dtype=torch.float)                 # float32\n",
    "double_tensor = torch.ones(1, dtype=torch.double)               # float64\n",
    "complex_float_tensor = torch.ones(1, dtype=torch.complex64)     # complex64\n",
    "complex_double_tensor = torch.ones(1, dtype=torch.complex128)   # complex128\n",
    "int_tensor = torch.ones(1, dtype=torch.int)                     # int32\n",
    "long_tensor = torch.ones(1, dtype=torch.long)                   # int64\n",
    "uint_tensor = torch.ones(1, dtype=torch.uint8)                  # uint8\n",
    "bool_tensor = torch.ones(1, dtype=torch.bool)                   # bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a194f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 📝 부동소수점 (float32, float64) 이해하기\n",
    "\n",
    "---\n",
    "\n",
    "## 1. float32와 float64 차이\n",
    "\n",
    "* float32 = 32비트 = 4바이트 → 약 7자리 십진수 정밀도\n",
    "* float64 = 64비트 = 8바이트 → 약 15~16자리 십진수 정밀도\n",
    "* 이유: 가수부(mantissa) 비트 수가 다르기 때문\n",
    "\n",
    "  * float32: 23비트 + 숨은 1비트 = 24비트 → 2^24 ≈ 1.6×10^7 → 약 7자리\n",
    "  * float64: 52비트 + 숨은 1비트 = 53비트 → 2^53 ≈ 9×10^15 → 약 15~16자리\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 부동소수점 구조 (IEEE 754)\n",
    "\n",
    "* **부호부 (Sign)**: 1비트 (0=양수, 1=음수)\n",
    "* **지수부 (Exponent)**: 소수점 위치, bias를 더해서 저장\n",
    "\n",
    "  * float32 → bias=127\n",
    "  * float64 → bias=1023\n",
    "* **가수부 (Mantissa)**: 실제 숫자의 모양(소수점 뒤 부분), 앞의 1은 항상 있다고 가정(hidden bit)\n",
    "\n",
    "예: 6.5\n",
    "\n",
    "```\n",
    "6.5 = 110.1₂ = 1.101₂ × 2²\n",
    "부호부: 0\n",
    "지수부: 2 + 127 = 129 → 10000001\n",
    "가수부: 10100000000000000000000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 지수와 가수 이해\n",
    "\n",
    "* **정규화 규칙**: 항상 1.xxx × 2^n 꼴로 바꿈\n",
    "* 지수부 = 소수점을 몇 칸 옮겼는지\n",
    "\n",
    "  * 6.5 → `1.101 × 2²` → 지수 = 2\n",
    "  * 13 → `1.101 × 2³` → 지수 = 3\n",
    "* 가수부 = `1.xxx`에서 맨 앞 `1.`을 뺀 소수점 뒤 부분만 저장\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 왜 정밀도가 7자리 / 15자리인가?\n",
    "\n",
    "* float32 → 가수부 24비트 → 2^24 ≈ 1.6×10^7 → 약 7자리 정밀도\n",
    "* float64 → 가수부 53비트 → 2^53 ≈ 9×10^15 → 약 15~16자리 정밀도\n",
    "* 여기서 `1.6×10^7`이나 `9×10^15` 같은 값은 **과학적 표기법**으로 나타낸 것\n",
    "\n",
    "---\n",
    "\n",
    "✅ **한 줄 요약**\n",
    "float32/64는 “비트 수 → 가수부 정밀도 → 십진수 자리수”로 이어지며, 그래서 각각 약 7자리, 15자리 정밀도를 가진다.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffe855",
   "metadata": {},
   "outputs": [],
   "source": [
    "10**(24 * 0.3010)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
