# BERT í•µì‹¬ ì •ë¦¬

## ğŸ“Œ BERTë€?

**BERT (Bidirectional Encoder Representations from Transformers)**
- Googleì´ 2018ë…„ 10ì›” ë°œí‘œ
- NLPì—ì„œ Pretrain-Finetune íŒ¨ëŸ¬ë‹¤ì„ì„ **ëŒ€ì¤‘í™”**ì‹œí‚¨ ëª¨ë¸
- Transformerì˜ **Encoderë§Œ** ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°
- ì§„ì •í•œ **ì–‘ë°©í–¥(Bidirectional)** ë¬¸ë§¥ í•™ìŠµ

### í•œ ì¤„ ìš”ì•½
> "Masked Language Modelë¡œ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ ì´í•´í•˜ê³ , Self-Supervised Learningìœ¼ë¡œ ë¬´í•œí•œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” Encoder ê¸°ë°˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸"

---

## ğŸ¯ í•µì‹¬ í˜ì‹  3ê°€ì§€

### 1. êµ¬ì¡°ì  í˜ì‹ : ì§„ì •í•œ ì–‘ë°©í–¥

**ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ í•œê³„:**

```
ELMo (2018.1):
- Forward LSTM + Backward LSTM ë”°ë¡œ í•™ìŠµ
- ë‘ ê²°ê³¼ë¥¼ concat
- "ë…ë¦½ì " ì–‘ë°©í–¥ (ì§„ì§œ ì–‘ë°©í–¥ ì•„ë‹˜)

GPT-1 (2018.6):
- Transformer Decoder ì‚¬ìš©
- ë‹¨ë°©í–¥ (â†’ë§Œ ë´„)
- ìƒì„±ì—ëŠ” ì¢‹ì§€ë§Œ ì´í•´ íƒœìŠ¤í¬ì— ë¶ˆë¦¬
```

**BERTì˜ í•´ê²°:**

```python
# Self-Attentionìœ¼ë¡œ ëª¨ë“  í† í°ì´ ë™ì‹œì— ìƒí˜¸ì‘ìš©
"ë‚˜ëŠ” [MASK] ë¨¹ì—ˆë‹¤"

# [MASK] ì˜ˆì¸¡ ì‹œ:
- "ë‚˜ëŠ”" (ì™¼ìª½ ë¬¸ë§¥) âœ“
- "ë¨¹ì—ˆë‹¤" (ì˜¤ë¥¸ìª½ ë¬¸ë§¥) âœ“
- ë™ì‹œì— ì°¸ì¡°! (Self-Attention)

â†’ ì§„ì§œ ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´!
```

**ELMo vs BERT ë¹„êµ:**
| ëª¨ë¸ | ì–‘ë°©í–¥ ë°©ì‹ | ê²°ê³¼ |
|------|-----------|------|
| ELMo | Forward + Backward ë…ë¦½ í•™ìŠµ â†’ concat | ë”°ë¡œë”°ë¡œ |
| BERT | Self-Attentionìœ¼ë¡œ ëª¨ë“  ë°©í–¥ ë™ì‹œ í•™ìŠµ | í†µí•©ì  |

### 2. í•™ìŠµ ë°©ë²•: Self-Supervised Learning

**í•µì‹¬: ì‚¬ëŒì´ ë¼ë²¨ë§ ì•ˆ í•´ë„ ë¨!**

```python
# ì›ë³¸ í…ìŠ¤íŠ¸ ìì²´ê°€ ì •ë‹µ
ì›ë³¸: "ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°”ë‹¤"
ì…ë ¥: "ë‚˜ëŠ” ì˜¤ëŠ˜ [MASK] ê°”ë‹¤"
ì •ë‹µ: "í•™êµì—" â† ìë™ ìƒì„±!

# ë¬´í•œí•œ í•™ìŠµ ë°ì´í„° ìƒì„± ê°€ëŠ¥
# ë¹„ìš©: $0
# ì‹œê°„: ë°ì´í„° ìˆ˜ì§‘ë§Œ
```

**íš¨ìœ¨ì„±:**
```
Supervised Learning:
- ë°ì´í„° ìˆ˜ì§‘: 1ì¼
- ë¼ë²¨ë§: 100ì¼ â† ì‚¬ëŒì´ ì¼ì¼ì´
- í•™ìŠµ: 1ì¼
ì´: 102ì¼

Self-Supervised Learning (BERT):
- ë°ì´í„° ìˆ˜ì§‘: 1ì¼
- ë¼ë²¨ë§: 0ì¼ â† ìë™!
- í•™ìŠµ: 1ì¼
ì´: 2ì¼
```

### 3. ì‚¬ì „í•™ìŠµ íƒœìŠ¤í¬: MLM + NSP

**MLM (Masked Language Model)**
```python
# ì…ë ¥ì˜ 15%ë¥¼ ëœë¤ ë§ˆìŠ¤í‚¹
ì›ë³¸: "ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤"
ì…ë ¥: "ë‚˜ëŠ” [MASK] ë¨¹ì—ˆë‹¤"
ëª©í‘œ: [MASK] = "ë°¥ì„" ì˜ˆì¸¡

# ì‹¤ì œ ë§ˆìŠ¤í‚¹ ì „ëµ:
- 80%: [MASK]ë¡œ ì¹˜í™˜
- 10%: ëœë¤ ë‹¨ì–´ë¡œ ì¹˜í™˜  
- 10%: ê·¸ëŒ€ë¡œ ìœ ì§€
```

**NSP (Next Sentence Prediction)**
```python
# ë‘ ë¬¸ì¥ì´ ì—°ì†ì¸ì§€ íŒë‹¨

# IsNext (50%)
A: "ë‚˜ëŠ” í•™ìƒì´ë‹¤"
B: "í•™êµì— ë‹¤ë‹Œë‹¤" â† ì‹¤ì œ ë‹¤ìŒ ë¬¸ì¥
ë¼ë²¨: 1

# NotNext (50%)
A: "ë‚˜ëŠ” í•™ìƒì´ë‹¤"
B: "í•˜ëŠ˜ì´ íŒŒë—ë‹¤" â† ëœë¤ ë¬¸ì¥
ë¼ë²¨: 0

# [CLS] í† í°ìœ¼ë¡œ ë¬¸ì¥ ê´€ê³„ ì˜ˆì¸¡
```

**ì°¸ê³ :** í›„ì† ì—°êµ¬(RoBERTa)ì—ì„œ NSPëŠ” íš¨ê³¼ê°€ ë¯¸ë¯¸í•œ ê²ƒìœ¼ë¡œ ë°í˜€ì§

---

## ğŸ—ï¸ BERT êµ¬ì¡°

### ì „ì²´ ì•„í‚¤í…ì²˜

```
Input
  â†“
[Embedding Layer]
  - Token Embedding
  - Segment Embedding (ë¬¸ì¥ A/B êµ¬ë¶„)
  - Position Embedding (ìœ„ì¹˜ ì •ë³´)
  â†“
[Encoder Block 1]
  - Multi-Head Self-Attention
  - Add & Norm
  - Feed-Forward Network (FFN)
  - Add & Norm
  â†“
[Encoder Block 2]
  - (ë™ì¼)
  â†“
... (12ê°œ ë˜ëŠ” 24ê°œ ë°˜ë³µ)
  â†“
[Encoder Block N]
  â†“
[Output]
  - [CLS] í† í°: ë¬¸ì¥ ì „ì²´ í‘œí˜„
  - ê° í† í°: ë¬¸ë§¥ ë°˜ì˜ëœ í‘œí˜„
```

### ëª¨ë¸ í¬ê¸°

| ëª¨ë¸ | Layers | Hidden Size | Attention Heads | íŒŒë¼ë¯¸í„° |
|------|--------|-------------|-----------------|---------|
| BERT-base | 12 | 768 | 12 | 110M |
| BERT-large | 24 | 1024 | 16 | 340M |

### Input í˜•ì‹

```python
# ë‹¨ì¼ ë¬¸ì¥
[CLS] ë¬¸ì¥ [SEP]

# ë¬¸ì¥ ìŒ (QA, NLI ë“±)
[CLS] ë¬¸ì¥A [SEP] ë¬¸ì¥B [SEP]

# Embedding
ìµœì¢… ì„ë² ë”© = Token Emb + Segment Emb + Position Emb
```

---

## ğŸ’¡ Self-Attentionì˜ ì§„ê°€

### ì–‘ë°©í–¥ ì´í•´ì˜ ë©”ì»¤ë‹ˆì¦˜

```python
# "ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆë‹¤"ì—ì„œ "ë°¥ì„" ì´í•´í•˜ê¸°

# Query: "ë°¥ì„"ì´ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì–¼ë§ˆë‚˜ ë³¼ê¹Œ?
Q = W_q @ ë°¥ì„_embedding

# Key: ê° ë‹¨ì–´ê°€ "ë°¥ì„"ì—ê²Œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í• ê¹Œ?
K = W_k @ [ë‚˜ëŠ”, ë°¥ì„, ë¨¹ì—ˆë‹¤]_embeddings

# Attention Score ê³„ì‚°
scores = softmax(Q @ K^T / sqrt(d_k))
# scores = [0.2, 0.3, 0.5]
#          â†‘    â†‘    â†‘
#         ë‚˜ëŠ”  ë°¥ì„  ë¨¹ì—ˆë‹¤
# "ë¨¹ì—ˆë‹¤"ì™€ ì—°ê´€ì„±ì´ ê°€ì¥ ë†’ìŒ!

# Value: ì‹¤ì œ ì •ë³´
V = W_v @ [ë‚˜ëŠ”, ë°¥ì„, ë¨¹ì—ˆë‹¤]_embeddings

# ìµœì¢… ì¶œë ¥
output = scores @ V
# "ë°¥ì„" í‘œí˜„ì— "ë¨¹ì—ˆë‹¤" ì •ë³´ê°€ ë§ì´ ë°˜ì˜ë¨
```

**í•µì‹¬:**
- ëª¨ë“  ë‹¨ì–´ê°€ ëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì‹œì— ì°¸ì¡°
- ë¬¸ë§¥ì— ë”°ë¼ ì¤‘ìš”ë„ ìë™ ê³„ì‚°
- ìˆœë°©í–¥/ì—­ë°©í–¥ êµ¬ë¶„ ì—†ì´ í†µí•©ì  ì´í•´

---

## ğŸš€ í•™ìŠµ ê³¼ì •

### ì‚¬ì „í•™ìŠµ (Pretraining)

**ë°ì´í„°:**
```
Wikipedia: 2.5B words
BookCorpus: 800M words
ì´: 3.3B words (33ì–µ ë‹¨ì–´)

â†’ ëª¨ë‘ ë¼ë²¨ë§ ì—†ì´ Self-Supervisedë¡œ í•™ìŠµ!
```

**í•™ìŠµ ëª©í‘œ:**
```python
Total_Loss = MLM_Loss + NSP_Loss

# ë™ì‹œì— ìµœì í™”
# - MLM: ë¬¸ë§¥ ì´í•´, ë‹¨ì–´ ì˜ë¯¸ í•™ìŠµ
# - NSP: ë¬¸ì¥ ê´€ê³„ í•™ìŠµ
```

**í•™ìŠµ í™˜ê²½:**
- GPU: TPU v3 (64ê°œ)
- í•™ìŠµ ì‹œê°„: ì•½ 4ì¼
- Batch size: 256

### Fine-tuning

**ë‹¤ì–‘í•œ Taskì— ì ìš©:**

```python
# 1. í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ê°ì • ë¶„ì„ ë“±)
[CLS] ë¬¸ì¥ [SEP]
â†’ [CLS] í† í° ì¶œë ¥ â†’ Classifier

# 2. ì§ˆì˜ì‘ë‹µ (QA)
[CLS] ì§ˆë¬¸ [SEP] ë¬¸ì„œ [SEP]
â†’ ê° í† í°ì—ì„œ ì‹œì‘/ë ìœ„ì¹˜ ì˜ˆì¸¡

# 3. ê°œì²´ëª… ì¸ì‹ (NER)
[CLS] ë¬¸ì¥ [SEP]
â†’ ê° í† í°ë§ˆë‹¤ ë¼ë²¨ ì˜ˆì¸¡

# 4. ìì—°ì–´ ì¶”ë¡  (NLI)
[CLS] ì „ì œ [SEP] ê°€ì„¤ [SEP]
â†’ [CLS]ë¡œ Entailment/Contradiction/Neutral ë¶„ë¥˜
```

**Fine-tuning íŠ¹ì§•:**
- ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ë¡œ ì‹œì‘
- Taskë³„ ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ ì¶”ê°€
- ì „ì²´ ëª¨ë¸ì„ ì‘ì€ learning rateë¡œ ì¬í•™ìŠµ
- ë³´í†µ 2-4 epochì´ë©´ ì¶©ë¶„

---

## ğŸ“Š BERTì˜ ì˜í–¥

### ì„±ëŠ¥ í–¥ìƒ

```
BERT ì´ì „ (2018.9):
- GLUE ë²¤ì¹˜ë§ˆí¬: í‰ê·  70-75%

BERT ë“±ì¥ (2018.10):
- GLUE ë²¤ì¹˜ë§ˆí¬: í‰ê·  80%+
- 11ê°œ NLP íƒœìŠ¤í¬ ì¤‘ 11ê°œ ëª¨ë‘ SOTA

â†’ NLPì˜ ImageNet ëª¨ë©˜íŠ¸!
```

### íŒŒìƒ ëª¨ë¸ë“¤

**BERT ì´í›„ ìŸì•„ì§„ ê°œì„  ëª¨ë¸:**

| ëª¨ë¸ | ì—°ë„ | ì£¼ìš” ê°œì„  |
|------|------|----------|
| **RoBERTa** | 2019 | NSP ì œê±°, ë” ê¸´ í•™ìŠµ, ë” í° ë°°ì¹˜ |
| **ALBERT** | 2019 | íŒŒë¼ë¯¸í„° ê³µìœ ë¡œ ê²½ëŸ‰í™” |
| **ELECTRA** | 2020 | Replaced Token Detection (ë” íš¨ìœ¨ì ) |
| **DistilBERT** | 2019 | Knowledge Distillationìœ¼ë¡œ 50% ê²½ëŸ‰í™” |

---

## ğŸ” ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ

### ELMo vs GPT vs BERT

| í•­ëª© | ELMo | GPT-1 | BERT |
|------|------|-------|------|
| **êµ¬ì¡°** | Bi-LSTM | Transformer Decoder | Transformer Encoder |
| **ë°©í–¥** | ë…ë¦½ì  ì–‘ë°©í–¥ | ë‹¨ë°©í–¥ (â†’) | í†µí•©ì  ì–‘ë°©í–¥ (â†”) |
| **ì‚¬ì „í•™ìŠµ** | LM (ì–‘ë°©í–¥ ë”°ë¡œ) | LM (ë‹¨ë°©í–¥) | MLM + NSP |
| **íŠ¹í™”** | ë¬¸ë§¥ í‘œí˜„ | ìƒì„± | ì´í•´ |
| **ë³‘ë ¬í™”** | ì–´ë ¤ì›€ (LSTM) | ì‰¬ì›€ | ì‰¬ì›€ |

### Transfer Learning ê´€ì 

```
CV (Computer Vision):
2012: AlexNet â†’ ImageNet ì‚¬ì „í•™ìŠµ
â†’ Transfer Learning ë³´í¸í™”

NLP:
2013-2017: Word2Vec, GloVe (ë‹¨ì–´ ìˆ˜ì¤€ë§Œ)
2018: BERT â†’ ë¬¸ë§¥ ê¸°ë°˜ Transfer Learning
â†’ NLPì˜ Transfer Learning ì‹œëŒ€ ê°œë§‰!
```

---

## ğŸ’» ì‹¤ì „ ì½”ë“œ ì˜ˆì‹œ

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# 1. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # Binary classification
)

# 2. ì…ë ¥ ì¤€ë¹„
text = "This movie is great!"
inputs = tokenizer(
    text,
    padding=True,
    truncation=True,
    return_tensors="pt"
)

# 3. ì˜ˆì¸¡
outputs = model(**inputs)
predictions = torch.softmax(outputs.logits, dim=-1)
print(predictions)  # [ë¶€ì • í™•ë¥ , ê¸ì • í™•ë¥ ]
```

### Fine-tuning ì˜ˆì‹œ

```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("imdb")

# í† í¬ë‚˜ì´ì§•
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

dataset = dataset.map(tokenize, batched=True)

# ëª¨ë¸
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5,  # BERT Fine-tuningì€ ì‘ì€ lr
    warmup_steps=500,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

# í•™ìŠµ
trainer.train()
```

### í•œêµ­ì–´ BERT ì‚¬ìš©

```python
# KoBERT, KoELECTRA, klue/bert ë“±

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# KLUE BERT
tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
model = AutoModelForSequenceClassification.from_pretrained(
    "klue/bert-base",
    num_labels=17  # íš¬íƒœë‹˜ ë¬¸ì„œ ë¶„ë¥˜ 17ê°œ í´ë˜ìŠ¤
)

# ì‚¬ìš©ë²•ì€ ì˜ì–´ BERTì™€ ë™ì¼!
text = "ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
```

---

## âš¡ ì‹¤ë¬´ íŒ

### 1. Fine-tuning í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
# ì¼ë°˜ì ì¸ ì¶”ì²œê°’
learning_rate = 2e-5  # BERTëŠ” ì‘ê²Œ! (1e-5 ~ 5e-5)
batch_size = 16 or 32
epochs = 2-4  # ë³´í†µ 3epochì´ë©´ ì¶©ë¶„
warmup_ratio = 0.1  # ì „ì²´ stepì˜ 10%

# ì‘ì€ ë°ì´í„°ì…‹ (<5000):
epochs = 3-5
learning_rate = 3e-5

# í° ë°ì´í„°ì…‹ (>100000):
epochs = 2-3
learning_rate = 2e-5
```

### 2. ë©”ëª¨ë¦¬ ì ˆì•½ íŒ

```python
# Gradient Checkpointing
model.gradient_checkpointing_enable()  # ë©”ëª¨ë¦¬ ì ˆë°˜

# Mixed Precision (FP16)
training_args = TrainingArguments(
    fp16=True,  # GPU ë©”ëª¨ë¦¬ ì ˆë°˜
)

# Gradient Accumulation
training_args = TrainingArguments(
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,  # effective batch = 32
)
```

### 3. ì…ë ¥ ê¸¸ì´ ì£¼ì˜

```python
# BERT ìµœëŒ€ í† í°: 512
# ê¸¸ì´ ì´ˆê³¼ ì‹œ ìë™ truncation

tokenizer(
    text,
    max_length=512,
    truncation=True,  # ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸°
    padding='max_length',  # ì§§ì€ í…ìŠ¤íŠ¸ íŒ¨ë”©
)
```

### 4. [CLS] í† í° í™œìš©

```python
# ë¬¸ì¥ ì „ì²´ í‘œí˜„ ì¶”ì¶œ
outputs = model(**inputs, output_hidden_states=True)
cls_embedding = outputs.hidden_states[-1][:, 0, :]  # [CLS] í† í°
# â†’ ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš© (ê²€ìƒ‰, ìœ ì‚¬ë„ ê³„ì‚° ë“±)
```

---

## ğŸ¯ BERTì˜ í•œê³„

### 1. ìƒì„± ë¶ˆê°€ëŠ¥
```python
# BERTëŠ” Encoder-only
# í…ìŠ¤íŠ¸ ìƒì„±(generation)ì—ëŠ” ë¶€ì í•©
# â†’ GPT ê³„ì—´ ì‚¬ìš© ê¶Œì¥
```

### 2. ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ í•œê³„
```python
# ìµœëŒ€ 512 í† í°
# ë…¼ë¬¸, ì±… ì „ì²´ ê°™ì€ ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ì–´ë ¤ì›€
# â†’ Longformer, BigBird ë“± ëŒ€ì•ˆ ëª¨ë¸ ì‚¬ìš©
```

### 3. ê³„ì‚° ë¹„ìš©
```python
# Self-Attentionì˜ ë³µì¡ë„: O(nÂ²)
# ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë§¤ìš° ëŠë¦¼
# â†’ DistilBERT, ALBERT ë“± ê²½ëŸ‰í™” ëª¨ë¸ ê³ ë ¤
```

### 4. NSPì˜ ë¹„íš¨ìœ¨
```python
# í›„ì† ì—°êµ¬ì—ì„œ NSP íš¨ê³¼ ë¯¸ë¯¸ ë°œê²¬
# RoBERTa, ELECTRA ë“±ì€ NSP ì œê±°
```

---

## ğŸ“š í•µì‹¬ ìš”ì•½

### BERTë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ

> "Masked Language Modelë¡œ Self-Supervised í•™ìŠµì„ í†µí•´ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ” Transformer Encoder ê¸°ë°˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸"

### BERTì˜ 3ëŒ€ í˜ì‹ 

1. **êµ¬ì¡°**: Self-Attention ê¸°ë°˜ ì§„ì§œ ì–‘ë°©í–¥
2. **í•™ìŠµ**: Self-Supervised (ìë™ ë¼ë²¨ ìƒì„±)
3. **ì „ì´**: Pretrain-Finetune íŒ¨ëŸ¬ë‹¤ì„ ëŒ€ì¤‘í™”

### ê¸°ì–µí•  í•µì‹¬

- âœ… **Encoder-only** êµ¬ì¡° (ì´í•´ íŠ¹í™”)
- âœ… **MLM** (15% ë§ˆìŠ¤í‚¹ â†’ ì˜ˆì¸¡)
- âœ… **Self-Supervised** (ë¬´í•œí•œ í•™ìŠµ ë°ì´í„°)
- âœ… **ì–‘ë°©í–¥** (Self-Attentionìœ¼ë¡œ í†µí•©ì  ë¬¸ë§¥)
- âœ… **Transfer Learning** (Pretrain â†’ Finetune)
- âœ… **[CLS]** í† í° (ë¬¸ì¥ ì „ì²´ í‘œí˜„)
- âœ… **Fine-tuning** (ëª¨ë“  NLP íƒœìŠ¤í¬ì— ì ìš© ê°€ëŠ¥)

### ì–¸ì œ BERTë¥¼ ì“¸ê¹Œ?

**âœ… BERT ì¶”ì²œ:**
- í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ê°ì • ë¶„ì„, ìŠ¤íŒ¸ í•„í„°)
- ì§ˆì˜ì‘ë‹µ (QA)
- ê°œì²´ëª… ì¸ì‹ (NER)
- ìì—°ì–´ ì¶”ë¡  (NLI)
- ë¬¸ì¥ ìœ ì‚¬ë„, ê²€ìƒ‰

**âŒ BERT ë¹„ì¶”ì²œ:**
- í…ìŠ¤íŠ¸ ìƒì„± â†’ GPT ê³„ì—´
- ë§¤ìš° ê¸´ ë¬¸ì„œ â†’ Longformer, BigBird
- ì‹¤ì‹œê°„ ì²˜ë¦¬ (ì†ë„ ì¤‘ìš”) â†’ DistilBERT, ALBERT

---

## ğŸ”— ì°¸ê³ ìë£Œ

### ë…¼ë¬¸
- **BERT ì›ë…¼ë¬¸:** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018)
- **Attention ë©”ì»¤ë‹ˆì¦˜:** [Attention is All You Need](https://arxiv.org/abs/1706.03762) (2017)

### êµ¬í˜„
- **Hugging Face Transformers:** https://github.com/huggingface/transformers
- **ê³µì‹ BERT (TensorFlow):** https://github.com/google-research/bert

### íŠœí† ë¦¬ì–¼
- [Hugging Face BERT ë¬¸ì„œ](https://huggingface.co/docs/transformers/model_doc/bert)
- [Jay Alammar: The Illustrated BERT](http://jalammar.github.io/illustrated-bert/)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-18  

> "BERTëŠ” NLPì˜ ImageNet ëª¨ë©˜íŠ¸ë¥¼ ë§Œë“¤ì—ˆë‹¤. Pretrain-Finetuneì´ ì´ì œ NLPì˜ í‘œì¤€ì´ë‹¤."