
---

# 📘 **[2장] Masked Multi-Head Self-Attention (디코더 블록 1)**

> **“디코더가 미래 단어를 보지 않고 스스로에게 어텐션하는 과정의 완전한 구조 요약”**

---

# 🧩 **1. 이 블록이 하는 일 (한 문장 요약)**

> **디코더가 “지금까지 생성된 단어들끼리” 서로 중요도를 계산하고,
> 다음 단어를 예측하기 위한 문맥 표현을 만드는 단계.
> 이때 "미래 단어는 못 보게" 가리는 것이 핵심.**

이 블록이 틀리면 번역/생성 모델은 **치팅**해서 미래 단어를 미리 보고 말이 안 되는 문장을 만들게 된다.

---

# 🧲 **2. 입력과 출력**

### **입력**

* 이전에 생성된 토큰 임베딩 + 포지셔널 인코딩
  **shape: `[batch, seq_len, d_model]`**

### **출력**

* Self-attention을 적용한 디코더 내부 문맥 표현
  **shape: `[batch, seq_len, d_model]`**
  (차원 유지!)

---

# 🔍 **3. 내부 흐름 (한눈에 딱 보이게)**

```
Input (임베딩)
   │
   ▼
[Q, K, V 생성]  (Linear 3개)
   │
   ▼
Mask 적용 (미래 위치 ∞로 막기)
   │
   ▼
Scaled Dot-Product Attention
   │
   ▼
Head별 Attention 결과
   │
   ▼
Concatenate
   │
   ▼
Output Linear
   │
   ▼
Residual Connection + LayerNorm
```

---

# 🧱 **4. 단계별 상세 설명 (중요 핵심만)**

## 🟦 **① Q, K, V 생성 (Query / Key / Value)**

입력 임베딩에서 3개의 가중치 W_q, W_k, W_v를 곱해서 만듦.

* Query: “내가 지금 보고 싶은 기준”
* Key: “참조되는 정보의 라벨”
* Value: “실제 가져올 정보”

**역할 요약:**

> Q는 "내가 중요도를 계산하고 싶은 위치",
> K는 "모든 위치가 가진 신분증",
> V는 "거기서 실제로 가져올 값".

---

## 🟥 **② Masking (Look-ahead Mask)**

디코더가 미래 단어를 보면 규칙 위반이라 **마스크로 차단**한다.

마스크는 다음처럼 생김:

```
t=1 → [ X 0 0 0 ... ]
t=2 → [ X X 0 0 ... ]
t=3 → [ X X X 0 ... ]
```

0 = 허용
-∞ = 차단

Softmax 전에 미래 위치는 모두 **-∞**를 넣어 softmax 결과가 0이 되게 함.

**핵심:**

> 현재 토큰은 자기보다 “이전 위치만” 볼 수 있다.

---

## 🟩 **③ Scaled Dot-Product Attention 계산**

```
Attention(Q, K, V) = Softmax( QKᵀ / √d_k + Mask ) V
```

* QKᵀ: Query와 Key를 비교해서 유사도 계산
* √d_k: 숫자 안정성 위해 스케일링
* Mask: 미래 단어 차단
* Softmax: 가중치 변환
* V: 의미 정보

---

## 🟧 **④ Multi-Head Attention**

Head를 여러 개 두는 이유:

* 단어 간 관계를 여러 관점에서 보려고
  (예: 문법적 관계 / 의미적 관계 / 위치적 관계 등)

구성:

* Head 1: Q₁, K₁, V₁
* Head 2: Q₂, K₂, V₂
* …
* Head h: Qₕ, Kₕ, Vₕ

각 Head에서 Attention 계산 → concat → Linear

---

## 🟪 **⑤ Residual Connection + LayerNorm**

Transformer 전통 공식:

```
Output = LayerNorm( Input + MultiHead(Q, K, V) )
```

* Gradient 흐름 안정
* 정보 손실 방지
* 깊은 층에서도 학습 안정성 제공

---

# 🎯 **5. 이 블록이 디코더에서 담당하는 ‘정확한 역할’**

1. 단어들끼리 스스로 문맥을 정리
2. 미래 단어를 절대 보지 않도록 보호
3. 다음 토큰 생성에 필요한 “현재까지의 상태 표현” 생성
4. 여러 Head 관점을 통해 다양한 의미 관계 추출
5. Encoder 정보를 보기 전에 **기초 문맥 상태** 정돈

즉:

> **이 블록이 디코더의 “기초 자기 인식 정보”를 만든다.
> 이게 제대로 돼야 이후 Encoder–Decoder Attention이 빛을 발한다.**

---

# 🧠 **6. Masked Self-Attention이 없으면 생기는 문제**

* 모델이 미리 정답을 보고 문장 생성 → 학습이 무너지거나 의미 없는 문장 생성
* 언어 모델이 “왼→오른쪽 생성”이라는 기본 규칙이 깨짐
* 비논리적/불연속적인 문장 생성 증가

이 블록이 디코더의 “언어 생성 규칙”을 유지하는 핵심.

---

# 🧾 **7. 블록 전체 요약 (초압축 버전)**

> **이전 단어들끼리의 관계만 본다 → 미래는 마스킹으로 차단 →
> 여러 헤드가 서로 다른 시각으로 문맥을 해석 →
> Residual + LayerNorm으로 안정화 →
> 디코더 내부 상태 벡터 출력.**

---

