

---

# 📘 **[4장] Position-wise Feed Forward Network (FFN)**

> **“토큰 단위로 독립적인 비선형 변환을 적용해 모델의 표현력을 확장시키는 단계”**

---

# 🧩 **1. FFN의 한 문장 요약**

> **어텐션으로 모은 정보를 ‘단어별로 개별 변환’하여
> 더 복잡한 패턴을 표현할 수 있게 만드는 비선형 확장 레이어.**

어텐션이 “관계”를 담당한다면,
FFN은 **각 토큰 자체를 더 똑똑하게 만드는 업그레이드 레이어**다.

---

# 🧲 **2. 입력과 출력**

### ✔ 입력

랜덤/어텐션을 거쳐 온 벡터
**shape: `[batch, seq_len, d_model]`**

### ✔ 출력

같은 shape 유지
**shape: `[batch, seq_len, d_model]`**

즉, **차원 변화는 없지만 표현력만 증가**한다.

---

# 🔍 **3. 전체 흐름 요약 (시각적)**

```
Input
  │
  ▼
Linear (확장) : d_model → d_ff
  │
  ▼
ReLU (또는 GELU)
  │
  ▼
Linear (축소) : d_ff → d_model
  │
  ▼
Residual Connection + LayerNorm
  │
  ▼
Output (표현력 강화된 토큰)
```

---

# 🧱 **4. 단계 상세 설명**

## 🟦 **① Position-wise 적용 (토큰별 독립)**

FFN은 “seq_len(토큰)” 방향으로는 아무 연산도 안 하고,
**각 토큰마다 동일한 MLP를 독립적으로 적용한다.**

즉:

```
토큰1 → FFN → out1  
토큰2 → FFN → out2  
토큰3 → FFN → out3  
...
```

이게 “position-wise”라는 뜻이다.

**어텐션 = 토큰 간 관계**
**FFN = 토큰 자체의 의미 변환**

둘의 역할이 완전히 다르다.

---

## 🟥 **② 첫 Linear: d_model → d_ff (확장)**

일반적으로:

* d_model = 512 또는 768
* d_ff = 2048 혹은 3072 등 **4~8배** 크다.

역할:

> **각 토큰의 표현 차원을 크게 넓혀서 더 복잡한 패턴을 만들 준비.**

---

## 🟩 **③ 활성함수 (ReLU or GELU)**

ReLU나 GELU를 통해 비선형성 부여.

역할:

> **선형 레이어만으로는 표현할 수 없는 함수적 복잡도를 추가**
> → 모델이 훨씬 풍부한 패턴을 학습함

GELU는 큰 모델에서 더 선호되며, GPT 계열은 GELU를 많이 씀.

---

## 🟧 **④ 두 번째 Linear: d_ff → d_model (축소)**

확장된 고차원 벡터를 다시 원래 차원(d_model)로 축소해서
다음 블록과 연결할 수 있게 함.

역할:

> 확장에서 얻은 복잡한 특성을 다시 “요약된 의미 벡터”로 집약.

---

## 🟪 **⑤ Residual + LayerNorm**

Transformer 공식:

```
Output = LayerNorm(Input + FFN(Input))
```

역할:

* 원본 정보 손실 방지
* 깊은 네트워크에서도 안정성 유지
* training dynamics 안정화
* gradient 소실 방지

---

# 🌌 **5. 왜 FFN이 필수적인가?**

### ✔ 이유 1) Attention만으로는 “관계”만 학습한다

Self-Attention은 토큰 간 상호작용을 처리할 뿐,
토큰 자체의 복잡한 의미 변환은 잘 못한다.

FFN은 그 빈 공간을 채운다.

### ✔ 이유 2) 언어의 비선형적인 패턴을 잡는다

언어는 직선적이지 않다.

* 부정어 처리
* 은유/추상 표현
* 문맥 유지
  이런 것들은 비선형 레이어가 필수.

### ✔ 이유 3) GPT, BERT에서 모델 크기를 결정하는 핵심

파라미터 대부분이 FFN에서 나온다.
즉, **모델의 “용량(capacity)” = FFN 크기(d_ff)로 거의 결정됨.**

---

# 🎯 **6. 이 블록의 역할 요약 6개**

1. 어텐션으로 모은 정보를 더 복잡한 표현으로 변환
2. 문장의 깊은 의미나 고차 패턴 학습
3. 각 토큰을 독립적으로 “지능 업그레이드”
4. 모델의 표현력(core capacity)을 담당
5. Residual+LayerNorm으로 안정성 확보
6. d_model 유지하여 다음 블록과 연결되기 쉬움

---

# 🧾 **7. 초압축 요약**

> **FFN = 확장 → 비선형 → 축소.
> 어텐션이 관계를 담당한다면, FFN은 토큰 자체의 의미를 강화한다.**

---

