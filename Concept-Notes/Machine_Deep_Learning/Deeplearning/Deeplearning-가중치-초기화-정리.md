
---

# 성능 고도화 방법 — 가중치 초기화

## 1. 왜 가중치 초기화가 필요할까?

* 모델의 층이 깊어질수록 **활성화 함수 이후 출력 값의 분포가 한쪽으로 치우칠 수 있음**.
  → 학습 불안정, 효율적인 학습 방해.
* 초기화를 잘못하면 **비효율적인 학습**, 또는 아예 학습이 진행되지 않을 수 있음.
* 손실 함수로부터의 기울기가 층을 거치며 **점점 작아지거나(기울기 소실) 커지는(기울기 폭발)** 문제가 발생.

---

## 2. 잘못된 초기화 방식

* **모든 가중치를 0으로 초기화**

  * 모든 뉴런이 같은 출력과 같은 기울기를 가지게 됨 → 학습이 전혀 진행되지 않음. (대칭성 문제, symmetry problem)
* **균등 초기화 (Uniform Initialization)**

  * 가중치가 일정 범위에 고르게 분포.
  * 출력 범위가 지나치게 넓어져 학습에 방해될 수 있음.
* **표준 정규분포 초기화 (Standard Normal Initialization)**

  * 가중치가 너무 크거나 작게 설정되어 **기울기 소실/폭발** 위험.

👉 핵심: 각 층의 출력 값이 **적당히 넓게, 고르게 분포**되도록 초기화해야 함.

---

## 3. 주요 초기화 기법

### (1) Xavier 초기화 (Glorot Initialization)

* **개념**: 입력/출력 노드 수를 고려해 가중치 분산을 설정.
* **적합한 경우**: Sigmoid, tanh 등 **대칭형 활성화 함수**.
* **공식**:
  [
  W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in}+n_{out}}}, \sqrt{\frac{6}{n_{in}+n_{out}}}\right)
  ]

  * (n_{in}): 입력 노드 수
  * (n_{out}): 출력 노드 수
* **장점**: 기울기가 너무 크거나 작아지는 문제를 완화.

---

### (2) He 초기화 (Kaiming Initialization)

* **배경**: Xavier는 ReLU 계열 활성화 함수에서 여전히 **기울기 소실** 문제가 존재.
* **개념**: ReLU는 음수 구간에서 출력이 0 → 기울기 소실 위험.
  → 이를 보완하기 위해 **더 큰 분산**을 가진 분포에서 초기화.
* **공식**:
  [
  W \sim \mathcal{N}(0, \frac{2}{n_{in}})
  ]
* **적합한 경우**: ReLU, Leaky ReLU, ELU 등 **비대칭 활성화 함수**.
* **장점**: 깊은 네트워크에서도 안정적인 학습 가능.

---

## 4. 추가적인 학습 안정화 기법

### (1) 가중치 감쇠 (Weight Decay, L2 Regularization)

* **개념**: 큰 가중치에 패널티를 부과하여 가중치를 작게 유지.
* **효과**:

  * 모델 복잡도 감소 → 과적합 방지
  * 훈련 데이터가 적거나 노이즈가 많을 때 특히 유용

---

### (2) 학습 조기 종료 (Early Stopping)

* **개념**: 검증 데이터 성능이 더 이상 개선되지 않으면 학습을 멈춤.
* **효과**:

  * 과적합 방지
  * 불필요한 학습 시간 절약

---

## 5. 전체 요약

* **가중치 초기화는 모델 학습의 출발점**으로, 잘못 설정하면 학습 불능 상태에 빠질 수 있음.
* **Xavier 초기화**: Sigmoid, tanh 계열에서 적합.
* **He 초기화**: ReLU 계열에서 적합.
* **가중치 감쇠, 조기 종료** 등과 함께 사용하면 안정적이고 강건한 학습 가능.

---

