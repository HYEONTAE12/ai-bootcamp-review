
---

# ğŸ“Œ PyTorch Dataset Â· DataLoader Â· Model Â· Training Â· Inference ì •ë¦¬

## 1. Datasetê³¼ DataLoader

### ğŸ”¹ Dataset

* **ì—­í• **: ë‹¨ì¼ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ì— ì í•©í•œ í˜•íƒœ(`torch.Tensor`)ë¡œ ë³€í™˜ ë° ë°˜í™˜.
* **ì¢…ë¥˜**

  * PyTorch ë‚´ì¥ Dataset: `MNIST`, `CIFAR10`, `ImageFolder` ë“±.
  * Custom Dataset: ì§ì ‘ êµ¬í˜„ í•„ìš”.
    `torch.utils.data.Dataset` í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ê³  ë‹¤ìŒ 3ê°œ ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•¨:

    1. `__init__`: ë°ì´í„°ì…‹ ë¡œë“œ, ê²½ë¡œ/ë³€ìˆ˜ ì„ ì–¸
    2. `__getitem__`: ì£¼ì–´ì§„ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ë°˜í™˜
    3. `__len__`: ì „ì²´ ë°ì´í„° ê°œìˆ˜ ë°˜í™˜
* **ì£¼ì˜ì‚¬í•­**

  * ë°˜í™˜ ë°ì´í„°ëŠ” **tensor**ì—¬ì•¼ í•¨.
  * ë°˜í™˜ í˜•íƒœ: `Tensor`, `(Tensor, Tensor)`, `{key: Tensor}` ê°€ëŠ¥.
  * ëª¨ë“  ìƒ˜í”Œì˜ ì°¨ì›ì€ **ì¼ê´€ì„±** ìˆì–´ì•¼ í•¨ (ë°°ì¹˜ êµ¬ì„± ë•Œë¬¸).

---

### ğŸ”¹ DataLoader

* **ì—­í• **: `Dataset`ì—ì„œ ë°ì´í„°ë¥¼ êº¼ë‚´ **ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„**ë¡œ ë¬¶ì–´ ì œê³µ.
* **ì£¼ìš” ì¸ì**

  1. `dataset`: ì‚¬ìš©í•  Dataset ê°ì²´ (í•„ìˆ˜)
  2. `batch_size` (ê¸°ë³¸=1): ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
  3. `shuffle` (ê¸°ë³¸=False): ë§¤ epochë§ˆë‹¤ ë°ì´í„° ìˆœì„œ ì„ê¸°
  4. `num_workers` (ê¸°ë³¸=0): ë°ì´í„° ë¡œë”©ì— ì‚¬ìš©í•  ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ ìˆ˜
  5. `drop_last` (ê¸°ë³¸=False): ë§ˆì§€ë§‰ ë°°ì¹˜ í¬ê¸°ê°€ ë§ì§€ ì•Šìœ¼ë©´ ë²„ë¦´ì§€ ì—¬ë¶€
  6. `collate_fn`: ë°°ì¹˜ ë°ì´í„° ê²°í•© ë°©ì‹ì„ ì •ì˜ (íŠ¹íˆ sequence dataì—ì„œ ì¤‘ìš”)
* **ë¯¸ë‹ˆë°°ì¹˜**: ì „ì²´ ë°ì´í„°ë¥¼ ì˜ê²Œ ë‚˜ëˆˆ ë¶€ë¶„ì§‘í•©. í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½ ë° í•™ìŠµ ì•ˆì •í™” íš¨ê³¼.

---

## 2. ëª¨ë¸ (Model)

### ğŸ”¹ PyTorch ì œê³µ ëª¨ë¸

* **Torchvision**: ResNet, VGG ë“± ì´ë¯¸ì§€ ë¶„ì„ íŠ¹í™” ëª¨ë¸.
* **PyTorch Hub**: Computer Vision, NLP, Audio, Generative ëª¨ë¸ ë“± ê³µê°œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥.

### ğŸ”¹ Custom Model

* ì´ìœ : ê¸°ì¡´ ëª¨ë¸ ë³€í˜•, ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì‹¤í—˜ í•„ìš”.
* **êµ¬í˜„ ë°©ë²•**

  * `torch.nn.Module` í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ì‘ì„±.
  * í•„ìˆ˜ ë©”ì„œë“œ:

    1. `__init__`: `super().__init__()` í˜¸ì¶œ, ë ˆì´ì–´ ì •ì˜ ë° íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
    2. `forward`: ì…ë ¥ ë°ì´í„° ì—°ì‚° ì •ì˜

---

## 3. í•™ìŠµ(Training)ì˜ ê¸°ë³¸ êµ¬ì¡°

### ğŸ”¹ í•™ìŠµ ë£¨í”„ ì¼ë°˜ êµ¬ì¡°

```python
for epoch in range(num_epochs):
    for data, label in dataloader:
        optimizer.zero_grad()           # 1. ì´ì „ gradient ì´ˆê¸°í™”
        output = model(data)            # 2. ìˆœì „íŒŒ
        loss = loss_function(output, label)  # 3. ì†ì‹¤ ê³„ì‚°
        loss.backward()                 # 4. ì—­ì „íŒŒ(gradient ê³„ì‚°, Autograd í™œìš©)
        optimizer.step()                 # 5. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
```

### ğŸ”¹ Autograd & Computational Graph

* **Autograd**: ìë™ ë¯¸ë¶„ ì‹œìŠ¤í…œ.
* **Computational graph**:

  * ë…¸ë“œ(Node): ì—°ì‚° (ì˜ˆ: +, Ã—, ReLU)
  * ì—£ì§€(Edge): ì…ë ¥/ì¶œë ¥ í…ì„œ
* PyTorchëŠ” tensor ì—°ì‚° ì‹œ ê·¸ë˜í”„ë¥¼ ê¸°ë¡ â†’ `loss.backward()` í˜¸ì¶œ ì‹œ ë¯¸ë¶„ ìë™ ê³„ì‚°.

---

## 4. ì¶”ë¡ (Inference)ê³¼ í‰ê°€(Evaluation)

* **ì¶”ë¡  ê³¼ì •**: í•™ìŠµëœ ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ì…ë ¥ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰.
* **í•„ìš” ì„¤ì •**

  1. `model.eval()`

     * Dropout, BatchNorm ê°™ì€ ë ˆì´ì–´ë¥¼ **ì¶”ë¡  ëª¨ë“œ**ë¡œ ì „í™˜.
  2. `with torch.no_grad():`

     * Autograd ë¹„í™œì„±í™” â†’ gradient ê³„ì‚° ì•ˆ í•¨.
     * ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆê°, ì†ë„ í–¥ìƒ.

### ğŸ”¹ ì˜ˆì‹œ

```python
model.eval()
with torch.no_grad():
    for data in test_loader:
        output = model(data)
        # ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
```

### ğŸ”¹ í‰ê°€ ì§€í‘œ

* ë¶„ë¥˜(Classification): Accuracy, Precision, Recall, F1-score
* íšŒê·€(Regression): MSE, RMSE, MAE

---

## 5. ì¶”ê°€ ë³´ì¶© ë‚´ìš©

* **Collate Function (`collate_fn`)**

  * DataLoaderê°€ ë°°ì¹˜ë¥¼ êµ¬ì„±í•  ë•Œ, ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë¬¶ì„ì§€ ì •ì˜í•˜ëŠ” í•¨ìˆ˜.
  * NLP ë“± ê¸¸ì´ê°€ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ íŒ¨ë”©(padding)ì„ ë„£ê±°ë‚˜, Dict í˜•íƒœë¥¼ ë§ì¶œ ë•Œ í•„ìš”.

* **Epoch**

  * í•™ìŠµ ë°ì´í„° ì „ì²´ë¥¼ í•œ ë²ˆ ë‹¤ í•™ìŠµí•˜ëŠ” ì£¼ê¸°.
  * `epoch` ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ê³¼ì í•© ìœ„í—˜ â†’ `early stopping` í™œìš©.

* **GPU ì‚¬ìš©**

  ```python
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)
  data, label = data.to(device), label.to(device)
  ```

* **Optimizer ì˜ˆì‹œ**

  * `torch.optim.SGD`, `torch.optim.Adam` ë“±.
  * í•˜ì´í¼íŒŒë¼ë¯¸í„°: learning rate, momentum, weight decay ë“±.

---

## âœ… ìµœì¢… ì •ë¦¬

* **Dataset**: ë‹¨ì¼ ë°ì´í„° ë‹¨ìœ„ ì²˜ë¦¬/ë°˜í™˜
* **DataLoader**: Datasetì„ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë¬¶ì–´ ë°˜í™˜
* **Model**: `nn.Module` ìƒì† â†’ `__init__` + `forward` ì •ì˜
* **Training Loop**: `zero_grad â†’ forward â†’ loss â†’ backward â†’ step`
* **Autograd**: computational graph ê¸°ë°˜ ìë™ ë¯¸ë¶„
* **Inference**: `model.eval()` + `torch.no_grad()`
* **ë³´ì¶© ê°œë…**: `collate_fn`, epoch, GPU ì‚¬ìš©ë²•, í‰ê°€ ì§€í‘œ

---

