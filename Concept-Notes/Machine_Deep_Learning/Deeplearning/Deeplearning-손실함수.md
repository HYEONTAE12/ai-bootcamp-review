

---

# 📘 손실 함수(Loss Function) 정리

## 1. 손실 함수란?

* **손실 함수(Loss Function)**: 모델이 예측한 값과 실제 값 사이의 차이를 수치화한 함수.
* 역할: 학습 과정에서 **모델의 성능을 평가**하는 기준.
* 손실 함수의 값이 작아진다는 것은 → 모델이 실제 데이터에 더 잘 맞는다는 의미.

---

## 2. 손실 함수에 따른 학습 결과 차이

* 어떤 손실 함수를 선택하느냐에 따라 **학습 속도, 수렴 방식, 최종 성능**이 달라질 수 있음.
* 예: 같은 데이터셋이라도 MSE를 쓰면 이상치(outlier)에 민감하고, MAE를 쓰면 이상치에 덜 민감.

---

## 3. 대표적인 손실 함수

### 🔹 평균제곱오차 (Mean Squared Error, MSE)

* 정의: 실제값과 예측값 차이를 **제곱**한 뒤 평균.
* 다른 이름: **Quadratic Loss**, **L2 Loss**.
* 특징:

  * 오차 제곱 → 큰 오차에 더 큰 패널티 → 이상치에 민감.
  * 초반 학습 속도 빠름.
  * 주로 **회귀 문제**에서 사용.

---

### 🔹 평균절대오차 (Mean Absolute Error, MAE)

* 정의: 실제값과 예측값 차이의 **절대값** 평균.
* 다른 이름: **L1 Loss**.
* 특징:

  * 단위가 실제값과 같아 직관적으로 해석 가능.
  * 이상치(outlier)에 비교적 둔감.
  * 단점:

    * 기울기(gradient)가 일정해 → 학습이 튀는(jumping) 현상 발생 가능.
    * 최소값 근처에서도 기울기가 0으로 수렴하지 않아 학습률을 직접 조절해야 하는 경우가 있음.

---

### 🔹 Huber Loss

* 정의: 오차가 작은 구간에서는 MSE, 큰 구간에서는 MAE를 적용.
* 특징:

  * 두 손실 함수의 장점을 결합.
  * 작은 오차 → 제곱항(MSE) 사용으로 부드러운 수렴.
  * 큰 오차 → 절댓값(MAE) 사용으로 이상치에 둔감.
  * 주로 **로버스트 회귀(Robust Regression)**에서 사용.

---

### 🔹 교차 엔트로피 (Cross Entropy, CE)

* 정의: 두 확률 분포 간의 차이를 측정하는 함수.
* 특징:

  * 분류 문제에서 가장 널리 쓰임.
  * **Binary Cross Entropy (BCE)**: 이진 분류 문제에서 사용. (Log Loss라고도 함)
  * **Categorical Cross Entropy (CCE)**: 다중 분류 문제에서 사용.
  * 예측 확률 분포가 실제 정답 분포와 멀어질수록 손실이 커짐.
  * 잘못된 예측일수록 큰 패널티 → 분류 문제에서 학습 효율이 좋음.

---

### 🔹 Hinge Loss

* 정의: 모델이 만든 **결정 경계(decision boundary)**와 데이터 사이의 **마진(margin)**을 최대화.
* 특징:

  * 주로 **이진 분류 문제**에서 사용.
  * 레이블 y는 +1(양성), -1(음성)으로 설정.
  * 잘못 분류된 샘플과 경계 근처 샘플에 큰 손실 부여.
  * **SVM(Support Vector Machine) Loss**라고도 불림.

---

## 4. 추가적으로 알아두면 좋은 손실 함수들

* **KL Divergence (Kullback–Leibler Divergence)**

  * 두 확률 분포 차이를 측정.
  * 보통 **정규화된 분포 비교**나 **딥러닝에서 확률 분포 정합** 시 활용.
* **Cosine Similarity Loss**

  * 두 벡터 사이의 방향 유사도를 측정.
  * NLP, 추천 시스템 등에서 많이 활용.
* **Focal Loss**

  * 불균형 데이터셋(예: 희귀 클래스 분류)에서 자주 사용.
  * 어려운 샘플에 더 큰 가중치를 주어 학습.

---

## 5. 정리

* 손실 함수는 단순히 오차를 측정하는 도구가 아니라, **학습 방향과 성능을 결정하는 핵심 요소**.
* 문제 유형에 따라 적절한 손실 함수를 선택해야 함:

  * **회귀 문제** → MSE, MAE, Huber Loss
  * **이진 분류** → BCE, Hinge Loss
  * **다중 분류** → Categorical Cross Entropy
* 이상치 처리, 데이터 분포, 문제 특성에 따라 올바른 손실 함수를 선택하는 것이 중요.

---
