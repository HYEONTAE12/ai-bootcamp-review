

---

# ⚡ 활성화 함수 (Activation Function)

## 1. 활성화 함수란?

* 신경망의 각 뉴런 출력에 적용되는 **비선형 함수**
* 역할:

  1. **비선형성 부여** → 신경망이 단순 선형모델을 넘어 복잡한 패턴 학습 가능
  2. **정보 흐름 조절** → 어떤 신호를 통과시킬지(활성화) 결정

---

## 2. 왜 필요한가?

* 선형 변환(Linear Layer)만 여러 층 쌓아도 결국 또 하나의 **선형 변환**으로 합쳐짐
  → 복잡한 비선형 관계를 학습 불가
* 활성화 함수가 들어가야 **곡선, 복잡한 경계, 고차원 패턴**을 학습할 수 있음

비유 👉

* 활성화 함수 없는 네트워크 = 자로 직선만 긋는 사람
* 활성화 함수 있는 네트워크 = 직선 + 곡선자도 쓰는 사람

---

## 3. 대표적인 활성화 함수들

### (1) Sigmoid

[
\sigma(x) = \frac{1}{1 + e^{-x}}
]

* 출력 범위: (0, 1)
* 장점: 확률처럼 해석 가능
* 단점: Gradient Vanishing 문제 (x가 크거나 작으면 기울기 거의 0)
* 사용처: 과거 이진 분류 출력층

---

### (2) Tanh (Hyperbolic Tangent)

[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
]

* 출력 범위: (-1, 1)
* Sigmoid 개선판 (평균이 0에 가까움)
* 단점: 여전히 Gradient Vanishing 존재

---

### (3) ReLU (Rectified Linear Unit)

[
f(x) = \max(0, x)
]

* 현재 가장 많이 쓰이는 함수
* 장점: 계산 간단, Gradient Vanishing 완화
* 단점: 음수 구간에서 뉴런이 죽는 **Dead ReLU** 문제 발생 가능

---

### (4) Leaky ReLU

[
f(x) = \max(\alpha x, x) \quad (\alpha \approx 0.01)
]

* ReLU 개선 → 음수 구간도 아주 작은 기울기 허용
* Dead ReLU 문제 완화

---

### (5) Softmax

[
\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
]

* 여러 클래스에 대한 확률 분포로 변환
* 출력 값 합이 1 → 분류 문제의 마지막 출력층에서 자주 사용

---

### (6) 기타

* **ELU, GELU, Swish**: ReLU 변형으로 더 매끄럽게 gradient 전달
* 최신 모델(GPT, BERT 등)은 주로 **GELU**를 사용

---

## 4. 사용 가이드

* 은닉층(hidden layer): ReLU / Leaky ReLU / GELU
* 출력층(output layer):

  * 이진 분류 → Sigmoid
  * 다중 분류 → Softmax
  * 회귀 → 활성화 함수 없음(Linear 출력)

---

## 5. 요약

* **활성화 함수 = 신경망에 비선형성 부여하는 장치**
* 없으면 단순 선형 모델에 불과 → 복잡한 데이터 학습 불가
* ReLU 계열이 현재 가장 널리 쓰임
* 출력층은 문제 성격(분류/회귀)에 따라 다르게 선택

---

👉 정리하면, **“가중치 초기화는 출발점, 활성화 함수는 곡선 길을 만들 수 있게 해주는 도구”**라고 보면 돼.

---

