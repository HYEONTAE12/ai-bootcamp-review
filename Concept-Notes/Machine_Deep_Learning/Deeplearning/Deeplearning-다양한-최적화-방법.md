
---

# 다양한 최적화 방법 정리

## 1. 기본적인 옵티마이저

### Gradient Descent (GD)

* **개념**: 가장 기본적인 최적화 알고리즘. 손실 함수의 기울기를 계산해 가중치를 갱신.

* **공식**:
  [
  \theta := \theta - \eta \nabla_\theta J(\theta)
  ]

  * (\theta): 파라미터(가중치)
  * (\eta): 학습률 (Learning Rate)
  * (J(\theta)): 손실 함수

* **단점**: 전체 데이터셋을 사용하므로 연산 비용이 크고, 대규모 데이터셋에 비효율적.

---

### Stochastic Gradient Descent (SGD)

* **개념**: 무작위로 선택한 하나의 샘플을 사용해 기울기 계산 및 갱신.
* **장점**: 빠른 업데이트, 메모리 효율적.
* **단점**: 기울기 변동이 크고, 최적화 경로가 불안정 → 손실 함수가 요동치듯 진행.

---

### Mini-Batch Gradient Descent

* **개념**: 전체 데이터셋(GD)과 하나의 샘플(SGD) 사이의 절충안.
* 미니배치 단위로 기울기를 계산하여 갱신.
* **딥러닝의 표준 방법**: 실제로는 대부분 미니배치 SGD 사용.
* **장점**:

  * 병렬 처리에 적합 (GPU 활용 용이)
  * 안정성과 속도 균형

---

## 2. SGD의 개선 기법

### Momentum

* **개념**: SGD에 관성(이전 기울기)을 반영하여, 급격한 방향 전환을 완화.
* **효과**:

  * 지역 최소값(Local Minima) 탈출에 유리
  * 매끄러운 최적화 경로 확보
* **공식**:
  [
  v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)
  ]
  [
  \theta := \theta - v_t
  ]

  * (\gamma): 모멘텀 계수 (보통 0.9)

---

### Nesterov Accelerated Gradient (NAG)

* **개념**: Momentum의 개선 버전.
* 단순히 이전 속도를 반영하는 것이 아니라, **미리 이동한 위치에서 기울기 계산**.
* **효과**: 더 정확한 업데이트 가능, 수렴 속도 향상.

---

## 3. 적응형 학습률 옵티마이저 (Adaptive Learning Rate)

### AdaGrad (Adaptive Gradient)

* **개념**: 특성(feature)마다 다른 학습률 적용.
* 자주 등장하는 특성 → 학습률 감소, 드문 특성 → 학습률 증가.
* **단점**: 시간이 지날수록 학습률이 계속 줄어들어, 학습이 멈추는 현상 발생.

---

### RMSProp (Root Mean Square Propagation)

* **개념**: AdaGrad의 단점 보완.
* 과거 모든 기울기를 누적하지 않고, **최근 기울기만 지수 이동 평균(EMA)**으로 반영.
* **효과**: 학습률 감소 문제 해결 → 안정적 수렴.

---

### Adam (Adaptive Moment Estimation)

* **개념**: Momentum + RMSProp 결합.
* 기울기의 **1차 모멘트(평균)**와 **2차 모멘트(분산)** 추정 → 학습률을 동적으로 조절.
* **장점**:

  * 대부분의 문제에서 좋은 성능
  * 하이퍼파라미터 튜닝에 덜 민감
* **공식**:
  [
  m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
  ]
  [
  v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
  ]
  [
  \theta := \theta - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
  ]

  * (m_t): 1차 모멘트(평균)
  * (v_t): 2차 모멘트(분산)

---

## 4. 각 옵티마이저 특징 비교

| Optimizer     | 특징               | 장점              | 단점          |
| ------------- | ---------------- | --------------- | ----------- |
| GD            | 전체 데이터로 갱신       | 안정적, 전역적 추세     | 계산량 큼, 느림   |
| SGD           | 1개 샘플로 갱신        | 빠름, 메모리 적음      | 진동 심함       |
| Mini-Batch GD | 소규모 배치로 갱신       | 속도·안정성 균형       | 배치 크기 선택 필요 |
| Momentum      | 이전 기울기 고려        | 지역 최소값 탈출 유리    | 관성 크기 조절 필요 |
| NAG           | 미래 위치 기울기 고려     | 더 정확, 빠른 수렴     | 구현 복잡       |
| AdaGrad       | 특성별 학습률 조정       | 희소 데이터에 효과적     | 학습률 소실 문제   |
| RMSProp       | 지수 이동 평균 반영      | AdaGrad 개선, 안정적 | 하이퍼파라미터 민감  |
| Adam          | Momentum+RMSProp | 널리 사용, 성능 우수    | 일부 상황 과적합   |

---

# 📌 전체 요약

* **기본 방식**: GD, SGD, Mini-Batch GD
* **개선 방식**: Momentum, NAG (업데이트 방향 개선)
* **적응형 학습률**: AdaGrad, RMSProp, Adam (학습률 조절)
* 실제 딥러닝에서는 **Mini-Batch + Adam**이 가장 보편적으로 쓰임.
* 하지만 특정 상황(예: 희소 데이터, NLP, 컴퓨터 비전 등)에서는 AdaGrad, RMSProp, Momentum이 더 유리할 수도 있음.

---

