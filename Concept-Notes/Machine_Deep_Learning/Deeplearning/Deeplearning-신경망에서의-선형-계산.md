
---

# 📐 신경망에서의 선형 계산 (Linear Transformation)

## 1. 선형 계산이란?

* **정의**: 입력 벡터와 가중치를 곱하고 편향을 더하는 연산
  [
  z = Wx + b
  ]
* 특징:

  * **곱셈 + 덧셈**만 포함 → 직선/평면 관계로 표현 가능
  * 1차식 (y = ax+b) 형태와 동일 → 직선 그래프
  * 그래서 "선형(linear)"이라 부름

---

## 2. 신경망에서의 선형 계산

* **L번째 층에서의 선형 계산**
  [
  z^{[L]} = W^{[L]} a^{[L-1]} + b^{[L]}
  ]

  * (W^{[L]}): 가중치 행렬
  * (a^{[L-1]}): 이전 층의 출력 (입력 데이터 포함)
  * (b^{[L]}): 편향 벡터
  * (z^{[L]}): **pre-activation 값**

* **활성화 적용**
  [
  a^{[L]} = f(z^{[L]})
  ]

  * (f): 활성화 함수 (ReLU, Sigmoid, Tanh 등)
  * (a^{[L]}): **post-activation 값**

---

## 3. 선형과 비선형의 조합

* **문제점**:

  * 선형 계산만 여러 층을 쌓아도 결국 또 다른 선형 변환이 됨
    → 아무리 층을 깊게 쌓아도 단일 직선 관계 이상을 학습 불가
* **해결책**:

  * 비선형 함수 (f) 추가 → 복잡한 패턴(곡선, 비선형 경계) 학습 가능
  * 즉, **신경망 = 선형 계산 + 비선형 변환의 반복**

---

## 4. 활성화 함수의 역할

* **ReLU**: (f(x) = \max(0, x)) → 기울기 소실 문제 완화, 가장 널리 사용
* **Sigmoid**: ([0,1]) 범위로 압축 → 확률적 해석에 유용하지만 깊은 층에서는 기울기 소실
* **Tanh**: ([-1,1]) 범위로 압축 → 음/양 모두 표현 가능, RNN에서 자주 사용

---

## 5. 용어 정리

* **pre-activation ((z^{[L]}))**: 선형 계산만 적용한 결과
* **post-activation ((a^{[L]}))**: 활성화 함수를 거친 최종 출력
* 이 두 단계를 통해 신경망은 입력을 변환하며 점점 복잡한 특징을 학습함

---

## 6. 핵심 요약

* 선형 계산은 **데이터 변환의 기본 틀**
* 비선형 활성화는 **복잡한 패턴 학습의 열쇠**
* 따라서 신경망은 **“선형 + 비선형”의 반복 구조**로 동작한다.

---

