

---

# 🧩 가중치 초기화(Weight Initialization)

## 1. 가중치 초기화란?

* 신경망 학습을 시작할 때 **가중치(Weight)와 편향(Bias)의 처음 값**을 정해주는 과정
* 즉, **학습이 출발하는 시작 위치**를 설정하는 것

---

## 2. 왜 중요한가?

1. **학습 시작점 결정**

   * 손실 함수(loss function) 최적화는 “곡면 위에서 출발해 경사하강으로 내려가는 과정”
   * 가중치 초기화는 그 **출발점 좌표**를 정하는 것

2. **대칭성 깨기(Symmetry Breaking)**

   * 모든 가중치를 0으로 초기화하면 → 모든 뉴런이 똑같이 업데이트됨 → 학습 불가능
   * 따라서 랜덤 요소가 꼭 필요함

3. **Gradient 안정화**

   * 가중치가 너무 크면 → **Gradient Exploding** (값이 폭발)
   * 가중치가 너무 작으면 → **Gradient Vanishing** (값이 0으로 사라짐)
   * 초기화는 이 문제를 줄이고 안정적인 학습을 돕는다

---

## 3. 대표적인 초기화 방법

### (1) Zero Initialization

* `nn.init.zeros_(m.weight)`
* 모든 weight를 0으로 초기화
* ❌ 학습 불가 (뉴런들이 같은 업데이트 → 대칭성 문제 발생)

---

### (2) Normal / Gaussian Initialization

* `nn.init.normal_(m.weight, mean=0, std=0.01)`
* 정규분포에서 무작위 샘플링
* 단순하지만 깊은 네트워크에선 불안정할 수 있음

---

### (3) Xavier Initialization (Glorot)

* `nn.init.xavier_normal_(m.weight)`
* 입력 노드 수 + 출력 노드 수 기반으로 분산 조절
* Sigmoid, Tanh 같은 활성화 함수와 잘 맞음
* 목표: **입출력 분산이 비슷하게 유지되도록**

---

### (4) Kaiming Initialization (He)

* `nn.init.kaiming_normal_(m.weight)`
* ReLU, LeakyReLU 계열 활성화 함수에 최적화
* 음수는 다 잘라내는 ReLU 특성을 고려해 분산을 조정

---

### (5) Bias 초기화

* 보통 `nn.init.zeros_(m.bias)`로 초기화
* 이유: bias는 단순히 평행이동 역할이라 0에서 시작해도 문제 없음
* 학습 도중 금방 업데이트됨

---

## 4. 요약

* **가중치 초기화 = 학습의 출발점**
* 목적:

  * 대칭성 깨기
  * gradient 안정화
  * 수렴 속도 개선
* 일반적으로:

  * Sigmoid/Tanh → **Xavier 초기화**
  * ReLU 계열 → **Kaiming 초기화**
  * Bias → 보통 **0**

---

👉 정리하면, **“좋은 초기화 = 좋은 출발점”**
학습이 빨라지고 안정적이며, 더 높은 성능으로 수렴할 가능성이 커짐.

---

---

# ❌ 모든 가중치를 0으로 초기화했을 때 생기는 문제

## 1. 대칭성 문제 (Symmetry Problem)

* 모든 뉴런이 **같은 입력 → 같은 출력**을 내게 됨.
* 역전파(backpropagation) 시 gradient도 모두 동일하게 계산됨.
* 결과적으로 모든 뉴런이 **똑같이 업데이트**되어, 여러 뉴런을 둔 의미가 사라짐.
* 결국 모델은 **한 개의 뉴런만 있는 단순 선형 모델**처럼 동작하게 됨.

---

## 2. 학습 불능 (No Learning)

* 처음부터 같은 값으로 시작 → 학습 과정에서 뉴런 간 차이가 생기지 않음.
* 층(layer)을 여러 개 쌓아도 **각 층이 하는 일이 똑같아져서 표현력이 상실**됨.
* 따라서 복잡한 패턴을 학습하지 못하고 **성능이 극도로 제한**됨.

---

## 3. Bias도 0이면?

* 보통 bias는 0으로 초기화해도 괜찮지만,
* weight까지 0이면 gradient가 전부 같은 방향으로만 업데이트됨 → **의미 있는 파라미터 학습 불가**.

---

## 4. 예시 상황

* XOR 문제처럼 **비선형 관계**를 학습해야 하는 경우,
* 모든 weight를 0으로 초기화하면 → 모델은 직선만 그릴 수 있어서 문제를 전혀 풀 수 없음.

---

## 5. 해결 방법

* **랜덤 초기화(Random Initialization)**: 최소한 weight는 랜덤하게 시작해야 대칭성이 깨짐.
* **Xavier Initialization**: Sigmoid/Tanh와 잘 맞음.
* **Kaiming Initialization**: ReLU 계열과 잘 맞음.
* Bias는 보통 0으로 두어도 학습 과정에서 금방 업데이트되므로 문제가 되지 않음.

---

## ✅ 요약

* 모든 가중치를 0으로 초기화하면

  * 뉴런들이 전부 같은 역할만 해서
  * 학습이 진행되지 않고
  * 모델은 단순한 선형 모델로 퇴화함.
* 따라서 weight는 반드시 **적절히 랜덤 초기화**해야 하고, bias만 0으로 두는 것이 일반적.

---

👉 정리하면:
**"가중치 초기화 = 대칭성을 깨고, 학습을 가능하게 하는 출발점"**
0으로 초기화하면 대칭성이 유지되어 → 학습이 멈추는 거야.

---



