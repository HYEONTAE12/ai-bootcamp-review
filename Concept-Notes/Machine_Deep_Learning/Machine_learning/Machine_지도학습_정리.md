
---

# 📘 지도학습(Supervised Learning) 요약노트

## 1. 지도학습 개요

* **정의**: 입력(X)과 정답(y, 레이블)을 가지고 학습하는 방식
* **목표**: 새로운 X가 들어오면 y를 예측

---

## 2. 지도학습의 세부 분류

### 🔹 회귀 (Regression)

* **y가 연속형 변수**일 때 사용
* 예: 집값 예측, 키 → 몸무게
* **손실함수**: MSE (Mean Squared Error)

  * 오차 제곱을 사용 → 큰 오차에 더 민감

👉 **해법**

* 해석적 해법: 수식으로 직접 최적해 계산 (정규방정식)
* 수치적 해법: 경사하강법(Gradient Descent)으로 근사

---

### 🔹 분류 (Classification)

* **y가 범주형 변수**일 때 사용
* 이진분류: 예/아니오 (스팸메일 여부)
* 다중분류: 여러 클래스 (손글씨 0\~9)

**손실함수**

* 크로스 엔트로피 (Cross Entropy)

  * 정답일 확률이 낮으면 손실 ↑ (강한 패널티)
  * 시그모이드/소프트맥스와 결합 시 미분 단순화

---

## 3. 주요 알고리즘

### 🌳 결정트리 (Decision Tree)

* 질문을 통해 데이터를 분할 → 리프 노드에서 예측
* **불순도(Impurity)**

  * 클래스가 섞여 있으면 ↑
  * 한 클래스만 있으면 ↓

---

### 🌲 랜덤포레스트 (Random Forest)

* 여러 결정트리를 독립적으로 학습 후 **투표/평균**
* 장점: 안정적, 과적합 완화
* 단점: 해석 어려움, 계산량 많음

---

### 👥 KNN (K-Nearest Neighbors)

* 새로운 데이터 → 기존 데이터와 거리 계산 → 가장 가까운 K개 확인 → 다수결 예측
* K 작음 → 과적합
* K 큼 → 과소적합
* 학습은 “데이터 저장”만, 계산은 예측 시점에 수행 (**게으른 학습**)

---

### ⚖️ SVM (Support Vector Machine)

* **마진 최대화**: 두 클래스를 가장 잘 나누는 결정경계 찾기
* **서포트 벡터**: 경계와 가장 가까운 데이터 포인트
* **커널 트릭**: 저차원에서 분리 불가능한 데이터 → 고차원으로 옮겨 선형 분리

---

## 4. 앙상블 학습 (Ensemble Learning)

### 🔹 배깅 (Bagging)

* 여러 모델을 병렬로 학습 후 평균/투표
* 예: 랜덤포레스트

### 🔹 부스팅 (Boosting)

* 여러 모델을 순차적으로 학습, 앞 모델의 오차를 보완
* 예: Gradient Boosting, XGBoost, LightGBM
* 장점: 높은 성능, 복잡한 패턴 잘 잡음
* 단점: 노이즈에 민감, 과적합 위험

---

## 5. 상관관계 분석

* **상관관계**: 두 변수가 함께 변하는 경향
* **인과관계**: 한 변수가 다른 변수에 원인/결과로 작용

📌 **상관관계 분석의 4가지 기본 가정**

1. 선형성 (Linearity)
2. 등분산성 (Homoscedasticity)
3. 정규성 (Normality)
4. 독립성 (Independence)

---

# 📝 한눈에 요약

* **회귀**: 연속형 y, 손실함수=MSE
* **분류**: 범주형 y, 손실함수=Cross Entropy
* **결정트리**: 불순도 ↓, 직관적
* **랜덤포레스트**: 안정성 ↑, 과적합 완화
* **KNN**: 거리 기반, K 값 조정 중요
* **SVM**: 마진 최대화, 서포트 벡터, 커널 트릭
* **앙상블**: 배깅=안정성 / 부스팅=정밀성
* **상관관계**: ≠ 인과관계, 4가지 가정 확인 필요

---

