
---

# ğŸ“ ë¶„ë¥˜ë¬¸ì œ ëª¨ë¸ë§ ë°©ë²•ë¡  ì •ë¦¬

## 1. ì‹¤ìŠµ ê°œìš”

* ë°ì´í„°ì…‹: **ìœ„ìŠ¤ì½˜ì‹  ìœ ë°©ì•” ë°ì´í„°ì…‹ (Breast Cancer Wisconsin)**
* ëª©í‘œ: í™˜ìì˜ ì¢…ì–‘ì´ \*\*ì•…ì„±(malignant, ì•”)\*\*ì¸ì§€ \*\*ì–‘ì„±(benign, ì•” ì•„ë‹˜)\*\*ì¸ì§€ ë¶„ë¥˜í•˜ëŠ” **ì´ì§„ë¶„ë¥˜ ë¬¸ì œ**
* í”¼ì²˜: 30ê°œ
* ìƒ˜í”Œ: 569ê°œ

---

## 2. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°ì™€ í™•ì¸

```python
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

print(cancer["data"].shape)       # (569, 30) â†’ 569ê°œ ìƒ˜í”Œ, 30ê°œ íŠ¹ì§•
print(cancer["feature_names"])    # íŠ¹ì§• ì´ë¦„
print(cancer["target_names"])     # ['malignant' 'benign']
```

* `0` â†’ malignant (ì•…ì„±, ì•”)
* `1` â†’ benign (ì–‘ì„±, ì•” ì•„ë‹˜)

ğŸ‘‰ ì¼ë°˜ì ì¸ ì§ê´€(0=ì•” ì•„ë‹˜, 1=ì•”)ê³¼ ë°˜ëŒ€ì´ë¯€ë¡œ ì£¼ì˜!

---

## 3. ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression)

### ê¸°ë³¸ ê°œë…

* ì¶œë ¥: **0 ë˜ëŠ” 1 í™•ë¥ **
* ê²°ì • ê²½ê³„: **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜**ë¥¼ ì´ìš©í•´ í™•ë¥ ë¡œ í•´ì„
* **intercept (ì ˆí¸)** = ë°”ì´ì–´ìŠ¤ $b$
* **coef (ê³„ìˆ˜)** = ê° í”¼ì²˜ì˜ ê¸°ìš¸ê¸° $w$

### í•œê³„ê°’(threshold)

```python
probs = model.predict_proba(X_test)[:, 1]
prediction = (probs > 0.5).astype(int)
```

* ê¸°ë³¸ê°’: 0.5
* threshold â†‘ (0.7 ë“±): ì•” íŒì •ì„ ì—„ê²©íˆ â†’ False Positiveâ†“, False Negativeâ†‘
* threshold â†“ (0.3 ë“±): ì•” íŒì •ì„ ì‰½ê²Œ â†’ False Negativeâ†“, False Positiveâ†‘

ğŸ‘‰ ì˜ë£Œ ë°ì´í„°ì—ì„œëŠ” ë³´í†µ **False Negative(ì•”ì¸ë° ë†“ì¹¨)** ì¤„ì´ëŠ” ê²Œ ë” ì¤‘ìš” â†’ thresholdë¥¼ ë‚®ê²Œ ì¡ìŒ

---

## 4. ê²°ì •íŠ¸ë¦¬ (Decision Tree)

### ë¶ˆìˆœë„(impurity)

* **ì§€ë‹ˆì§€ìˆ˜(Gini Index)** ë˜ëŠ” \*\*ì—”íŠ¸ë¡œí”¼(Entropy)\*\*ë¡œ ë…¸ë“œì˜ ìˆœë„ë¥¼ ì¸¡ì •
* íŠ¸ë¦¬ëŠ” **ë¶ˆìˆœë„ë¥¼ ê°€ì¥ ë§ì´ ì¤„ì´ëŠ” í”¼ì²˜**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 

### ì‹œê°í™”

```python
from sklearn.tree import export_graphviz
export_graphviz(dec_tree, out_file="tree.dot",
                class_names=cancer["target_names"],
                feature_names=cancer["feature_names"],
                impurity=True, filled=True)
```

* Graphvizë¡œ **íŠ¸ë¦¬ êµ¬ì¡° ê·¸ë¦¼** ì¶œë ¥
* ë…¸ë“œë§ˆë‹¤ ì‚¬ìš©í•œ feature, threshold, ìƒ˜í”Œ ìˆ˜, ë¶ˆìˆœë„ í‘œì‹œ

### Feature Importance

```python
print(dec_tree.feature_importances_)
```

* ê° í”¼ì²˜ê°€ ë¶ˆìˆœë„ ê°ì†Œì— ê¸°ì—¬í•œ ì •ë„
* í•© = 1
* ê°’ì´ í´ìˆ˜ë¡ ë” ì¤‘ìš”í•œ feature
* ì˜ˆ: `"worst concave points"`ê°€ ë†’ì€ ì¤‘ìš”ë„ë¥¼ ê°€ì§

ğŸ‘‰ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”:

```python
plt.barh(np.arange(n_features), dec_tree.feature_importances_)
plt.yticks(np.arange(n_features), cancer["feature_names"])
```

---

## 5. ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  (SVM)

### ê¸°ë³¸ ê°œë…

* ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ë‚˜ëˆ„ëŠ” \*\*ì´ˆí‰ë©´(hyperplane)\*\*ì„ ì°¾ìŒ
* ì´ˆí‰ë©´:

  * 2D â†’ ì§ì„ 
  * 3D â†’ í‰ë©´
  * Nì°¨ì› â†’ ê³ ì°¨ì› í‰ë©´
* ë§ˆì§„(margin): ì´ˆí‰ë©´ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì (ì„œí¬íŠ¸ ë²¡í„°) ì‚¬ì´ ê±°ë¦¬

### ì„ í˜• SVM

```python
clf = SVC(kernel='linear', C=1.0)
clf.fit(X, y)
```

* `C` ê°’: ê·œì œ ê°•ë„

  * C â†‘ â†’ ë§ˆì§„ ì¢ê²Œ, ì˜¤ì°¨ ì ê²Œ (ê³¼ì í•© ìœ„í—˜â†‘)
  * C â†“ â†’ ë§ˆì§„ ë„“ê²Œ, ì˜¤ì°¨ í—ˆìš© (ì¼ë°˜í™”â†‘)

### ê²°ì • ê²½ê³„ ì‹œê°í™”

```python
Z = clf.decision_function(xy).reshape(XX.shape)
ax.contour(XX, YY, Z, levels=[-1,0,1], linestyles=['--','-','--'])
```

* 0: ê²°ì •ê²½ê³„
* Â±1: ë§ˆì§„ì„ 
* support\_vectors\_: ë§ˆì§„ì— ë‹¿ì€ ìƒ˜í”Œë“¤

---

## 6. ì£¼ìš” ê°œë… ìš”ì•½

* **Intercept(bias)**: ëª¨ë“  feature=0ì¼ ë•Œì˜ ê¸°ë³¸ê°’
* **Coef(weights)**: featureê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
* **Standardization(í‘œì¤€í™”)**: í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜ â†’ í•™ìŠµ ì•ˆì •í™”
* **Threshold**: ë¶„ë¥˜ í™•ë¥  ê¸°ì¤€ì , ë¯¼ê°ë„/íŠ¹ì´ë„ ì¡°ì ˆ
* **Feature Importance**: íŠ¸ë¦¬ ëª¨ë¸ì—ì„œ ê° featureì˜ ì˜í–¥ë ¥
* **Hyperplane**: ë¶„ë¥˜ ê²½ê³„, SVMì˜ í•µì‹¬
* **Support Vectors**: ë§ˆì§„ ê²½ê³„ì— ìœ„ì¹˜í•œ ê²°ì •ì  ìƒ˜í”Œ

---

