
---

# 📘 변수 전처리 정리

## 1. 연속형 변수 변환

### 🔹 로그 변환 (Log Transformation)

* **정의**: 비대칭적인 분포를 정규분포에 가깝게 변환하는 기법
* **효과**:

  * 데이터의 정규성 확보 → 선형 회귀, t-검정, ANOVA 등 가정 기반 모델에 유리
  * 큰 값을 줄이고 작은 값을 상대적으로 키워줌 → 스케일 균형
* **주의사항**:

  * 값이 **0 이하인 경우** 로그 변환 불가능 → `log(x+1)`과 같은 변형 필요

---

### 🔹 제곱근 변환 (Square Root Transformation)

* **정의**: 값의 제곱근을 취하는 변환
* **특징**:

  * 로그 변환보다 완만하게 편차를 줄임
  * 분산이 크지 않은 데이터에서 효과적
* **사용 예시**: 포아송 분포 형태(카운트 데이터)

---

### 🔹 Box-Cox 변환

* **정의**: 람다(λ) 파라미터에 따라 다양한 형태의 변환을 수행
* **장점**:

  * λ=0 → 로그 변환
  * λ=0.5 → 제곱근 변환
  * 데이터에 맞는 최적 λ를 찾을 수 있음
* **제한**:

  * **데이터가 양수여야만 사용 가능**

---

### 🔹 Yeo-Johnson 변환

* **정의**: Box-Cox의 확장판 → 음수 값도 처리 가능
* **장점**:

  * 데이터 범위 제약이 없음
  * 사이킷런의 `PowerTransformer`에서 지원

---

## 2. 스케일링 (Scaling)

### 왜 필요한가?

* 변수마다 단위와 범위가 다르면 모델이 특정 변수에 과도하게 의존
* 거리 기반 알고리즘(KNN, SVM, K-Means 등)에서 특히 중요

---

### 🔹 Min-Max 스케일링

* **정의**: 모든 값을 0\~1 범위로 변환
* **공식**:

  $$
  x' = \frac{x - x_{min}}{x_{max} - x_{min}}
  $$
* **특징**:

  * 값의 상대적 크기를 보존
  * 이상치(Outlier)에 매우 민감

---

### 🔹 표준화 (Standardization, Z-score Normalization)

* **정의**: 평균을 0, 표준편차를 1로 맞춤
* **공식**:
| 방법                        | 수식                             |
| ------------------------- | ------------------------------ |
| **표준화 (Standardization)** | $x' = \dfrac{x - \mu}{\sigma}$ |

 **특징**:

  * 정규분포 가정 모델(선형회귀, 로지스틱 회귀, PCA 등)에 적합

---

### 🔹 로버스트 스케일링 (Robust Scaling)

* **정의**: 중앙값과 IQR(사분위 범위)을 기준으로 스케일링
* **공식**:

| 방법                             | 수식                             |
| ------------------------------ | ------------------------------ |
| **로버스트 스케일링 (Robust Scaling)** | $x' = \dfrac{x - Q2}{Q3 - Q1}$ |

 **특징**:

  * 이상치 영향 최소화
  * 데이터 분포가 왜도(skewness)를 가질 때 유용

---

## 3. 구간화 (Binning)

* **정의**: 연속형 변수를 범주형 변수로 변환
* **방법**:

  * **등간격 구간화**: 동일한 구간 크기로 나눔
  * **등빈도 구간화**: 동일한 데이터 개수로 나눔
* **장점**:

  * 모델 해석력 향상
  * 이상치 영향 감소
* **단점**:

  * 정보 손실 가능성

---

## 4. 범주형 변수 변환

### 🔹 원-핫 인코딩 (One-Hot Encoding)

* **정의**: 범주를 이진 벡터(0/1)로 변환
* **장점**:

  * 순서 정보 왜곡 없음
  * 대부분의 알고리즘에서 바로 사용 가능
* **단점**:

  * 범주 수가 많으면 차원이 급격히 증가 (**차원의 저주**)

---

### 🔹 레이블 인코딩 (Label Encoding)

* **정의**: 각 범주를 정수 값으로 매핑
* **장점**:

  * 단일 변수로 관리 → 메모리 효율
* **단점**:

  * 숫자의 크기/순서가 의미 없는 경우 왜곡 발생

---

### 🔹 빈도 인코딩 (Frequency Encoding)

* **정의**: 각 범주의 등장 빈도를 값으로 변환
* **장점**:

  * 데이터 분포 반영
  * 단일 변수로 관리 가능
* **단점**:

  * 다른 범주라도 빈도가 같으면 같은 값으로 인식

---

### 🔹 타겟 인코딩 (Target Encoding)

* **정의**: 각 범주를 타겟 변수의 평균 값으로 변환
* **장점**:

  * 범주 간 수치적 의미 부여
  * 단일 변수로 관리 가능
* **단점**:

  * **과적합 위험** (특히 범주 수가 적을 때)
  * **데이터 누출(Data Leakage)** 위험

#### 과적합 방지 방법

* **스무딩**: 전체 평균과 섞어서 안정화
* **K-Fold 인코딩**: 교차 검증 방식으로 인코딩 수행

---

## 5. 관련 라이브러리

* **scikit-learn**

  * `OneHotEncoder`, `LabelEncoder`, `StandardScaler`, `MinMaxScaler`, `RobustScaler`, `PowerTransformer`
* **category\_encoders**

  * 다양한 인코딩 기법 지원 (`TargetEncoder`, `WOEEncoder`, `HelmertEncoder` 등)
  * scikit-learn과 호환되는 API 제공

---

