
---

# ğŸ“š ì˜¤ëŠ˜ ì •ë¦¬í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ & ì½”ë“œ

## 1. **Pandas (`pd`)**

ë°ì´í„° ë¶„ì„ì˜ í•µì‹¬ ë„êµ¬. CSV ë¶ˆëŸ¬ì˜¤ê¸°, ê°€ê³µ, ì§‘ê³„, ì‹œê°í™” ë³´ì¡°ì— ìì£¼ ì“°ì„.

### ì£¼ìš” ê¸°ëŠ¥

```python
pd.date_range('2025-08-27', periods=4, freq='10S')  
# 2025-08-27ë¶€í„° ì‹œì‘í•´ 10ì´ˆ ê°„ê²©ìœ¼ë¡œ 4ê°œì˜ ì‹œê³„ì—´ ìƒì„±

df.resample('5S').mean()  
# ì‹œê³„ì—´ ë°ì´í„°ë¥¼ 5ì´ˆ ë‹¨ìœ„ë¡œ ë¦¬ìƒ˜í”Œë§í•´ í‰ê·  ê³„ì‚°

df.interpolate()  
# ê²°ì¸¡ê°’ì„ ì„ í˜• ë³´ê°„ìœ¼ë¡œ ì±„ì›€

pd.concat([df_target,
           pd.DataFrame([{'bysm' : 'total'}])],
          ignore_index=True)
# í–‰ì„ í•©ì³ì„œ ìƒˆë¡œìš´ DataFrame ìƒì„±

groupby('a', as_index=False)['b'].sum()
# ê·¸ë£¹í™” í›„ í•©ê³„ ì§‘ê³„
```

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„
* ê²°ì¸¡ê°’ ì²˜ë¦¬
* ì—¬ëŸ¬ DataFrameì„ í•©ì¹  ë•Œ
* ê³ ê°/ìƒí’ˆ/ê²½ê¸°ë³„ ì§‘ê³„

---

## 2. **NumPy (`np`)**

ìˆ˜ì¹˜ ì—°ì‚°, ë°°ì—´ ì²˜ë¦¬ì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬.

### ì‚¬ìš© ì˜ˆ

```python
mask = np.triu(np.ones_like(df_clean.corr(), dtype=bool))
# ìƒì‚¼ê°í–‰ë ¬ mask ìƒì„± (íˆíŠ¸ë§µ ëŒ€ì¹­ ë¶€ë¶„ ì œê±°ìš©)
```

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* í–‰ë ¬/ë²¡í„° ì—°ì‚°
* ë§ˆìŠ¤í¬(mask) ë§Œë“¤ì–´ì„œ ì¡°ê±´ í•„í„°ë§í•  ë•Œ
* ìˆ˜ì¹˜ ì‹œë®¬ë ˆì´ì…˜

---

## 3. **Matplotlib (`plt`)**

ê¸°ë³¸ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. ì„¸ë°€í•œ ì œì–´ ê°€ëŠ¥.

### ì‚¬ìš© ì˜ˆ

```python
%matplotlib inline     # ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ê·¸ë˜í”„ ë°”ë¡œ ë³´ì´ê²Œ
plt.figure(figsize=(16, 12))  
plt.show()             # ê·¸ë˜í”„ ì¶œë ¥
```

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* ê·¸ë˜í”„ í¬ê¸°, ì¶•, ë ˆì´ë¸”, ì œëª© ë“± ì„¸ë°€í•œ ì œì–´ê°€ í•„ìš”í•  ë•Œ

---

## 4. **Seaborn (`sns`)**

Matplotlib ê¸°ë°˜ì˜ ê³ ê¸‰ ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. ì½”ë“œ ê°„ê²°.

### ì‚¬ìš© ì˜ˆ

```python
sns.set_style('darkgrid')  

sns.heatmap(df_clean.drop('blueWins', axis=1).corr(),
            cmap='YlGnBu', annot=True, fmt='.2f', vmin=0, mask=mask)

g = sns.pairplot(data=df_clean,
                 vars=['blueKills','blueWardsPlaced','blueAssists','blueTotalGold'],
                 hue='blueWins', size=3, palette='Set1')
g.map_diag(plt.hist)          # ëŒ€ê°ì„  â†’ íˆìŠ¤í† ê·¸ë¨
g.map_offdiag(plt.scatter)    # ë¹„ëŒ€ê°ì„  â†’ ì‚°ì ë„
g.add_legend()                # ë²”ë¡€ ì¶”ê°€
```

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* ìƒê´€ê´€ê³„ ì‹œê°í™”
* ê·¸ë£¹ë³„ ë¶„í¬ ë¹„êµ
* íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)

---

## 5. **imblearn (imbalanced-learn)**

ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬.

### ì‚¬ìš© ì˜ˆ

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)
```

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* íƒ€ê²Ÿ ë°ì´í„° ë¶ˆê· í˜•ì¼ ë•Œ (ì˜ˆ: ìŠ¹ë¥  ë°ì´í„° 90:10 â†’ 50:50 ë§ì¶”ê¸°)

---

## 6. **Scikit-Learn (`sklearn`)**

ë¨¸ì‹ ëŸ¬ë‹ì˜ ëŒ€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬. ë°ì´í„° ì „ì²˜ë¦¬, ë¶„í• , ëª¨ë¸ë§, í‰ê°€ê¹Œì§€ ê°€ëŠ¥.

### (1) ë°ì´í„° ì „ì²˜ë¦¬

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X)            # ìµœì†Œê°’, ìµœëŒ€ê°’ í•™ìŠµ
X_scaled = scaler.transform(X)  # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
```

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* ë³€ìˆ˜ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ í´ ë•Œ (í‚¬ ìˆ˜=10, ê³¨ë“œ=20000)

---

### (2) ë°ì´í„° ë¶„í• 

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)
```

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* í•™ìŠµìš©/í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¶„ë¦¬í•´ì„œ ê³¼ì í•© ë°©ì§€í•  ë•Œ

---

### (3) ëª¨ë¸ë§

```python
from sklearn.naive_bayes import GaussianNB
clf_nb = GaussianNB()
clf_nb.fit(X_train, y_train)          # ëª¨ë¸ í•™ìŠµ
pred_nb = clf_nb.predict(X_test)      # ì˜ˆì¸¡
```

* **GaussianNB**: ì¡°ê±´ë¶€ í™•ë¥  ê¸°ë°˜ ë¶„ë¥˜ (ë‹¨ìˆœ, ë¹ ë¦„)

```python
from sklearn import tree
from sklearn.model_selection import GridSearchCV

dt = tree.DecisionTreeClassifier()
grid = {'min_samples_split': [5,10,20,50,100]}
clf_tree = GridSearchCV(dt, grid, cv=5)
clf_tree.fit(X_train, y_train)
pred_tree = clf_tree.predict(X_test)
```

* **DecisionTreeClassifier**: ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜, í•´ì„ ìš©ì´
* **GridSearchCV**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
grid = {'n_estimators': [100,200,300,400,500],
        'max_depth': [2,5,10]}
clf_rf = GridSearchCV(rf, grid, cv=5)
clf_rf.fit(X_train, y_train)
pred_rf = clf_rf.predict(X_test)
```

* **RandomForestClassifier**: ì•™ìƒë¸”(íŠ¸ë¦¬ ì—¬ëŸ¬ ê°œ) â†’ ì •í™•ë„â†‘, ê³¼ì í•©â†“

---

### (4) ì„±ëŠ¥ í‰ê°€

```python
from sklearn.metrics import accuracy_score

acc = accuracy_score(y_test, pred_rf)
print(acc)
```
**accuracy_score**: ì •í™•ë„ í‰ê°€

**ë‹¤ë¥¸ ì§€í‘œ**: confusion_matrix, classification_report, roc_auc_score

ğŸ‘‰ **ì–¸ì œ ì“°ë©´ ì¢‹ë‚˜?**

* ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ íƒ

## âœ… ì˜¤ëŠ˜ ë°°ìš´ íë¦„

* Pandas/NumPy â†’ ë°ì´í„° ì¤€ë¹„, ì§‘ê³„, ê²°ì¸¡ ì²˜ë¦¬

* Matplotlib/Seaborn â†’ ë¶„í¬Â·ìŠ¹ë¥ Â·ìƒê´€ê´€ê³„ ì‹œê°í™”

* SMOTE â†’ ë¶ˆê· í˜• ë°ì´í„° ë³´ì •

* Scikit-Learn â†’ ìŠ¤ì¼€ì¼ë§ â†’ ë°ì´í„° ë¶„í•  â†’ ë‹¤ì–‘í•œ ëª¨ë¸ í•™ìŠµ â†’ ì„±ëŠ¥ ë¹„êµ
