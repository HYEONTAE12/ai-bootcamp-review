
---

# 🧠 CNN 대화 정리

## 1. Convolution Layer (컨볼루션 레이어)

* **역할**: 이미지 위에 작은 필터(커널)를 슬라이딩하면서 곱셈/합산 → 특징(feature) 추출.
* **의미**: 이미지의 지역적인 패턴(에지, 모양 등)을 잡아냄.
* **포인트**: 층을 쌓아가며 저수준 → 고수준 특징까지 학습.

---

## 2. Pooling Layer (풀링 레이어)

* **역할**: 특징 맵(feature map)의 크기를 줄임.
* **종류**:

  * 맥스 풀링(Max Pooling): 영역 내 최대값 선택 → 가장 강한 특징 보존.
  * 평균 풀링(Average Pooling): 영역 내 평균값 선택 → 전체적인 흐름 유지.
* **이점**:

  * 연산량 감소 → 효율적 학습.
  * 노이즈에 덜 민감.
  * 오버피팅 방지에 기여.

---

## 3. Activation Function (활성화 함수)

* **필요성**: 네트워크에 **비선형성**을 부여 → 복잡한 패턴 학습 가능.
* **예시**:

  * ReLU: `max(0, x)` → 단순하면서도 gradient vanishing 문제 완화.
  * Sigmoid/Tanh: 0~1, -1~1 범위로 출력, 하지만 깊은 네트워크에선 vanishing 문제가 있음.

---

## 4. Batch Normalization (배치 정규화)

* **역할**: 각 미니배치 단위로 평균과 분산을 정규화 → 분포를 안정적으로 유지.
* **이점**:

  1. 그래디언트 폭발/소실 완화.
  2. 학습 속도 향상 → 더 큰 학습률 사용 가능.
  3. **내적 공변량 변화(Internal Covariate Shift)** 완화.

---

## 5. Flatten Layer (플래튼)

* **역할**: CNN이 추출한 2D/3D 형태의 특징 맵을 **1D 벡터로 펼침**.
* **이유**: Fully Connected Layer는 1차원 입력을 요구하기 때문.
* **효과**: CNN이 추출한 특징들을 분류기(head)에 전달해 최종 예측 가능.

---

## 6. Filter(커널) 크기 & 개수

* **필터 크기**:

  * 작을수록 → 미세한 특징(디테일) 포착.
  * 클수록 → 더 넓은 영역(맥락) 인식.
* **필터 개수**:

  * 많을수록 다양한 특징 학습 가능.
  * 하지만 연산량과 파라미터 수도 증가.

---

✅ **정리**
CNN은 **Convolution → Activation → Pooling → (BatchNorm) → Flatten → Fully Connected** 흐름으로 동작.

* **Backbone** 역할: 공통적인 시각 특징 추출.
* **Head** 역할: 태스크에 맞게 출력 (분류/탐지/세그멘테이션 등).

---


