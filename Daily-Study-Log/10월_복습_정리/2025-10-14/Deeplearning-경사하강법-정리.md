
---

# 🧭 경사하강법 (Gradient Descent)

---

## 1️⃣ 미분(Derivative)과 기울기(Gradient)

### 💡 미분의 개념

> “변수의 움직임에 따른 함수값의 변화를 측정하는 도구”

* 한 점에서의 **함수 변화율**을 나타냄.
* **기울기(gradient)**는 함수가 증가하거나 감소하는 방향과 속도를 알려줌.
* **최적화(Optimization)**에서 가장 많이 사용되는 수학적 도구.

### 📈 2차원 평면에서의 미분

* 함수 ( f(x) )의 **미분값**은 점 ( (x, f(x)) )에서의 **접선의 기울기**와 동일하다.
* 미분값이 **양수**면 함수는 증가하는 방향,
  **음수**면 감소하는 방향이다.
* 딥러닝에서는 손실을 **줄이기 위해**,
  함수가 **커지는 방향의 반대 방향**으로 이동한다.

---

## 2️⃣ 편미분(Partial Derivative)과 그래디언트 벡터

### 💡 편미분이란?

* 여러 변수를 가진 다변수 함수 ( f(x_1, x_2, ..., x_n) )에서
  한 변수만 바꾸고 나머지는 고정한 상태에서의 변화율.
* 각 변수에 대해 하나씩 미분을 수행 → **각 변수의 기울기**를 알 수 있다.

### 💡 그래디언트(Gradient Vector)

> 모든 편미분 값을 모은 벡터.
> 즉, **함수가 가장 빠르게 증가하는 방향**을 알려주는 벡터다.

[
\nabla f(x, y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)
]

* 딥러닝에서는 **손실 함수의 그래디언트**를 이용해,
  그 반대 방향으로 이동하며 손실을 줄인다.

---

## 3️⃣ 경사하강법 (Gradient Descent)

### 💡 개념 요약

> “기울기가 감소하는 방향으로 x를 움직여서 함수 f(x)의 최소값을 찾는 알고리즘”

* **최적화 알고리즘의 기본 구조**
* 손실 함수의 기울기(Gradient)를 이용해 파라미터를 점진적으로 업데이트함.
* 수식:
  [
  \theta = \theta - \alpha \frac{\partial L}{\partial \theta}
  ]

| 기호                                     | 의미                        |
| -------------------------------------- | ------------------------- |
| ( \theta )                             | 모델의 파라미터 (가중치, bias 등)    |
| ( L )                                  | 손실 함수 (Loss Function)     |
| ( \alpha )                             | 학습률(Learning Rate): 이동 크기 |
| ( \frac{\partial L}{\partial \theta} ) | 손실 함수의 기울기(gradient)      |

---

## 4️⃣ 경사하강법의 학습 과정

| 단계 | 설명                                                                         |
| -- | -------------------------------------------------------------------------- |
| ①  | 모델 파라미터 ( \theta = {W, b} ) 초기화                                            |
| ②  | 손실 함수 ( L ) 계산                                                             |
| ③  | 손실 함수의 기울기 ( \frac{\partial L}{\partial \theta} ) 계산                       |
| ④  | 파라미터 업데이트: ( \theta = \theta - \alpha \frac{\partial L}{\partial \theta} ) |
| ⑤  | 종료 조건(수렴 or 반복 횟수 도달) 검사                                                   |

이 과정을 반복하면서 모델은 점차 손실을 최소화하게 된다.

---

## 5️⃣ 학습률 (Learning Rate, α)

* 한 번에 이동하는 **스텝 크기**를 의미.
* 너무 **크면** → 최소점을 지나쳐 발산하거나 불안정함.
* 너무 **작으면** → 수렴은 하지만 너무 느림.

📉 적절한 학습률을 선택하는 것이 매우 중요하다.

---

## 6️⃣ 확률적 경사하강법 (Stochastic Gradient Descent, SGD)

> “모든 데이터를 한 번에 쓰지 않고, 일부 데이터로 자주 업데이트하는 방식”

| 방식                 | 설명                        |
| ------------------ | ------------------------- |
| **배치 경사하강법**       | 전체 데이터로 한 번에 경사 계산        |
| **확률적 경사하강법(SGD)** | 한 개의 샘플로 계산 후 즉시 업데이트     |
| **미니배치 경사하강법**     | 여러 샘플을 묶어서 업데이트 (일반적인 방식) |

🔹 장점:

* 계산 효율이 높고, 빠르게 수렴
* 자주 업데이트되어 **더 빠른 학습 가능**

---

## 🧩 복습문제 & 해설

### **Q1.**

함수의 미분값이 크다는 것은, 함수가 ( ) 방향으로 ( ) 속도로 변하고 있다는 뜻이다.
✅ **정답:** 증가 또는 감소 방향으로 빠르게 변하고 있다.

---

### **Q2.**

딥러닝에서 미분(기울기)을 사용하는 이유는?
✅ **정답:** 손실 함수의 변화율을 알아내어, 손실이 줄어드는 방향을 찾기 위해.

---

### **Q3.**

편미분을 사용하는 이유는?
✅ **정답:** 딥러닝의 손실 함수는 여러 파라미터(W, b 등)를 포함하므로,
각 변수별로 손실이 얼마나 변하는지 알아야 전체 방향(그래디언트)을 계산할 수 있기 때문이다.

---

### **Q4.**

그래디언트 벡터는 어떤 역할을 하나요?
✅ **정답:** 함수가 가장 빠르게 증가하는 방향을 나타내는 벡터이며,
손실을 최소화하기 위해 그 반대 방향으로 이동한다.

---

### **Q5.**

경사하강법의 수식 θ = θ - α ∂L/∂θ 에서 ‘–’ 부호는 왜 붙을까?
✅ **정답:** 기울기는 손실이 증가하는 방향을 가리키므로,
그 반대 방향(감소 방향)으로 이동해야 손실이 줄어들기 때문이다.

---

### **Q6.**

학습률(α)이 너무 크거나 작을 때의 문제는?
✅ **정답:**

* 너무 크면 최소점을 지나쳐 발산
* 너무 작으면 수렴은 하지만 너무 느리다.

---

### **Q7.**

경사하강법의 학습 단계 순서를 올바르게 나열하세요.
✅ **정답:**
1️⃣ 파라미터 초기화
2️⃣ 손실 계산
3️⃣ 기울기 계산
4️⃣ 파라미터 업데이트
5️⃣ 종료 조건 확인

---

### **Q8.**

확률적 경사하강법(SGD)의 장점은?
✅ **정답:**

* 연산 효율이 높고 자주 업데이트되어 빠르게 수렴함
* 큰 데이터셋에서도 계산 부담이 적음

---

## 🎯 핵심 요약

> **“경사하강법은 손실 함수의 기울기를 이용해, 손실이 줄어드는 방향으로 파라미터를 반복적으로 조정하여 최적의 해를 근사적으로 찾는 알고리즘이다.”**

---

