{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1789d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688e7a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c74059e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I can't understand why you wouldn't believe me!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a3ebce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'understand',\n",
       " 'why-you',\n",
       " 'would',\n",
       " \"n't\",\n",
       " 'believe',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 기준으로 토큰화\n",
    "# 부정을 정확하게 파악하기 위해서는 word_tokenize 사용\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'you',\n",
       " 'wouldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'believe',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단순한 분리 방식\n",
    "# 밀도 있는 분석에서는 word_tokenize()를 더 많이 사용\n",
    "# 장점 : 빠르고, 보기 편하다.\n",
    "punct_tokenizer = WordPunctTokenizer()\n",
    "punct_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "832a483d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'understand',\n",
       " 'why',\n",
       " 'you',\n",
       " 'would',\n",
       " \"n't\",\n",
       " 'believe',\n",
       " 'me',\n",
       " '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "# doesn't 와 같은 단어는 축약형으로 분리해준다.\n",
    "tree_tokenizer = TreebankWordTokenizer()\n",
    "tree_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad464140",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I can't believe it's already 2025! She said, This is a state-of-the-art technology. \n",
    "The well-known scientist doesn't agree with the theory. However, they've made significant progress. \n",
    "Dr. Smith earned $50,000 last year. The temperature was -5.5°C yesterday.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc41a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I can't believe it's already 2025!\",\n",
       " 'She said, This is a state-of-the-art technology.',\n",
       " \"The well-known scientist doesn't agree with the theory.\",\n",
       " \"However, they've made significant progress.\",\n",
       " 'Dr. Smith earned $50,000 last year.',\n",
       " 'The temperature was -5.5°C yesterday.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 점 단위로 분리하는 것이 아닌 내부적으로 규칙에 맞게 문장 단위로 분리한다.\n",
    "\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'just', 'ain', 'where', 'ma', 'weren', 'doing', 'their', 'hasn', 'have', 'then', 'we', 'most', 'a', 'themselves', 'for', 'by', \"weren't\", 'hadn', 'is', 'below', 'haven', 'while', 'does', 'this', 'isn', \"aren't\", 'who', 'y', 'because', 'the', 'as', 'with', 'which', 'its', 'o', \"hasn't\", 'mightn', 'did', 'off', \"they've\", \"should've\", 'nor', \"we've\", 'were', \"they'll\", 'it', \"it's\", 'than', 'no', 'out', 'above', \"hadn't\", 'wouldn', \"they'd\", \"you're\", 'aren', 'are', \"i'd\", 'needn', 'or', \"won't\", \"i'll\", \"she'll\", 'hers', 'more', 'd', 'once', 'don', 'do', 'will', 'why', 'am', \"i'm\", 'such', 'through', 'shan', 'what', \"wouldn't\", 'herself', 'again', 'how', \"isn't\", 'to', 'very', 'should', \"we'd\", 'here', \"wasn't\", 'now', \"she'd\", \"it'd\", 'up', 'himself', 'being', 'these', 'between', 's', 'of', 'theirs', 'myself', 'but', \"haven't\", \"he'll\", 'couldn', 'under', 'further', \"you've\", 'him', 'she', \"we're\", 'me', 'our', 'own', 'any', 'so', 'before', \"he's\", \"they're\", \"she's\", 'didn', 'having', \"i've\", 'other', 'there', 'be', 'been', \"mightn't\", 're', \"couldn't\", 'my', 'all', 'm', 'each', 'them', 'they', 'too', 'those', 'shouldn', 'yourselves', 'from', \"we'll\", 'about', 'both', \"you'll\", \"mustn't\", 'ours', \"don't\", 'and', \"needn't\", 'i', \"didn't\", 'your', 'at', 'won', 'against', 'few', 'mustn', 'after', 'an', 'had', 'same', 'was', 'he', 'his', 'ourselves', 'on', \"doesn't\", 't', 'doesn', 'that', 'll', 'her', 'in', 'some', 'you', 'into', 'until', 'yours', 'down', 'has', 'itself', \"it'll\", 'whom', 'yourself', 'only', 'over', \"shouldn't\", 'during', 'if', 'when', \"you'd\", \"that'll\", 'wasn', \"he'd\", 've', \"shan't\", 'can', 'not'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872d46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 :  ['I', 'want', 'to', 'go', 'to', 'shopping', 'and', 'a', 'I', 'want', 'to', 'buy', 'some', 'of', 'snack']\n",
      "불용어 미포함 :  ['want', 'go', 'shopping', 'want', 'buy', 'snack']\n"
     ]
    }
   ],
   "source": [
    "# nltk의 stopwords 는 전부 소문자로 이루어져 있기 때문에 lower() 함수로 token을 소문자 처리하는 것이 중요하다.\n",
    "# len(tok.lower()) > 1 : tok 단어를 소문자로 변환을 하고 길이가 1 보다 크다면 True\n",
    "# (tok.lower() not in stop) : tok 단어를 소문자로 변환을 하고 not in stop 이라는 객체안에 같은 단어가 없을 경우 True\n",
    "# 최종적으로 불용어들은 False 불용어가 아닌 단어들은 clean_token에 append 되어서 출력함\n",
    "\n",
    "sen = 'I want to go to shopping and a I want to buy some of snack'\n",
    "\n",
    "tokens = word_tokenize(sen)\n",
    "\n",
    "clean_token = []\n",
    "for tok in tokens:\n",
    "    if len(tok.lower()) > 1 and (tok.lower() not in stop):\n",
    "        clean_token.append(tok)\n",
    "\n",
    "print(\"불용어 : \", tokens)\n",
    "print(\"불용어 미포함 : \", clean_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 기법중에 하나인 stemming\n",
    "# rule 알고리즘으로 이루어지기 때문에 등록 되어있지 않은 단어의 경우에는 잘못인식되는 경우 부정확할 수 있는 여지가 있다.\n",
    "# 정규화를 위해서 word_tokenize() 함수를 사용해서 단어 단위로 토큰화를 하고, 각 단어의 stemming을 적용한다.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc1b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2025', '!', 'She', 'said', ',', 'This', 'is', 'a', 'state-of-the-art', 'technology', '.', 'The', 'well-known', 'scientist', 'does', \"n't\", 'agree', 'with', 'the', 'theory', '.', 'However', ',', 'they', \"'ve\", 'made', 'significant', 'progress', '.', 'Dr.', 'Smith', 'earned', '$', '50,000', 'last', 'year', '.', 'The', 'temperature', 'was', '-5.5°C', 'yesterday', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = PorterStemmer()\n",
    "text = \"\"\"I can't believe it's already 2025! She said, This is a state-of-the-art technology. \n",
    "The well-known scientist doesn't agree with the theory. However, they've made significant progress. \n",
    "Dr. Smith earned $50,000 last year. The temperature was -5.5°C yesterday.\"\"\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6862b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I => i\n",
      "ca => ca\n",
      "n't => n't\n",
      "believe => believ\n",
      "it => it\n",
      "'s => 's\n",
      "already => alreadi\n",
      "2025 => 2025\n",
      "! => !\n",
      "She => she\n",
      "said => said\n",
      ", => ,\n",
      "This => thi\n",
      "is => is\n",
      "a => a\n",
      "state-of-the-art => state-of-the-art\n",
      "technology => technolog\n",
      ". => .\n",
      "The => the\n",
      "well-known => well-known\n",
      "scientist => scientist\n",
      "does => doe\n",
      "n't => n't\n",
      "agree => agre\n",
      "with => with\n",
      "the => the\n",
      "theory => theori\n",
      ". => .\n",
      "However => howev\n",
      ", => ,\n",
      "they => they\n",
      "'ve => 've\n",
      "made => made\n",
      "significant => signific\n",
      "progress => progress\n",
      ". => .\n",
      "Dr. => dr.\n",
      "Smith => smith\n",
      "earned => earn\n",
      "$ => $\n",
      "50,000 => 50,000\n",
      "last => last\n",
      "year => year\n",
      ". => .\n",
      "The => the\n",
      "temperature => temperatur\n",
      "was => wa\n",
      "-5.5°C => -5.5°c\n",
      "yesterday => yesterday\n",
      ". => .\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(f\"{i} => {s.stem(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b46f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규화 기범중에 하나인 Lemmatization\n",
    "# 문장 속에서 다양한 형태의 굴절어를 단어의 표제인lemma를 찾는 일을 한다.\n",
    "# 단어의 원형 추출\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동사를 정의\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['walk', 'walked', 'walking', 'eat', 'ate', 'eating']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37213f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk => walk\n",
      "walked => walk\n",
      "walking => walk\n",
      "eat => eat\n",
      "ate => eat\n",
      "eating => eat\n"
     ]
    }
   ],
   "source": [
    "# 동사를 lematizer.lemmatize(i, pos='v') : 동사들을 반복을 돌리면서, lemmatize 에 i 와 단어가 동사, 품사라는 사실을 알려줄 수 있다.\n",
    "# 즉, 단어들이 문장에서 동사로 쓰였다는 것을 알고, 표제어 추출기는 정보를 보존하면서 정확한 lemma를 출력\n",
    "\n",
    "for i in words:\n",
    "    lemma = lemmatizer.lemmatize(i, pos='v')\n",
    "    print(f\"{i} => {lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1614673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit Distance\n",
    "# 2개의 문자열이 얼만큼 다른가를 거리개념으로 치환해 숫자로 표현한 것\n",
    "\n",
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bbab3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(edit_distance(\"come\", \"he\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2045314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c4cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8565c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PyTorch', 'Skip', 'to', 'main', 'content', 'Hit', 'enter', 'to', 'search', 'or', 'ESC', 'to', 'close', 'Search', 'Close', 'Search', 'search', 'Menu', 'Learn', 'Get', 'Started', 'Tutorials', 'Learn', 'the', 'Basics', 'PyTorch', 'Recipes', 'Intro', 'to', 'PyTorch', '–', 'YouTube', 'Series', 'Webinars', 'Community', 'Landscape', 'Join', 'the', 'Ecosystem', 'Community', 'Hub', 'Forums', 'Developer', 'Resources', 'Community', 'Events', 'PyTorch', 'Contributor', 'Awards', 'PyTorch', 'Ambassadors', 'Projects', 'PyTorch', 'vLLM', 'DeepSpeed', 'Host', 'Your', 'Project', 'RAY', 'Docs', 'PyTorch', 'Domains', 'Blog', '&', 'News', 'Blog', 'Announcements', 'Case', 'Studies', 'Events', 'Newsletter', 'About', 'PyTorch', 'Foundation', 'Members', 'Governing', 'Board', 'Technical', 'Advisory', 'Council', 'Cloud', 'Credit', 'Program', 'Staff', 'Contact', 'Brand', 'Guidelines', 'JOIN', 'github', 'search', 'Get', 'Started', 'Choose', 'Your', 'Path:', 'Install', 'PyTorch', 'Locally', 'or', 'Launch', 'Instantly', 'on', 'Supported', 'Cloud', 'Platforms', 'Get', 'started', 'October', '22,', '2025', 'in', 'Announcements,', 'Blog', 'PyTorch', 'Foundation', 'Welcomes', 'Ray', 'to', 'Deliver', 'a', 'Unified', 'Open', 'Source', 'AI', 'Compute', 'Stack', 'Ray', 'joins', 'leading', 'open', 'source', 'AI', 'projects', 'including', 'PyTorch', 'and', 'vLLM', 'to', 'minimize', 'AI', 'computing', 'complexity', 'and', 'speed', 'production', 'The', 'PyTorch', 'Foundation', 'has', 'welcomed', 'Ray', 'as', 'its', 'newest', 'foundation-hosted', 'project.…', 'Read', 'More', 'October', '22,', '2025', 'in', 'Announcements,', 'Blog', 'Dell', 'Technologies', 'Joins', 'the', 'PyTorch', 'Foundation', 'as', 'a', 'Premier', 'Member', 'The', 'PyTorch', 'Foundation,', 'a', 'community-driven', 'hub', 'supporting', 'the', 'open', 'source', 'PyTorch', 'framework', 'and', 'a', 'broader', 'portfolio', 'of', 'innovative', 'open', 'source', 'AI', 'projects,', 'is', 'announcing', 'today', 'that', 'Dell', 'Technologies', 'has', 'joined…', 'Read', 'More', 'October', '22,', '2025', 'in', 'Blog', 'Monarch', '+', 'Lightning', 'AI:', 'Unlocking', 'New', 'Possibilities', 'in', 'Distributed', 'Training', 'Introduction:', 'Empowering', 'the', 'Next', 'Generation', 'of', 'AI', 'Builders', 'We', 'are', 'excited', 'to', 'announce', 'a', 'partnership', 'between', 'the', 'PyTorch', 'Team', 'at', 'Meta,', 'leading', 'Monarch,', 'and', 'Lightning', 'AI,', 'that', 'combines', 'the', 'power…', 'Read', 'More', 'Join', 'PyTorch', 'Foundation', 'As', 'a', 'member', 'of', 'the', 'PyTorch', 'Foundation,', 'you’ll', 'have', 'access', 'to', 'resources', 'that', 'allow', 'you', 'to', 'be', 'stewards', 'of', 'stable,', 'secure,', 'and', 'long-lasting', 'codebases.', 'You', 'can', 'collaborate', 'on', 'training,', 'local', 'and', 'regional', 'events,', 'open-source', 'developer', 'tooling,', 'academic', 'research,', 'and', 'guides', 'to', 'help', 'new', 'users', 'and', 'contributors', 'have', 'a', 'productive', 'experience.', 'EXPLORE', 'BENEFITS', 'Key', 'Features', '&', 'Capabilities', 'Production', 'Ready', 'Transition', 'seamlessly', 'between', 'eager', 'and', 'graph', 'modes', 'with', 'TorchScript,', 'and', 'accelerate', 'the', 'path', 'to', 'production', 'with', 'TorchServe.', 'Distributed', 'Training', 'Scalable', 'distributed', 'training', 'and', 'performance', 'optimization', 'in', 'research', 'and', 'production', 'is', 'enabled', 'by', 'the', 'torch.distributed', 'backend.', 'Robust', 'Ecosystem', 'A', 'rich', 'ecosystem', 'of', 'tools', 'and', 'libraries', 'extends', 'PyTorch', 'and', 'supports', 'development', 'in', 'computer', 'vision,', 'NLP', 'and', 'more.', 'Cloud', 'Support', 'PyTorch', 'is', 'well', 'supported', 'on', 'major', 'cloud', 'platforms,', 'providing', 'frictionless', 'development', 'and', 'easy', 'scaling.', 'Install', 'PyTorch', 'Select', 'your', 'preferences', 'and', 'run', 'the', 'install', 'command.', 'Stable', 'represents', 'the', 'most', 'currently', 'tested', 'and', 'supported', 'version', 'of', 'PyTorch.', 'This', 'should', 'be', 'suitable', 'for', 'many', 'users.', 'Preview', 'is', 'available', 'if', 'you', 'want', 'the', 'latest,', 'not', 'fully', 'tested', 'and', 'supported,', 'builds', 'that', 'are', 'generated', 'nightly.', 'Please', 'ensure', 'that', 'you', 'have', 'met', 'the', 'prerequisites', 'below', '(e.g.,', 'numpy),', 'depending', 'on', 'your', 'package', 'manager.', 'Anaconda', 'is', 'our', 'recommended', 'package', 'manager', 'since', 'it', 'installs', 'all', 'dependencies.', 'You', 'can', 'also', 'install', 'previous', 'versions', 'of', 'PyTorch.', 'Note', 'that', 'LibTorch', 'is', 'only', 'available', 'for', 'C++.', 'NOTE:', 'Latest', 'Stable', 'PyTorch', 'requires', 'Python', '3.9', 'or', 'later.', 'Latest', 'Preview', '(Nightly)', 'PyTorch', 'requires', 'Python', '3.10', 'or', 'later.', 'PyTorch', 'Build', 'Your', 'OS', 'Package', 'Language', 'Compute', 'Platform', 'Run', 'this', 'Command:', 'PyTorch', 'Build', 'Stable', '(2.7.0)', 'Preview', '(Nightly)', 'Your', 'OS', 'Linux', 'Mac', 'Windows', 'Package', 'Pip', 'LibTorch', 'Source', 'Language', 'Python', 'C++', '/', 'Java', 'Compute', 'Platform', 'CUDA', '11.8', 'CUDA', '12.6', 'CUDA', '12.8', 'ROCm', '6.3', 'CPU', 'Run', 'this', 'Command:', 'pip3', 'install', 'torch', 'torchvision', 'torchaudio', '--index-url', 'https://download.pytorch.org/whl/cu118', 'Previous', 'versions', 'of', 'PyTorch', 'Quick', 'Start', 'With', 'Cloud', 'Partners', 'Get', 'up', 'and', 'running', 'with', 'PyTorch', 'quickly', 'through', 'popular', 'cloud', 'platforms', 'and', 'machine', 'learning', 'services.', 'Amazon', 'Web', 'Services', 'PyTorch', 'on', 'AWS', 'Amazon', 'SageMaker', 'AWS', 'Deep', 'Learning', 'Containers', 'AWS', 'Deep', 'Learning', 'AMIs', 'Google', 'Cloud', 'Platform', 'Cloud', 'Deep', 'Learning', 'VM', 'Image', 'Deep', 'Learning', 'Containers', 'Microsoft', 'Azure', 'PyTorch', 'on', 'Azure', 'Azure', 'Machine', 'Learning', 'Azure', 'Functions', 'Lightning', 'Studios', 'lightning.ai', 'Ecosystem', 'BROWSE', 'PROJECTS', 'Featured', 'Projects', 'Explore', 'a', 'rich', 'ecosystem', 'of', 'libraries,', 'tools,', 'and', 'more', 'to', 'support', 'development.', 'Captum', 'Captum', '(“comprehension”', 'in', 'Latin)', 'is', 'an', 'open', 'source,', 'extensible', 'library', 'for', 'model', 'interpretability', 'built', 'on', 'PyTorch.', 'PyTorch', 'Geometric', 'PyTorch', 'Geometric', 'is', 'a', 'library', 'for', 'deep', 'learning', 'on', 'irregular', 'input', 'data', 'such', 'as', 'graphs,', 'point', 'clouds,', 'and', 'manifolds.', 'skorch', 'skorch', 'is', 'a', 'high-level', 'library', 'for', 'PyTorch', 'that', 'provides', 'full', 'scikit-learn', 'compatibility.', 'Companies', '&', 'Universities', 'Using', 'PyTorch', 'Amazon', 'Advertising', 'Reduce', 'inference', 'costs', 'by', '71%', 'and', 'scale', 'out', 'using', 'PyTorch,', 'TorchServe,', 'and', 'AWS', 'Inferentia.', 'READ', 'CASE', 'STUDIES', 'Salesforce', 'Pushing', 'the', 'state', 'of', 'the', 'art', 'in', 'NLP', 'and', 'Multi-task', 'learning.', 'Stanford', 'University', 'Using', 'PyTorch’s', 'flexibility', 'to', 'efficiently', 'research', 'new', 'algorithmic', 'approaches.', 'Docs', 'Access', 'comprehensive', 'developer', 'documentation', 'for', 'PyTorch', 'View', 'Docs', '›', 'Tutorials', 'Get', 'in-depth', 'tutorials', 'for', 'beginners', 'and', 'advanced', 'developers', 'View', 'Tutorials', '›', 'Resources', 'Find', 'development', 'resources', 'and', 'get', 'your', 'questions', 'answered', 'View', 'Resources', '›', 'Stay', 'in', 'touch', 'for', 'updates,', 'event', 'info,', 'and', 'the', 'latest', 'news', 'By', 'submitting', 'this', 'form,', 'I', 'consent', 'to', 'receive', 'marketing', 'emails', 'from', 'the', 'LF', 'and', 'its', 'projects', 'regarding', 'their', 'events,', 'training,', 'research,', 'developments,', 'and', 'related', 'announcements.', 'I', 'understand', 'that', 'I', 'can', 'unsubscribe', 'at', 'any', 'time', 'using', 'the', 'links', 'in', 'the', 'footers', 'of', 'the', 'emails', 'I', 'receive.', 'Privacy', 'Policy.', 'x-twitterfacebooklinkedinyoutubegithubslackdiscordwechat', '©', '2025', 'PyTorch.', 'Copyright', '©', 'The', 'Linux', 'Foundation®.', 'All', 'rights', 'reserved.', 'The', 'Linux', 'Foundation', 'has', 'registered', 'trademarks', 'and', 'uses', 'trademarks.', 'For', 'more', 'information,', 'including', 'terms', 'of', 'use,', 'privacy', 'policy,', 'and', 'trademark', 'usage,', 'please', 'see', 'our', 'Policies', 'page.', 'Trademark', 'Usage.', 'Privacy', 'Policy.', 'Close', 'Menu', 'Learn', 'Get', 'Started', 'Tutorials', 'Learn', 'the', 'Basics', 'PyTorch', 'Recipes', 'Intro', 'to', 'PyTorch', '–', 'YouTube', 'Series', 'Webinars', 'Community', 'Landscape', 'Join', 'the', 'Ecosystem', 'Community', 'Hub', 'Forums', 'Developer', 'Resources', 'Community', 'Events', 'PyTorch', 'Contributor', 'Awards', 'PyTorch', 'Ambassadors', 'Projects', 'PyTorch', 'vLLM', 'DeepSpeed', 'Host', 'Your', 'Project', 'RAY', 'Docs', 'PyTorch', 'Domains', 'Blog', '&', 'News', 'Blog', 'Announcements', 'Case', 'Studies', 'Events', 'Newsletter', 'About', 'PyTorch', 'Foundation', 'Members', 'Governing', 'Board', 'Technical', 'Advisory', 'Council', 'Cloud', 'Credit', 'Program', 'Staff', 'Contact', 'Brand', 'Guidelines', 'JOIN', 'github']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://pytorch.org/'\n",
    "responce = urllib.request.urlopen(url= url)\n",
    "bs4 = BeautifulSoup(responce, 'html.parser').get_text()\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for i in bs4.split():\n",
    "    tokens.append(i)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "clean_token = []\n",
    "for tok in tokens:\n",
    "    if len(tok.lower()) > 1 and (tok.lower() not in stop):\n",
    "        clean_token.append(tok)\n",
    "\n",
    "Freq_dist_nltk = nltk.FreqDist(clean_token)\n",
    "Freq_dist_nltk.plot(30, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c151ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
