
---

# 성능 고도화 방법 정리

## 1. 과적합 (Overfitting)

* **정의**: 학습 데이터에 대한 오차는 매우 낮으나, 새로운 데이터(일반화 데이터)에 대한 오차가 큰 현상.
* **문제점**: 일반화 성능 저하 → 실전 데이터에 취약.
* **해결 방법**

  * 드롭아웃(Dropout)
  * 정규화(Regularization, L1/L2 등)
  * 데이터 증강(Data Augmentation)
  * 교차 검증(Cross Validation)
  * 학습 조기 종료(Early Stopping)

## 2. 적합 상태 (Good Fitting)

* 과소적합(Underfitting)도 아니고, 과적합도 아닌 상태.
* 모델이 학습 데이터와 평가 데이터 모두에서 균형 잡힌 성능을 보일 때 **강건하다(Robust)**고 표현.

---

## 3. 편향과 분산 (Bias & Variance)

* **편향(Bias)**: 모델이 너무 단순하여 실제 데이터 패턴을 제대로 잡아내지 못하는 정도. → 과소적합 발생.
* **분산(Variance)**: 모델이 너무 복잡하여 학습 데이터의 작은 변화에도 민감하게 반응하는 정도. → 과적합 발생.
* **트레이드오프**:

  * 편향이 낮아지면 분산이 높아지고, 분산이 낮아지면 편향이 높아질 수 있음.
  * 목표는 두 가지를 동시에 최소화하여 평가 데이터 성능을 최대화하는 것.
  * → 적절한 모델 복잡성 선택이 핵심.

---

## 4. 최적화 과정에서의 최소값

### 지역 최소값 (Local Minimum)

* 손실 함수가 특정 영역에서 가장 낮은 점.
* 국소적인 범위 내에서는 최소값이지만 전체적으로는 최적이 아님.
* 빠지면 학습 정체 현상 발생.
* **해결 방법**:

  * 적절한 학습률 조정
  * 모멘텀(Momentum) 적용
  * Adam, RMSProp 등의 고급 옵티마이저 활용

### 전역 최소값 (Global Minimum)

* 손실 함수 전체 영역에서 가장 낮은 점.
* 모든 매개변수 조합 중 손실이 최소인 지점.
* **목표**: 학습 과정에서 전역 최소값 또는 전역 최소값에 가까운 값에 도달하는 것.

---

## 5. 가중치 초기화 (Weight Initialization)

* **중요성**: 잘못된 초기화 → 기울기 소실/폭발 문제 발생, 학습 불안정.
* **방법**:

  * **균등 분포 초기화(Uniform Initialization)**
  * **정규 분포 초기화(Normal Initialization)**
  * **Xavier 초기화**: 입력/출력 노드 개수 고려 (tanh, sigmoid에 적합)
  * **He 초기화**: ReLU 계열 활성화 함수에 적합

---

## 6. 네트워크 안정화 기법

### (1) 드롭아웃 (Dropout)

* 학습 시 임의의 노드를 일정 확률로 비활성화.
* 특정 노드에 의존하지 않고 **앙상블 효과**를 내며 일반화 성능 개선.

---

### (2) 피처 스케일링 (Feature Scaling)

* 입력 특성 값을 일정한 범위로 맞추는 작업.
* 모든 특성을 편향 없이 학습할 수 있도록 함.
* **대표적 방법**:

  * **정규화(Normalization)**: 0~1 범위로 조정 (Min-Max Scaling)
  * **표준화(Standardization)**: 평균 0, 분산 1로 조정

---

### (3) 정규화 (Normalization)

* 내부 활성화 값 분포를 일정하게 유지하여 안정적인 학습 지원.
* 주요 방법:

  * **배치 정규화 (Batch Normalization)**

    * 배치 단위 평균/분산 기준 정규화.
    * 내부 공변량 변화(Internal Covariate Shift) 완화.
    * 기울기 소실/폭발 방지.
    * **구조**: 완전연결(FC) → 배치 정규화(BN) → 활성화 함수

  * **레이어 정규화 (Layer Normalization)**

    * 하나의 샘플 내 모든 특성 기준으로 평균/분산 계산.
    * 배치 크기 영향 없음.
    * RNN/LSTM 같은 시계열 모델에서 유리.

  * **인스턴스 정규화 (Instance Normalization)**

    * 각 샘플의 채널 단위 평균/분산 계산.
    * 이미지 스타일 변환(Style Transfer)에서 주로 사용.

  * **그룹 정규화 (Group Normalization)**

    * 채널을 그룹으로 묶어 그룹 단위 평균/분산 계산.
    * 소규모 배치 환경에서 적합.
    * 객체 인식, 영역 분할 등 배치 크기 제한 문제에서 효과적.

---

# 📌 전체 요약

* **과적합 ↔ 과소적합** 균형을 맞춰 **Good Fitting** 상태를 유지하는 것이 목표.
* **편향-분산 트레이드오프**를 고려해 모델 복잡성을 조절.
* 최적화 시 **지역 최소값 회피, 전역 최소값 근접**을 목표로 다양한 옵티마이저/학습률 전략 사용.
* **가중치 초기화**와 **정규화 기법**은 안정적 학습의 핵심.
* 드롭아웃, 피처 스케일링, 다양한 정규화 기법을 적절히 활용해 네트워크를 안정화하고 성능을 고도화할 수 있음.

---


