
---

# 데이터 증강 (Data Augmentation)

데이터 증강은 원본 데이터를 인위적으로 변형하거나 조작하여 **데이터 다양성을 확보**하고, **일반화 성능을 향상**시키는 방법이다.
특히 데이터가 부족하거나 과적합 위험이 높은 경우 효과적이다.

---

## 1. 이미지 데이터 증강

### 기본적인 이미지 증강 기법

* **리사이즈(Resize)**: 이미지 크기를 조정.
* **회전(Rotation)**: 특정 각도로 회전.
* **뒤집기(Flipping)**: 좌우/상하 반전.
* **자르기(Cropping)**: 이미지의 일부 영역만 사용.

➡️ Python에서는 **OpenCV**, **Pillow**, **TensorFlow/Keras ImageDataGenerator** 등을 활용 가능.

---

### 고급 이미지 증강 기법

* **Cutout**: 이미지 일부 영역을 검은색 또는 특정 값으로 가려서 일부 정보 제거.
* **Mixup**: 두 이미지를 픽셀 단위로 선형 결합 → 레이블도 동일 비율로 혼합.
* **CutMix**: 한 이미지의 일부를 잘라 다른 이미지에 삽입 → 레이블도 영역 비율에 따라 혼합.
* **Color Jittering**: 밝기, 대비, 채도, 색조 변경.
* **Gaussian Noise 추가**: 랜덤 노이즈 삽입.

---

### 오픈소스 라이브러리

* **Albumentations**: 이미지 분류/탐지/분할 등 다양한 태스크에서 활용 가능한 강력한 라이브러리.
* **ImgAug**: 객체 인식(Object Detection) 등에서 라벨(box, keypoint 등)까지 함께 변환 가능.
* **Torchvision.transforms**: PyTorch 공식 지원 라이브러리.

---

### 주의할 점

* 데이터의 **도메인 특성**을 고려해야 함.
* 의미 왜곡 금지 (예: 숫자 6을 180° 회전 → 9로 변질될 수 있음).
* 증강은 **일반화 성능을 높이되, 데이터 의미를 보존**해야 함.

---

## 2. 텍스트 데이터 증강

### 기본 기법 (EDA, Easy Data Augmentation)

* **동의어 대체(Synonym Replacement)**: 불용어가 아닌 단어를 무작위 동의어로 치환.
* **무작위 삽입(Random Insertion)**: 특정 단어의 동의어를 문장 내 임의 위치에 추가.
* **무작위 교체(Random Swap)**: 두 단어 위치를 교환.
* **무작위 삭제(Random Deletion)**: 단어를 일정 확률로 제거.

---

### 의미 유지 변환

* **구조 변환**: 능동태 ↔ 수동태 변환.

  * 예: "고양이가 쥐를 쫓았다." → "쥐가 고양이에게 쫓겼다."
* **직접화 ↔ 간접화 변환**:

  * 예: "그녀가 '안녕하세요'라고 말했다." → "그녀가 인사를 했다."

---

### 역번역 (Back Translation)

* 문장을 다른 언어로 번역 후 다시 원래 언어로 번역.
* 의미는 유지하면서 문장 구조 다양화.

---

### 사전학습된 언어모델 활용

* **BERT, GPT-2, BART** 등 Pre-trained LM을 이용해 문맥 기반 데이터 증강.
* 문장 내 토큰 마스킹 후 대체, 혹은 새로운 문장 생성 가능.

---

## 3. 전이 학습 (Transfer Learning)

* **개념**: 다른 문제에서 학습된 모델의 지식을 새로운 작업에 활용.
* **대표 사례**: 의료 이미지 분석 (소량 데이터로도 좋은 성능).
* **한계**: 소스와 타겟 도메인 차이가 크면 성능 저하.

---

## 4. 생성 기반 학습

### 생성 학습 (Generative Learning)

* 동일한 데이터를 여러 방식으로 변형해 원본과 변형 간 관계를 학습.
* 예: GAN(Generative Adversarial Networks) → 새로운 이미지 생성.

### 대체 작업 학습 (Surrogate Task Learning)

* **개념**: 원래 목표 태스크 대신, 관련 있는 다른 태스크를 학습하면서 일반화 성능 강화.
* **예시**:

  * 이미지: 원본 → 회전 각도 예측(0°, 90°, 180°, 270°) → 특징 학습 강화.
  * NLP: 다음 단어 예측, 문장 순서 맞추기 → 데이터 부족 문제 해결.

---

## 5. 대조 학습 (Contrastive Learning)

* **개념**: 데이터 쌍 간 유사도 학습.

  * 비슷한 데이터 → 임베딩 공간에서 가깝게.
  * 다른 데이터 → 멀리 떨어지게.
* **활용**: 자가 지도 학습(Self-Supervised Learning), 이미지 검색, 클러스터링.
* **대표 알고리즘**: SimCLR, MoCo, BYOL 등.

---

# 📌 전체 요약

* **이미지 증강**: Resize, Rotation, Flipping, Crop → Cutout, Mixup, CutMix 등 확장.
* **텍스트 증강**: 동의어 대체, 무작위 삽입/교체/삭제, 역번역, 구조 변환.
* **전이 학습**: 소량 데이터 문제 해결에 효과적이지만, 도메인 차이 크면 한계 존재.
* **생성 학습 & 대체 작업 학습**: 원본-변형 관계 학습, 보조 과제 학습을 통한 일반화 강화.
* **대조 학습**: 데이터 간 유사도/차이 학습 → 최신 자가 지도 학습 핵심 기법.

---
