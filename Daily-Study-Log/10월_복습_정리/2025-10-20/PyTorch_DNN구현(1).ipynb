{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e45bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch.cuda')\n",
    "\n",
    "import numpy as np # 기본적인 연산을 위한 라이브러리\n",
    "import matplotlib.pyplot as plt # 시각화를 위한 라이브러리\n",
    "from tqdm.notebook import tqdm # 상태 바를 나타내기 위한 라이브러리\n",
    "\n",
    "import torch # PyTorch 라이브러리\n",
    "import torch.nn as nn # 모델 구성을 위한 라이브러리\n",
    "import torch.optim as optim # optimizer 설정을 위한 라이브러리\n",
    "from torch.utils.data import Dataset, DataLoader # 데이터셋 설정을 위한 라이브러리\n",
    "\n",
    "import torchvision # PyTorch의 컴퓨터 비전 라이브러리\n",
    "import torchvision.transforms as T # 이미지 변환을 위한 모듈\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score # 성능지표 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bbcd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "def random_seed(seed_num):\n",
    "    torch.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.cuda.manual_seed_all(seed_num)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(seed_num)\n",
    "\n",
    "random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2d0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 불러올 때, 필요한 변환(transform)을 정의\n",
    "# T.Compose : 여러 개의 변환을 순서대로 묶어서 적용할 때 사용\n",
    "# T.ToTensor : 이미지를 PyTorch에서 다룰 수 있는 Tensor로 변환\n",
    "mnist_transform = T.Compose([ \n",
    "    T.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db46b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision 라이브러리를 사용하여 MNIST Dataset을 불러오기\n",
    "download_root = ',/MNIST_DATASET'\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(download_root, transform=mnist_transform, train= True, download = True)\n",
    "test_dataset = torchvision.datasets.MNIST(download_root, transform=mnist_transform, train= False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d9b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset 개수 : 48000\n",
      "Validation dataset 개수 : 12000\n"
     ]
    }
   ],
   "source": [
    "# train, valid 데이터 셋 나누기\n",
    "total_size = len(train_dataset)\n",
    "train_num, valid_num = int(total_size * 0.8), int(total_size * 0.2)\n",
    "print(f\"Train dataset 개수 : {train_num}\")\n",
    "print(f\"Validation dataset 개수 : {valid_num}\")\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_num, valid_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b413f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader는 인자로 주어진 Dataset을 이용하여, 단일 데이터들을 정해진 개수만큼 모아 미니 배치를 구성하는 역할\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size , shuffle = True) \n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "667ce12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델 코드\n",
    "class DNN(nn.Module):\n",
    "  def __init__(self, hidden_dims, num_classes, dropout_ratio, apply_batchnorm, apply_dropout, apply_activation, set_super):\n",
    "    if set_super:\n",
    "      super().__init__()\n",
    "\n",
    "    self.hidden_dims = hidden_dims\n",
    "    self.layers = nn.ModuleList()\n",
    "\n",
    "    for i in range(len(self.hidden_dims) - 1):\n",
    "      self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))\n",
    "\n",
    "      if apply_batchnorm:\n",
    "        self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1]))\n",
    "\n",
    "      if apply_activation:\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "      if apply_dropout:\n",
    "        self.layers.append(nn.Dropout(dropout_ratio))\n",
    "\n",
    "    self.classifier = nn.Linear(self.hidden_dims[-1], num_classes)\n",
    "    self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Input and Output Summary\n",
    "\n",
    "    Input:\n",
    "      x: [batch_size, 1, 28, 28]\n",
    "    Output:\n",
    "      output: [batch_size, num_classes]\n",
    "\n",
    "    \"\"\"\n",
    "    x = x.view(x.shape[0], -1)  # [batch_size, 784]\n",
    "\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "\n",
    "    x = self.classifier(x) # [batch_size, 10]\n",
    "    output = self.softmax(x) # [batch_size, 10]\n",
    "    return output\n",
    "\n",
    "  def weight_initialization(self, weight_init_method):\n",
    "    for m in self.modules():\n",
    "      if isinstance(m, nn.Linear):\n",
    "        if weight_init_method == 'gaussian':\n",
    "          nn.init.normal_(m.weight)\n",
    "        elif weight_init_method == 'xavier':\n",
    "          nn.init.xavier_normal_(m.weight)\n",
    "        elif weight_init_method == 'kaiming':\n",
    "          nn.init.kaiming_normal_(m.weight)\n",
    "        elif weight_init_method == 'zeros':\n",
    "          nn.init.zeros_(m.weight)\n",
    "\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "  def count_parameters(self):\n",
    "    return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed5a4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 정의\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f80dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률, DNN 매개변수 정의\n",
    "# optimizer : 가중치를 업데이트하는 알고리즘 정의\n",
    "lr = 0.001\n",
    "hidden_dim = 128\n",
    "hidden_dims = [784, hidden_dim * 4, hidden_dim * 2, hidden_dim]\n",
    "model = DNN(hidden_dims = hidden_dims, num_classes = 10, dropout_ratio = 0.2, apply_batchnorm = True, apply_dropout = True, apply_activation = True, set_super = True)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7c0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs):\n",
    "  model.train()  # 모델을 학습 모드로 설정\n",
    "  train_loss = 0.0\n",
    "  train_accuracy = 0\n",
    "\n",
    "  tbar = tqdm(dataloader)\n",
    "  for images, labels in tbar:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # 순전파\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # 역전파 및 weights 업데이트\n",
    "      # loss.backward() = 연산 그래프를 따라 미분 → .grad에 저장\n",
    "      # optimizer.step() = 저장된 .grad를 이용해 파라미터 업데이트\n",
    "      # optimizer.zero_grad() = .grad 초기화\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # 손실과 정확도 계산\n",
    "      train_loss += loss.item()\n",
    "      # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      train_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "      # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "      tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "  # 에폭별 학습 결과 출력\n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_accuracy = train_accuracy / len(train_dataset)\n",
    "\n",
    "  return model, train_loss, train_accuracy\n",
    "\n",
    "def evaluation(model, dataloader, valid_dataset, criterion, device, epoch, num_epochs):\n",
    "  model.eval()  # 모델을 평가 모드로 설정\n",
    "  valid_loss = 0.0\n",
    "  valid_accuracy = 0\n",
    "\n",
    "  with torch.no_grad(): # model의 업데이트 막기\n",
    "      tbar = tqdm(dataloader)\n",
    "      for images, labels in tbar:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          # 순전파\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "\n",
    "          # 손실과 정확도 계산\n",
    "          valid_loss += loss.item()\n",
    "          # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "          _, predicted = torch.max(outputs, 1)\n",
    "          valid_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "          # tqdm의 진행바에 표시될 설명 텍스트를 설정\n",
    "          tbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}\")\n",
    "\n",
    "  valid_loss = valid_loss / len(dataloader)\n",
    "  valid_accuracy = valid_accuracy / len(valid_dataset)\n",
    "\n",
    "  return model, valid_loss, valid_accuracy\n",
    "\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name):\n",
    "    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n",
    "    early_stop_counter = 0  # 카운터\n",
    "    valid_max_accuracy = -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_accuracy = training(model, train_dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\n",
    "        model, valid_loss, valid_accuracy = evaluation(model, valid_dataloader, valid_dataset, criterion, device, epoch, num_epochs)\n",
    "\n",
    "        if valid_accuracy > valid_max_accuracy:\n",
    "          valid_max_accuracy = valid_accuracy\n",
    "\n",
    "        # validation loss가 감소하면 모델 저장 및 카운터 리셋\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"./model_{model_name}.pt\")\n",
    "            early_stop_counter = 0\n",
    "\n",
    "        # validation loss가 증가하거나 같으면 카운터 증가\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f} Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "        # 조기 종료 카운터가 설정한 patience를 초과하면 학습 종료\n",
    "        if early_stop_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, valid_max_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3338e850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5742a10627e0475e9fffc093e8d48453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c6f67772b041bf93b9141a24bccc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.3156, Train Accuracy: 0.9045 Valid Loss: 0.1222, Valid Accuracy: 0.9628\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c069b03fe746477795f5626e279ffb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6078f3a2b9e4f09bb97d51d13b873b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 0.1660, Train Accuracy: 0.9498 Valid Loss: 0.0997, Valid Accuracy: 0.9690\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de423e2ed65749879a81f63655f2714e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4231d2d9e0144608cb95f084e2b5657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 0.1321, Train Accuracy: 0.9593 Valid Loss: 0.0838, Valid Accuracy: 0.9742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc024a5bb7749f5853e3a70d8fe0426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[0;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m lr)\n\u001b[1;32m---> 15\u001b[0m model, valid_max_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m scores[model_name] \u001b[38;5;241m=\u001b[39m valid_max_accuracy\n",
      "Cell \u001b[1;32mIn[10], line 75\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\u001b[0m\n\u001b[0;32m     72\u001b[0m valid_max_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 75\u001b[0m     model, train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     model, valid_loss, valid_accuracy \u001b[38;5;241m=\u001b[39m evaluation(model, valid_dataloader, valid_dataset, criterion, device, epoch, num_epochs)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_accuracy \u001b[38;5;241m>\u001b[39m valid_max_accuracy:\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, dataloader, train_dataset, criterion, optimizer, device, epoch, num_epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 역전파 및 weights 업데이트\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# loss.backward() = 연산 그래프를 따라 미분 → .grad에 저장\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# optimizer.step() = 저장된 .grad를 이용해 파라미터 업데이트\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# optimizer.zero_grad() = .grad 초기화\u001b[39;00m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 손실과 정확도 계산\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\pytorch_test\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\pytorch_test\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\pytorch_test\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "patience = 3\n",
    "scores = dict()\n",
    "device = 'cpu' # gpu 설정\n",
    "model_name = 'exp1'\n",
    "init_method = 'kaiming' # gaussian, xavier, kaiming, zeros\n",
    "\n",
    "model = DNN(hidden_dims = hidden_dims, num_classes = 10, dropout_ratio = 0.2, apply_batchnorm = True, apply_dropout = True, apply_activation = True, set_super = True)\n",
    "model.weight_initialization(init_method)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f07100",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m DNN(hidden_dims \u001b[38;5;241m=\u001b[39m hidden_dims, num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, dropout_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, apply_batchnorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, apply_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, apply_activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, set_super \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mweight_initialization(init_method)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[0;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m lr)\n\u001b[0;32m     10\u001b[0m model, valid_max_accuracy \u001b[38;5;241m=\u001b[39m training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch normalization을 제외하고 학습을 진행합니다.\n",
    "model_name = 'exp2'\n",
    "init_method = 'kaiming' # gaussian, xavier, kaiming, zeros\n",
    "\n",
    "model = DNN(hidden_dims = hidden_dims, num_classes = 10, dropout_ratio = 0.2, apply_batchnorm = False, apply_dropout = True, apply_activation = True, set_super = True)\n",
    "model.weight_initialization(init_method)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76713d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout을 제외하고 학습을 진행합니다.\n",
    "# dropout_ratio는 하이퍼 파라미터입니다. 최적의 dropout_ratio에 따라 모델의 성능이 달라질 수 있습니다.\n",
    "model_name = 'exp3'\n",
    "init_method = 'kaiming' # gaussian, xavier, kaiming, zeros\n",
    "\n",
    "model = DNN(hidden_dims = hidden_dims, num_classes = 10, dropout_ratio = 0.2, apply_batchnorm = True, apply_dropout = False, apply_activation = True, set_super = True)\n",
    "model.weight_initialization(init_method)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209dbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수(activation function)를 제외하고 학습을 진행합니다.\n",
    "model_name = 'exp4'\n",
    "init_method = 'kaiming' # gaussian, xavier, kaiming, zeros\n",
    "\n",
    "model = DNN(hidden_dims = hidden_dims, num_classes = 10, dropout_ratio = 0.2, apply_batchnorm = True, apply_dropout = True, apply_activation = False, set_super = True)\n",
    "model.weight_initialization(init_method)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3391c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'exp5'\n",
    "init_method = 'zeros' # gaussian, xavier, kaiming, zeros\n",
    "\n",
    "model = DNN(hidden_dims = hidden_dims, num_classes = 10, dropout_ratio = 0.2, apply_batchnorm = True, apply_dropout = True, apply_activation = True, set_super = True)\n",
    "model.weight_initialization(init_method)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "model, valid_max_accuracy = training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, device, num_epochs, patience, model_name)\n",
    "scores[model_name] = valid_max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험별 validation accuracy 최댓값\n",
    "print(f\"BatchNorm, Dropout 사용 모델: {scores['exp1']:.4f}\")\n",
    "print(f\"BatchNorm 제거 모델: {scores['exp2']:.4f}\")\n",
    "print(f\"Dropout 제거 모델: {scores['exp3']:.4f}\")\n",
    "print(f\"Activation Function 제거 모델: {scores['exp4']:.4f}\")\n",
    "print(f\"가중치를 0으로 초기화한 모델: {scores['exp5']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm, Dropout을 사용한 모델을 로드합니다.\n",
    "model = DNN(hidden_dims = hidden_dims, num_classes = 10, dropout_ratio = 0.2, apply_batchnorm = True, apply_dropout = True, apply_activation = True, set_super = True)\n",
    "model.load_state_dict(torch.load(\"./model_exp1.pt\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66557cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_labels = []\n",
    "total_preds = []\n",
    "total_probs = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        outputs = model(images)\n",
    "        # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_preds.extend(predicted.detach().cpu().tolist())\n",
    "        total_labels.extend(labels.tolist())\n",
    "        total_probs.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "total_preds = np.array(total_preds)\n",
    "total_labels = np.array(total_labels)\n",
    "total_probs = np.concatenate(total_probs, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, f1를 계산합니다.\n",
    "precision = precision_score(total_labels, total_preds, average='macro')\n",
    "recall = recall_score(total_labels, total_preds, average='macro')\n",
    "f1 = f1_score(total_labels, total_preds, average='macro')\n",
    "\n",
    "# AUC를 계산합니다.\n",
    "# 모델의 출력으로 nn.LogSoftmax 함수가 적용되어 결과물이 출력되게 됩니다. sklearn의 roc_auc_score 메서드를 사용하기 위해서는 단일 데이터의 클래스들의 확률 합은 1이 되어야 합니다. 이를 위해 확률 matrix에 지수 함수를 적용합니다.\n",
    "total_probs = np.exp(total_probs)\n",
    "auc = roc_auc_score(total_labels, total_probs, average='macro', multi_class = 'ovr')\n",
    "\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}, AUC: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29db4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
