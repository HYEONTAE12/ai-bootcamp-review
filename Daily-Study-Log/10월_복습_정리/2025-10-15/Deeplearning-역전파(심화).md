

---

# 📘 역전파 심화 완전 정복 노트 🎯

## 📌 기본 표기법

### z와 a의 차이

* **z (활성화 함수 입력값)**:
  [
  z^l = W^l \cdot a^{l-1} + b^l
  ]
  → 가중치·편향까지 더한, 아직 활성화 함수를 통과하지 않은 값

* **a (활성화 함수 출력값)**:
  [
  a^l = \sigma(z^l)
  ]
  → z에 활성화 함수를 적용한 값, 다음 레이어의 입력이 됨

**예시**

```
입력: a^1 = [0.5, 0.3]
가중치: W^2 = [[0.4, 0.6], [0.1, 0.9]]
편향: b^2 = [0.2, 0.1]

W^2 · a^1 = [0.38, 0.32]
z^2 = [0.38, 0.32] + [0.2, 0.1] = [0.58, 0.42]
a^2 = σ(z^2) ≈ [0.641, 0.603]
```

---

## 🎯 손실 함수의 2가지 기본 가정

### 1. 총 손실 = 각 샘플 손실의 평균

[
C = \frac{1}{n}\sum_x C_x
]

* 전체 손실을 계산하기 어려우므로, 각 샘플 손실만 구해서 합산/평균
* 미분도 동일:
  [
  \frac{\partial C}{\partial w} = \sum_x \frac{\partial C_x}{\partial w}
  ]

👉 **한 샘플 단위로 역전파를 이해해도 OK!**

---

### 2. 손실은 최종 출력 (a^L)의 함수

* 중간 레이어 출력을 직접 쓰지 않음 → 역전파 단순화
* 예시:

  * ✅ 가능:
    [
    C = \tfrac{1}{2}|y - a^L|^2
    ]
  * ❌ 불가능:
    [
    C = f(a^2, a^3, a^L)
    ]

---

## 🔥 역전파 4대 방정식

### 1. 출력층 에러 δ^L

[
\delta^L_j = \frac{\partial C}{\partial a^L_j}\cdot \sigma'(z^L_j)
]

* 출력값이 손실에 미치는 영향 × 활성화 함수의 민감도
* 이차 손실 함수일 때:
  [
  \delta^L = (a^L - y) \odot \sigma'(z^L)
  ]

---

### 2. 이전 레이어 에러 δ^l

[
\delta^l = (W^{l+1})^T \delta^{l+1} \odot \sigma'(z^l)
]

* **연쇄법칙**으로부터 유도
* “다음 레이어의 에러가 현재 레이어로 전파되는 과정”

---

### 3. 편향에 대한 그래디언트

[
\frac{\partial C}{\partial b^l_j} = \delta^l_j
]

* 편향은 단순히 더해지므로, 에러 δ^l 자체가 그래디언트가 됨

---

### 4. 가중치에 대한 그래디언트

[
\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \cdot \delta^l_j
]

행렬 형태:
[
\nabla_{W^l} C = \delta^l \cdot (a^{l-1})^T
]

👉 차원까지 딱 맞아떨어짐

---

## 🎬 역전파 알고리즘 전체 흐름

1. **입력 설정**
   [
   a^1 = x
   ]

2. **순전파**
   [
   z^l = W^l a^{l-1} + b^l, \quad a^l = \sigma(z^l)
   ]

3. **출력층 에러 계산**
   [
   \delta^L = \nabla_a C \odot \sigma'(z^L)
   ]

4. **역전파**
   [
   \delta^l = (W^{l+1})^T \delta^{l+1} \odot \sigma'(z^l)
   ]

5. **그래디언트 계산**
   [
   \frac{\partial C}{\partial W^l} = \delta^l (a^{l-1})^T,\quad
   \frac{\partial C}{\partial b^l} = \delta^l
   ]

---

## 💡 중요한 인사이트

### 1. 낮은 활성화 뉴런

* (a^{l-1}*k \approx 0 \implies \frac{\partial C}{\partial w^l*{jk}} \approx 0)
* 뉴런이 거의 죽어있으면 가중치 업데이트가 거의 없음 → 학습 느려짐

### 2. 포화된 뉴런

* σ(z)가 0 또는 1에 가까우면 σ'(z) ≈ 0
* δ ≈ 0 → **그래디언트 소실 문제 발생**

---

## 🎯 파라미터 업데이트

경사하강법:
[
W^l \leftarrow W^l - \frac{\eta}{m}\sum_x \delta^{(x,l)} (a^{(x,l-1)})^T
]
[
b^l \leftarrow b^l - \frac{\eta}{m}\sum_x \delta^{(x,l)}
]

* η: 학습률
* m: 미니배치 크기

---

## ✅ 정리

* 역전파는 연쇄법칙을 활용해 효율적으로 모든 그래디언트를 구한다.
* 각 층의 에러 δ를 계산하고, 이를 통해 W와 b에 대한 미분을 손쉽게 구한다.
* 모든 파라미터는 “손실을 줄이는 방향(–∇C)”으로 동시에 업데이트된다.

---


