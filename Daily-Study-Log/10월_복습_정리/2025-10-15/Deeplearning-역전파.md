
---

# 📘 역전파(Backpropagation) 정리

## 1. 역전파의 등장

* 신경망은 수많은 **파라미터(가중치, 편향)**로 이루어짐.
* 학습을 위해서는 각 파라미터가 **손실(loss)**에 얼마나 기여했는지 알아야 함.
* 이를 수치로 나타낸 것이 **손실 함수의 기울기(gradient)**.
* 기울기를 구하면 → **경사 하강법(Gradient Descent)** 으로 손실을 줄이는 방향으로 파라미터를 업데이트 가능.

---

## 2. 손실 함수의 기울기

* **정의**: 모델의 각 파라미터에 대한 손실 함수의 기울기.
* 의미: 해당 파라미터가 잘못된 예측에 얼마나 기여했는지를 나타냄.
* 활용: 경사 하강법을 적용하려면 모델을 구성하는 **모든 파라미터에 대한 미분(그래디언트)**이 필요.

---

## 3. 계산 그래프 (Computational Graph)

* **연산 과정을 노드와 엣지로 표현한 방향 그래프**.
* 구성 요소:

  * **노드(Node)**: 하나의 연산(예: 덧셈, 곱셈, 활성화 함수).
  * **엣지(Edge)**: 노드(연산)에 필요한 입력값.

### 계산 그래프의 장점

* **편미분 계산 가능**: 연산 과정을 나누어 국소 기울기 계산 후 합성 가능.
* **중간 결과 보관**: 계산 재사용이 가능해 효율적.
* **복잡한 문제 단순화**: 작은 문제 단위로 쪼개어 해결 가능.
* **효율적 미분**: 각 변수에 대한 미분을 빠르게 계산 가능.

---

## 4. 연쇄 법칙 (Chain Rule)

* **합성 함수 미분**을 위해 사용하는 규칙.
* 원리: **각 단계의 변화율을 곱해서 전체 변화율을 구한다.**

예시:

* `a → u → L` 이라는 흐름에서,

  * ∂L/∂u = u가 변할 때 L의 변화량
  * ∂u/∂a = a가 변할 때 u의 변화량
  * 따라서 **∂L/∂a = (∂L/∂u) × (∂u/∂a)**

👉 연쇄 법칙 덕분에, 복잡한 함수도 **작은 변화율들의 곱**으로 단순하게 계산 가능.

---

## 5. 역전파 알고리즘 (Backpropagation Algorithm)

* **정의**: 신경망의 추론 방향(순전파)과 반대 방향으로, 오차(loss)에 의한 편미분을 전파하여 파라미터를 업데이트하는 과정.
* 핵심: **국소 기울기를 곱하면서 오차를 뒤로 전달하는 것**.

---

## 6. 미분 기호와 의미

* **미분** = “출력 변화량 ÷ 입력 변화량”
* **편미분 ∂L/∂a**:

  * a가 바뀔 때 손실 L이 얼마나 변하는가?
  * a가 1만큼 변하면 손실이 얼마나 변하는지를 나타냄 → **기울기**
* 연쇄 법칙에서 나오는 곱셈 구조가 분수의 **약분처럼 보이지만**, 실제로는 변화율을 전파하는 원리.

---

## 7. 학습 과정 (Step by Step)

### 1️⃣ 파라미터 초기화

* 각 엣지(가중치, 편향)에 임의의 초기값 설정.
* 주로 정규분포, 균등분포 등을 이용해 랜덤 초기화.

### 2️⃣ 순전파 (Forward Propagation)

* 입력 데이터를 넣고, 계산 그래프(곱셈 → 덧셈 → 활성화 함수 …)를 따라 출력 계산.
* 마지막에서 **손실 함수 L**을 계산.

### 3️⃣ 역전파 (Backward Propagation)

* 손실 L을 기준으로 각 파라미터의 기울기를 계산.
* 연쇄 법칙을 이용해 계산 그래프를 거꾸로 따라가며 **기울기 전파**.

### 4️⃣ 파라미터 업데이트 (Update)

* 구한 기울기에 **학습률 η**를 곱해 파라미터 조정.
* 손실이 줄어드는 방향으로 이동.

---

## 8. 역전파의 이점

✅ 복잡한 연산을 작은 단위로 분해해 단순화
✅ 모든 파라미터의 기울기를 효율적으로 계산 가능
✅ 자동 학습 알고리즘 구현 가능
✅ 딥러닝 학습을 가능하게 하는 핵심 알고리즘

---

# 🚀 최종 요약

* **역전파**: 오차를 거꾸로 전달하여 각 파라미터가 손실에 얼마나 기여했는지 계산 → 파라미터 업데이트.
* **계산 그래프**: 연산 과정을 구조화하여 효율적 편미분 가능.
* **연쇄 법칙**: 작은 변화율을 곱해 전체 기울기를 구하는 원리.
* **학습 절차**: 파라미터 초기화 → 순전파 → 손실 계산 → 역전파 → 파라미터 업데이트.

---

