
---

# 📘 MLOps 엔지니어링 개요 (정리 41\~80p)

---

### 🚀 1. 모델 배포의 현실적인 어려움

* 데이터 과학자 → “이게 제일 잘 나온 모델이에요!” 하고 전달
* 문제:

  * 어떤 환경에서 학습했는지 기록 없음
  * 프로덕션 서버 환경과 충돌 (OS, Python, GPU 버전 다름)
* 결과: **연구실에서는 OK → 서버에서는 Error** 😓

---

### 🖥️ 2. 배포 환경 차이 사례

* **연구 환경(Local)**: Ubuntu 22.04, Python 3.11, RTX 3060 GPU
* **프로덕션 서버**: CentOS 5.11, Python 3.8, A100 GPU, 10TB SSD
* 서로 다르면 모델이 그대로는 절대 안 돌아감 → **환경 일관성 확보 필수**

---

### 📋 3. 모델 배포 시 고려사항

1. 모델 일관성 & 버전 관리
2. 데이터 스키마 & 버전 관리
3. 태스크 수행 파라미터 관리
4. 서빙 방식 (REST, gRPC, On-device 등)
5. 실행 시간 & 자원 요구사항 충족
6. 확장성 & 탄력성 확보
7. 응답 지연(latency) 최소화
8. 모니터링 & 로깅

👉 배포는 단순히 모델 파일 올리는 게 아니라 **운영 요건 전체를 만족**시켜야 함.

---

### 📦 4. 컨테이너화 (Containerization)

* **문제 해결**: 연구 환경 ≠ 운영 환경
* **방법**: Docker로 실행 환경을 패키징
* **구조**

  1. 운영체제 + 라이브러리 버전
  2. 모델 코드 + 학습된 가중치
  3. API 서버 코드

👉 어디서 실행해도 동일한 환경 보장 → **안정적 배포 가능**

---

### 🔄 5. 워크플로우 관리 도구

* 배포 과정은 여러 단계(Task)의 파이프라인으로 구성
* 컨테이너 안에서 실행되지만, **결과(모델·로그)는 휘발성** → 외부 저장소에 기록 필수
* **도구 예시**: Airflow, Kubeflow, MLflow

---

### 🗄️ 6. 모델 저장소 & 로그 관리

* **결과 데이터**: MinIO, AWS S3, MySQL, MongoDB
* **아티팩트(모델 파일)**: S3, MinIO
* **성능 지표**: ELK, EFK 같은 모니터링 시스템
* **수행 로그**: Airflow, MLflow, W\&B

👉 저장소는 **데이터 성격에 따라 적합한 것**을 선택해야 함.

---

### 📊 7. 모델 모니터링

배포 후에는 **모델을 계속 감시**해야 함.

* **자원 모니터링**: CPU, GPU, 메모리, 디스크, 네트워크
* **상태 점검**: API가 정상 동작하는지 Health Check
* **ML 성능 모니터링**

  * 오프라인 지표: Train/Validation Loss, Accuracy, RMSE
  * 온라인 지표: 클릭 수, 전환율, 이탈률 (실사용자 반응)
* **인프라 지표**: 호출 수, 응답 지연(latency), 오류율

👉 **섀도우 테스트, A/B 테스트**를 통해 실제 환경에서 성능 비교 가능

---

### 🔁 8. 피드백 루프

* **모니터링 → 성능 저하 감지 → 데이터 탐색/재학습 → 재배포**
* 모델은 한 번 만들고 끝나는 게 아니라, 계속 순환(Feedback Loop)해야 한다.
* 예: LoL 추천 모델 → 패치 후 성능 급락 → 모니터링 알림 → 새 데이터 반영 → 재학습 → 재배포

---

### 📉 9. MLOps가 필요한 이유

* **현실 통계**

  * ML 프로젝트의 80% 이상이 배포 전에 중단
  * 배포까지 평균 6개월 이상
  * 기업 10%만 AI에서 금전적 이익 창출
  * 전체 투자 대비 평균 ROI = 1.3%
* **원인**:

  * 데이터/비즈니스 요구 변화
  * 이해관계자 간 소통 부족
  * 데이터 과학자의 운영 역량 부족

👉 결론: **MLOps 없으면 모델은 금방 무용지물이 됨**

---

### 🔧 10. DevOps vs MLOps

* **DevOps**: 코드 중심 (빌드, 테스트, 배포 자동화 = CI/CD)
* **MLOps**: DevOps + 데이터 검증, 모델 학습, 모델 분석, 피처 엔지니어링, 메타데이터 관리, 모니터링 포함

👉 **MLOps = 더 넓고 복잡한 생태계**

---

### 📈 11. MLOps 아키텍처 수준

* **Level 0 (수동)**

  * 데이터 분석, 학습, 검증, 배포 모두 사람이 수동 처리
  * 느리고 재현성 없음

* **Level 1 (ML 파이프라인 자동화)**

  * 데이터 준비, 학습, 검증 과정을 자동화
  * 모델·소스 코드 저장 → 재현성 확보
  * 배포·서빙은 여전히 수동

* **Level 2 (CI/CD 자동화)**

  * 코드 수정 → 자동 빌드, 학습, 검증, 배포
  * 모델 레지스트리·메타데이터 관리 포함
  * 운영 단계까지 완전 자동화

👉 **Level 0 → Level 1 → Level 2**로 갈수록 자동화 범위와 안정성이 커진다.

---

✅ **핵심 요약**

* 연구 환경 vs 배포 환경 차이 → 컨테이너화로 해결
* 워크플로우 & 저장소 관리 → 자동화 + 기록 필수
* 모니터링 요소 = 자원, 상태, 성능 지표, 인프라 지표
* MLOps 필요성 = 현실적으로 배포율 낮고 ROI도 낮음
* 아키텍처 수준 = Level 0(수동) → Level 1(파이프라인 자동화) → Level 2(CI/CD 자동화)

---


