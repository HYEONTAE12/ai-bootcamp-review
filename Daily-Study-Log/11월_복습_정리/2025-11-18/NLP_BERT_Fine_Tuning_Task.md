# BERT Fine-tuning Tasks ê°€ì´ë“œ

## ğŸ“Œ Fine-tuning ê°œìš”

### ê¸°ë³¸ ì›ë¦¬

**í•µì‹¬ ì•„ì´ë””ì–´:**
> ì‚¬ì „í•™ìŠµëœ BERTì— **í…ŒìŠ¤í¬ë³„ ë ˆì´ì–´ 1-2ê°œë§Œ ì¶”ê°€**í•˜ê³ , ì „ì²´ë¥¼ ì‘ì€ learning rateë¡œ ì¬í•™ìŠµ

```
Pretrained BERT (í•¨ê»˜ í•™ìŠµë¨, ê³ ì • X)
     â†“
+ Task-specific Head (ìƒˆë¡œ ì¶”ê°€, 1-2 layers)
     â†“
Task Output
```

### ê³µí†µ êµ¬ì¡°

**ì „ì²´ í”„ë¡œì„¸ìŠ¤:**

```python
# 1. Pretrained BERT ë¡œë“œ
model = BertForTaskXXX.from_pretrained('bert-base-uncased')
# â†’ BERT + Task-specific Head ìë™ ìƒì„±

# 2. ë°ì´í„° ì¤€ë¹„ ë° Fine-tuning
# â†’ BERT ê°€ì¤‘ì¹˜ + Head ëª¨ë‘ í•™ìŠµ

# 3. Task ìˆ˜í–‰
# â†’ í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡
```

**ê³µí†µ ì›ì¹™:**

| í•­ëª© | ì„¤ì • |
|------|------|
| **Learning Rate** | 2e-5 ~ 5e-5 (ì‘ê²Œ!) |
| **Epochs** | 2 ~ 4 (ì§§ê²Œ!) |
| **Batch Size** | 16 ~ 32 |
| **Warmup** | ì „ì²´ stepì˜ 10% |
| **Optimizer** | AdamW |

**ì™œ ì‘ì€ lrê³¼ ì§§ì€ epoch?**
- BERTëŠ” ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸
- ë„ˆë¬´ ë§ì´ í•™ìŠµí•˜ë©´ Pretrain ì§€ì‹ ì†ì‹¤ (Catastrophic Forgetting)
- Task-specific Headë§Œ ì£¼ë¡œ í•™ìŠµë˜ë„ë¡

---

## ğŸ¯ 4ê°€ì§€ ì£¼ìš” Fine-tuning Tasks

### Task ë¹„êµí‘œ

| Task | ì…ë ¥ í˜•ì‹ | ì‚¬ìš© í† í° | Head êµ¬ì¡° | ì¶œë ¥ |
|------|---------|---------|----------|------|
| **ë¬¸ì¥ ë¶„ë¥˜** | `[CLS] ë¬¸ì¥ [SEP]` | [CLS]ë§Œ | Linear(768 â†’ labels) | í´ë˜ìŠ¤ |
| **í† í° ë¶„ë¥˜** | `[CLS] í† í°ë“¤ [SEP]` | ëª¨ë“  í† í° | Linear(768 â†’ labels) Ã— ê° í† í° | í† í°ë³„ ë¼ë²¨ |
| **QA** | `[CLS] Q [SEP] Doc [SEP]` | ëª¨ë“  í† í° | 2ê°œ Linear(768 â†’ 1) | ì‹œì‘/ë ìœ„ì¹˜ |
| **ë¬¸ì¥ ìŒ** | `[CLS] A [SEP] B [SEP]` | [CLS]ë§Œ | Linear(768 â†’ labels) | ê´€ê³„ |

---

## 1ï¸âƒ£ ë¬¸ì¥ ë¶„ë¥˜ (Sequence Classification)

### ê°œë…

**ì •ì˜:** ë¬¸ì¥ í•˜ë‚˜ â†’ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ ì˜ˆì¸¡

**ìš©ë„:**
- ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
- ìŠ¤íŒ¸ í•„í„°ë§
- ì£¼ì œ ë¶„ë¥˜
- ë…ì„± ëŒ“ê¸€ íƒì§€

### êµ¬ì¡°

```
ì…ë ¥: [CLS] This movie is great [SEP]
       â†“
    [BERT Encoder]
       â†“
[CLS][í† í°1][í† í°2][í† í°3][í† í°4][SEP]
  â†“
[CLS] í† í°ë§Œ ì¶”ì¶œ (ë¬¸ì¥ ì „ì²´ í‘œí˜„)
  â†“
[Linear: 768 â†’ num_labels]
  â†“
[Softmax]
  â†“
í´ë˜ìŠ¤ í™•ë¥ : [0.1, 0.9] â†’ ê¸ì •!
```

### í•µì‹¬ í¬ì¸íŠ¸

**ì™œ [CLS] í† í°ë§Œ?**
- BERT ì‚¬ì „í•™ìŠµ ì‹œ [CLS]ê°€ ë¬¸ì¥ ì „ì²´ë¥¼ ìš”ì•½í•˜ë„ë¡ í•™ìŠµë¨
- NSP(Next Sentence Prediction) íƒœìŠ¤í¬ì—ì„œ [CLS] ì‚¬ìš©
- ë¬¸ì¥ ë ˆë²¨ í‘œí˜„ì„ ë‹´ê³  ìˆìŒ

**Head êµ¬ì¡°:**
```python
cls_output = bert_output[:, 0, :]  # [batch, 768]
logits = Linear(cls_output)         # [batch, num_labels]
probs = Softmax(logits)
```

### ê°„ë‹¨ ì½”ë“œ

```python
from transformers import BertForSequenceClassification

# ëª¨ë¸ (BERT + Classification Head)
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # ê¸ì •/ë¶€ì •
)

# ì˜ˆì¸¡
inputs = tokenizer("This movie is great", return_tensors="pt")
outputs = model(**inputs)
prediction = torch.argmax(outputs.logits, dim=-1)
```

---

## 2ï¸âƒ£ í† í° ë¶„ë¥˜ (Token Classification)

### ê°œë…

**ì •ì˜:** ê° í† í°ë§ˆë‹¤ ë¼ë²¨ ì˜ˆì¸¡

**ìš©ë„:**
- NER (Named Entity Recognition): ê°œì²´ëª… ì¸ì‹
- POS (Part-of-Speech) Tagging: í’ˆì‚¬ íƒœê¹…
- Chunking: êµ¬ë¬¸ ë¶„ì„

### êµ¬ì¡°

```
ì…ë ¥: [CLS] Elon Musk founded Tesla [SEP]

    [BERT Encoder]
       â†“
[CLS][Elon][Musk][founded][Tesla][SEP]
  â†“    â†“     â†“      â†“       â†“     â†“
ê° í† í°ë§ˆë‹¤ Linear ì ìš©
  â†“    â†“     â†“      â†“       â†“
  O  B-PER I-PER    O     B-ORG
```

### NER ë¼ë²¨ ì²´ê³„ (BIO)

**BIO Tagging:**
```
B (Begin):  ê°œì²´ì˜ ì‹œì‘
I (Inside): ê°œì²´ì˜ ì¤‘ê°„/ë
O (Outside): ê°œì²´ ì•„ë‹˜

ì˜ˆì‹œ:
"Elon Musk founded Tesla Inc"
  â†“
B-PER I-PER   O    B-ORG I-ORG
```

**ì£¼ìš” ê°œì²´ íƒ€ì…:**
- PER: Person (ì¸ëª…)
- ORG: Organization (ì¡°ì§ëª…)
- LOC: Location (ì§€ëª…)
- DATE: ë‚ ì§œ
- MISC: ê¸°íƒ€

### í•µì‹¬ í¬ì¸íŠ¸

**ì™œ ëª¨ë“  í† í°?**
- ê° í† í°ì´ ì–´ë–¤ ê°œì²´ì¸ì§€ ê°œë³„ì ìœ¼ë¡œ íŒë‹¨
- í† í° ë ˆë²¨ ì˜ˆì¸¡ í•„ìš”

**Head êµ¬ì¡°:**
```python
# ëª¨ë“  í† í°ì˜ ì¶œë ¥ ì‚¬ìš©
token_outputs = bert_output  # [batch, seq_len, 768]

# ê° í† í°ë§ˆë‹¤ ë¶„ë¥˜
logits = Linear(token_outputs)  # [batch, seq_len, num_labels]
predictions = torch.argmax(logits, dim=-1)
# ê° ìœ„ì¹˜ì˜ ë¼ë²¨ ì˜ˆì¸¡
```

**[CLS]ì™€ [SEP]ì€?**
- ë³´í†µ ì˜ˆì¸¡í•˜ì§€ ì•Šê±°ë‚˜ 'O' ë¼ë²¨
- Loss ê³„ì‚° ì‹œ ì œì™¸í•˜ëŠ” ê²½ìš° ë§ìŒ

### ê°„ë‹¨ ì½”ë“œ

```python
from transformers import BertForTokenClassification

# ëª¨ë¸
model = BertForTokenClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=9  # O, B-PER, I-PER, B-ORG, ...
)

# ì˜ˆì¸¡
inputs = tokenizer("Elon Musk founded Tesla", return_tensors="pt")
outputs = model(**inputs)
predictions = torch.argmax(outputs.logits, dim=-1)
# ê° í† í°ì˜ ë¼ë²¨: [O, B-PER, I-PER, O, B-ORG]
```

---

## 3ï¸âƒ£ ì§ˆì˜ì‘ë‹µ (Question Answering)

### ê°œë…

**ì •ì˜:** ì§ˆë¬¸ê³¼ ë¬¸ì„œê°€ ì£¼ì–´ì§€ë©´, ë¬¸ì„œì—ì„œ ë‹µì˜ ìœ„ì¹˜ ì°¾ê¸°

**ìš©ë„:**
- SQuAD (Stanford Question Answering Dataset)
- ë¬¸ì„œ ê¸°ë°˜ QA
- Reading Comprehension

### êµ¬ì¡°

```
ì…ë ¥: [CLS] Who founded Tesla [SEP] Tesla was founded by Elon Musk in 2003 [SEP]
í† í°:   0     1    2      3      4     5    6    7      8   9    10   11  12  13

    [BERT Encoder]
       â†“
ê° í† í°ì˜ ì¶œë ¥
       â†“
Start Head: ê° í† í°ì´ ë‹µ ì‹œì‘ì¼ í™•ë¥ 
End Head:   ê° í† í°ì´ ë‹µ ëì¼ í™•ë¥ 
       â†“
Start: í† í° 9 (Elon) â† ê°€ì¥ ë†’ì€ í™•ë¥ 
End:   í† í° 10 (Musk) â† ê°€ì¥ ë†’ì€ í™•ë¥ 
       â†“
ë‹µ: "Elon Musk"
```

### í•µì‹¬ í¬ì¸íŠ¸

**ë‘ ê°œì˜ Head:**
```python
# Start Position Head
start_logits = Linear_start(bert_output)  # [batch, seq_len]
start_probs = Softmax(start_logits)
# ê° í† í°ì´ ë‹µì˜ ì‹œì‘ì¼ í™•ë¥ 

# End Position Head
end_logits = Linear_end(bert_output)  # [batch, seq_len]
end_probs = Softmax(end_logits)
# ê° í† í°ì´ ë‹µì˜ ëì¼ í™•ë¥ 

# ë‹µ ì¶”ì¶œ
start_idx = argmax(start_logits)
end_idx = argmax(end_logits)
answer = tokens[start_idx:end_idx+1]
```

**ì œì•½ ì¡°ê±´:**
- `start_idx â‰¤ end_idx` (ì‹œì‘ì´ ëë³´ë‹¤ ì•)
- ë³´í†µ ì§ˆë¬¸ ë¶€ë¶„([SEP] ì´ì „)ì€ ì œì™¸
- `end_idx - start_idx < max_answer_length` (ë„ˆë¬´ ê¸´ ë‹µ ë°©ì§€)

**ë‹µì´ ì—†ëŠ” ê²½ìš°:**
```
ì§ˆë¬¸: "What is Google's founder?"
ë¬¸ì„œ: "Tesla was founded by Elon Musk"

â†’ [CLS] í† í°ì„ start/endë¡œ ì˜ˆì¸¡
â†’ "ë‹µ ì—†ìŒ" ì²˜ë¦¬
```

### ì…ë ¥ êµ¬ì„±

```python
# ì§ˆë¬¸ê³¼ ë¬¸ì„œ ë¶„ë¦¬
[CLS] ì§ˆë¬¸ [SEP] ë¬¸ì„œ [SEP]
  â†‘          â†‘       â†‘
Segment 0  ê²½ê³„  Segment 1

# Segment Embedding:
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
 â†‘ ì§ˆë¬¸ ë¶€ë¶„     â†‘ ë¬¸ì„œ ë¶€ë¶„

# ë‹µì€ í•­ìƒ Segment 1 (ë¬¸ì„œ)ì—ì„œë§Œ ì°¾ìŒ
```

### ê°„ë‹¨ ì½”ë“œ

```python
from transformers import BertForQuestionAnswering

# ëª¨ë¸
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')

# ì˜ˆì¸¡
question = "Who founded Tesla?"
context = "Tesla was founded by Elon Musk in 2003"
inputs = tokenizer(question, context, return_tensors="pt")

outputs = model(**inputs)
start_idx = torch.argmax(outputs.start_logits)
end_idx = torch.argmax(outputs.end_logits)

# ë‹µ ì¶”ì¶œ
answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx+1])
```

---

## 4ï¸âƒ£ ë¬¸ì¥ ìŒ ë¶„ë¥˜ (Sentence Pair Classification)

### ê°œë…

**ì •ì˜:** ë‘ ë¬¸ì¥ì˜ ê´€ê³„ ì˜ˆì¸¡

**ìš©ë„:**
- NLI (Natural Language Inference): ìì—°ì–´ ì¶”ë¡ 
- Paraphrase Detection: íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ íƒì§€
- Semantic Similarity: ì˜ë¯¸ ìœ ì‚¬ë„

### êµ¬ì¡°

```
ì…ë ¥: [CLS] A man is playing guitar [SEP] A person is making music [SEP]
        â†“
    [BERT Encoder]
        â†“
[CLS] í† í° ì¶”ì¶œ (ë‘ ë¬¸ì¥ì˜ ê´€ê³„ í‘œí˜„)
        â†“
[Linear: 768 â†’ 3]
        â†“
[Softmax]
        â†“
[0.85, 0.05, 0.10]
   â†‘
Entailment (í•¨ì˜)
```

### NLI (ìì—°ì–´ ì¶”ë¡ )

**3ê°€ì§€ ê´€ê³„:**

**1. Entailment (í•¨ì˜)**
```
ì „ì œ(A)ê°€ ì°¸ì´ë©´ ê°€ì„¤(B)ë„ ë°˜ë“œì‹œ ì°¸

A: "ëª¨ë“  ê³ ì–‘ì´ëŠ” ë™ë¬¼ì´ë‹¤"
B: "ë‚´ ê³ ì–‘ì´ëŠ” ë™ë¬¼ì´ë‹¤"
â†’ Entailment âœ“
```

**2. Contradiction (ëª¨ìˆœ)**
```
ì „ì œ(A)ê°€ ì°¸ì´ë©´ ê°€ì„¤(B)ëŠ” ê±°ì§“

A: "ê·¸ëŠ” ì§‘ì— ìˆë‹¤"
B: "ê·¸ëŠ” í•™êµì— ìˆë‹¤"
â†’ Contradiction âœ—
```

**3. Neutral (ì¤‘ë¦½)**
```
ì „ì œ(A)ë¡œ ê°€ì„¤(B)ì˜ ì°¸/ê±°ì§“ íŒë‹¨ ë¶ˆê°€

A: "ê·¸ëŠ” í•™ìƒì´ë‹¤"
B: "ê·¸ëŠ” í‚¤ê°€ í¬ë‹¤"
â†’ Neutral ?
```

### í•µì‹¬ í¬ì¸íŠ¸

**ì™œ [CLS] í† í°?**
- NSP ì‚¬ì „í•™ìŠµì—ì„œ [CLS]ê°€ ë¬¸ì¥ ìŒ ê´€ê³„ í•™ìŠµ
- ë‘ ë¬¸ì¥ì˜ ìƒí˜¸ì‘ìš©ì´ [CLS]ì— ì§‘ì•½ë¨

**[SEP] í† í°ì˜ ì—­í• :**
```
[CLS] ë¬¸ì¥A [SEP] ë¬¸ì¥B [SEP]
            â†‘
      ë‘ ë¬¸ì¥ êµ¬ë¶„ í‘œì‹œ
      Segment Embeddingë„ ë‹¤ë¦„
```

**ë‹¤ë¥¸ í™œìš©:**

**Paraphrase Detection (íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ):**
```
A: "He is a student"
B: "He studies at school"
â†’ Paraphrase (ê°™ì€ ì˜ë¯¸) or Not
```

**Semantic Similarity (ì˜ë¯¸ ìœ ì‚¬ë„):**
```
A: "Dog is running"
B: "Cat is sleeping"
â†’ Similarity Score: 0.3 (ë‚®ìŒ)

A: "Dog is running"
B: "Puppy is running"
â†’ Similarity Score: 0.9 (ë†’ìŒ)
```

### ê°„ë‹¨ ì½”ë“œ

```python
from transformers import BertForSequenceClassification

# ëª¨ë¸
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=3  # Entailment, Contradiction, Neutral
)

# ì˜ˆì¸¡
premise = "A man is playing guitar"
hypothesis = "A person is making music"
inputs = tokenizer(premise, hypothesis, return_tensors="pt")

outputs = model(**inputs)
prediction = torch.argmax(outputs.logits, dim=-1)
# 0: Entailment, 1: Contradiction, 2: Neutral
```

---

## ğŸ”§ Fine-tuning ì‹¤ì „ íŒ

### Learning Rate ì „ëµ

**ê¸°ë³¸ ì„¤ì •:**
```python
learning_rate = 2e-5  # ë˜ëŠ” 3e-5, 5e-5
# BERTëŠ” ì´ë¯¸ í•™ìŠµë¨ â†’ ì‘ê²Œ!
```

**Layer-wise Learning Rate Decay:**
```python
# ì•„ë˜ì¸µ(ì„ë² ë”©): ë” ì‘ì€ lr (ë²”ìš© ì§€ì‹ ë³´ì¡´)
# ìœ„ì¸µ(ì¶œë ¥): ë” í° lr (íƒœìŠ¤í¬ íŠ¹í™”)

optimizer = AdamW([
    {'params': model.bert.embeddings.parameters(), 'lr': 1e-5},
    {'params': model.bert.encoder.layer[:6].parameters(), 'lr': 2e-5},
    {'params': model.bert.encoder.layer[6:].parameters(), 'lr': 3e-5},
    {'params': model.classifier.parameters(), 'lr': 5e-5},
])
```

### Warmup

**ëª©ì :** ì´ˆê¸°ì— ì‘ì€ lrë¡œ ì‹œì‘ â†’ ì ì§„ì  ì¦ê°€

```python
# ì „ì²´ stepì˜ 10%ë¥¼ warmup
warmup_steps = total_steps * 0.1

# Scheduler
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)
```

### ë°ì´í„° í¬ê¸°ë³„ ê°€ì´ë“œ

| ë°ì´í„° í¬ê¸° | Epochs | Learning Rate | Batch Size |
|------------|--------|---------------|-----------|
| ì†Œê·œëª¨ (<5K) | 5-10 | 3e-5 ~ 5e-5 | 8-16 |
| ì¤‘ê·œëª¨ (5K-50K) | 3-5 | 2e-5 ~ 3e-5 | 16-32 |
| ëŒ€ê·œëª¨ (>50K) | 2-3 | 1e-5 ~ 2e-5 | 32-64 |

### Overfitting ë°©ì§€

```python
# 1. Early Stopping
# validation lossê°€ ì¦ê°€í•˜ë©´ ì¤‘ë‹¨

# 2. Dropout
model.config.hidden_dropout_prob = 0.1
model.config.attention_probs_dropout_prob = 0.1

# 3. Weight Decay
optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

# 4. Data Augmentation
# Back-translation, synonym replacement ë“±
```

---

## ğŸ“Š Taskë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### GLUE Benchmark (BERT-base)

| Task | Type | Metric | BERT Score |
|------|------|--------|-----------|
| **MNLI** | ë¬¸ì¥ ìŒ (NLI) | Accuracy | 84.6% |
| **QQP** | ë¬¸ì¥ ìŒ (Paraphrase) | Accuracy | 71.2% |
| **QNLI** | ë¬¸ì¥ ìŒ (QAâ†’NLI) | Accuracy | 90.5% |
| **SST-2** | ë¬¸ì¥ ë¶„ë¥˜ (ê°ì •) | Accuracy | 93.5% |
| **CoLA** | ë¬¸ì¥ ë¶„ë¥˜ (ë¬¸ë²•) | Matthews Corr | 52.1 |
| **STS-B** | ë¬¸ì¥ ìŒ (ìœ ì‚¬ë„) | Pearson Corr | 85.8 |
| **MRPC** | ë¬¸ì¥ ìŒ (Paraphrase) | F1 | 88.9% |
| **RTE** | ë¬¸ì¥ ìŒ (Entailment) | Accuracy | 66.4% |

### SQuAD (QA)

| Model | SQuAD 1.1 (EM/F1) | SQuAD 2.0 (EM/F1) |
|-------|------------------|------------------|
| **BERT-base** | 80.8 / 88.5 | 73.7 / 76.3 |
| **BERT-large** | 84.1 / 90.9 | 78.7 / 81.9 |
| Human | 82.3 / 91.2 | 86.8 / 89.5 |

---

## ğŸ¯ í•µì‹¬ ì •ë¦¬

### Task ì„ íƒ ê°€ì´ë“œ

```
ì…ë ¥ 1ê°œ ë¬¸ì¥ + í´ë˜ìŠ¤ ì˜ˆì¸¡
â†’ ë¬¸ì¥ ë¶„ë¥˜ (Sequence Classification)

ì…ë ¥ 1ê°œ ë¬¸ì¥ + ê° í† í° ë¼ë²¨
â†’ í† í° ë¶„ë¥˜ (Token Classification)

ì…ë ¥ ì§ˆë¬¸ + ë¬¸ì„œ â†’ ë‹µ ìœ„ì¹˜
â†’ QA (Question Answering)

ì…ë ¥ 2ê°œ ë¬¸ì¥ + ê´€ê³„ ì˜ˆì¸¡
â†’ ë¬¸ì¥ ìŒ ë¶„ë¥˜ (Sentence Pair Classification)
```

### [CLS] vs ëª¨ë“  í† í°

**[CLS] í† í°ë§Œ ì‚¬ìš©:**
- ë¬¸ì¥ ë¶„ë¥˜
- ë¬¸ì¥ ìŒ ë¶„ë¥˜
- ì´ìœ : ë¬¸ì¥ ì „ì²´ í‘œí˜„ í•„ìš”

**ëª¨ë“  í† í° ì‚¬ìš©:**
- í† í° ë¶„ë¥˜ (NER)
- QA (ìœ„ì¹˜ ì°¾ê¸°)
- ì´ìœ : í† í°ë³„ ì˜ˆì¸¡ í•„ìš”

### Fine-tuning ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
âœ… Pretrained BERT ë¡œë“œ
âœ… Task-specific Head ìë™ ì¶”ê°€ë¨
âœ… Learning rate: 2e-5 (ì‘ê²Œ!)
âœ… Epochs: 2-4 (ì§§ê²Œ!)
âœ… Warmup: 10%
âœ… Batch size: 16-32
âœ… Gradient clipping: 1.0
âœ… Weight decay: 0.01
âœ… Early stopping ì„¤ì •
```

### ì¼ë°˜ì ì¸ Fine-tuning ì½”ë“œ íŒ¨í„´

```python
from transformers import BertForXXX, Trainer, TrainingArguments

# 1. ëª¨ë¸ ë¡œë“œ (BERT + Task Head)
model = BertForXXX.from_pretrained(
    'bert-base-uncased',
    num_labels=num_labels
)

# 2. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=100,
    eval_steps=500,
    save_steps=500,
    load_best_model_at_end=True,
)

# 3. Trainerë¡œ í•™ìŠµ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

trainer.train()
```

---

## ğŸ“š ì°¸ê³ ìë£Œ

### ë…¼ë¬¸
- **BERT:** [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) (2018)
- **SQuAD:** [SQuAD: 100,000+ Questions for Machine Comprehension](https://arxiv.org/abs/1606.05250) (2016)
- **GLUE:** [GLUE: A Multi-Task Benchmark](https://arxiv.org/abs/1804.07461) (2018)

### ë²¤ì¹˜ë§ˆí¬
- **GLUE:** https://gluebenchmark.com/
- **SQuAD:** https://rajpurkar.github.io/SQuAD-explorer/
- **SuperGLUE:** https://super.gluebenchmark.com/

### ì‹¤ìŠµ ìë£Œ
- **Hugging Face Course:** https://huggingface.co/course
- **BERT Fine-tuning Tutorial:** https://mccormickml.com/2019/07/22/BERT-fine-tuning/

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-18  

> "BERT Fine-tuningì€ ê°„ë‹¨í•˜ë‹¤: Pretrained BERT + Task Head 1-2ê°œ + ì‘ì€ lrë¡œ ì§§ê²Œ í•™ìŠµ. ì´ê²ƒì´ ì „ë¶€ë‹¤."