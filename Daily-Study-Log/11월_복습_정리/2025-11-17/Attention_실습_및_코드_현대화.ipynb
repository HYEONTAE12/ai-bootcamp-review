{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8e2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30f546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        data = [(input_ids, target_ids), ...]\n",
    "        input_ids, target_ids = LongTensor (seq_len,)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e75080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch, x_pad_idx=0, y_pad_idx=0, device=\"cpu\"):\n",
    "    # batch = [(x, y), (x, y), ...]\n",
    "    x_seqs, y_seqs = zip(*batch)\n",
    "\n",
    "    # ① 각 문장 길이 저장\n",
    "    input_lens  = [len(seq) for seq in x_seqs]\n",
    "    target_lens = [len(seq) for seq in y_seqs]\n",
    "    \n",
    "    # ② pad_sequence 적용 (batch_first=True)\n",
    "    x_padded = pad_sequence(\n",
    "        x_seqs,\n",
    "        batch_first=True,\n",
    "        padding_value=x_pad_idx\n",
    "    ).to(device)\n",
    "\n",
    "    y_padded = pad_sequence(\n",
    "        y_seqs,\n",
    "        batch_first=True,\n",
    "        padding_value=y_pad_idx\n",
    "    ).to(device)\n",
    "\n",
    "    return x_padded, y_padded, input_lens, target_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a864ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(train_data, batch_size, device=\"cpu\", x_pad_idx=0, y_pad_idx=0):\n",
    "    dataset = Seq2SeqDataset(train_data)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: collate_fn(\n",
    "            batch,\n",
    "            x_pad_idx=x_pad_idx,\n",
    "            y_pad_idx=y_pad_idx,\n",
    "            device=device\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ad1eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(train_data, batch_size, device, x_pad_idx, y_pad_idx)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_dataloader\u001b[39m(train_data, batch_size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_pad_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, y_pad_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m      2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m Seq2SeqDataset(train_data)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx_pad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_pad_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43my_pad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pad_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\NLP_Test\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\NLP_Test\\lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = ''\n",
    "train_loader = get_dataloader(train_data, batch_size=32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83adcc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "RE_PUNCT = re.compile(r\"([,.!?])\")\n",
    "RE_NON_ASCII = re.compile(r\"[^a-zA-Z,.!?]+\")\n",
    "RE_MULTI_SPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def unicode_to_ascii_new(s:str) -> str:\n",
    "    normalized = unicodedata.normalize(\"NFD\", s)\n",
    "    return \"\".join(c for c in normalized if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def normalize_text_new(text: str) -> str:\n",
    "    text = unicode_to_ascii_new(text.lower().strip())\n",
    "    text = RE_PUNCT.sub(r\" \\1\", text)\n",
    "    text = RE_NON_ASCII.sub(r\" \", text)\n",
    "    text = RE_MULTI_SPACE.sub(r\" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31bb2c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30000\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "def load_parallel_corpus(path: str, limit: int = 30000):\n",
    "    corpus = []\n",
    "    with io.open(path, \"r\", encoding=\"utf-8\")as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= limit:\n",
    "                break\n",
    "            corpus.append(line.strip())\n",
    "    return corpus\n",
    "\n",
    "corpus = load_parallel_corpus(\"NLP/fra.txt\", limit=30000)\n",
    "print(\"Loaded\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29427 29427\n",
      "['i', 'see', '.'] ['je', 'comprends', '.']\n"
     ]
    }
   ],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 25\n",
    "\n",
    "def is_valid_length(tokens, min_len=3, max_len=25):\n",
    "    return min_len <= len(tokens) <= max_len\n",
    "\n",
    "X_r = []\n",
    "y_r = []\n",
    "\n",
    "for line in corpus:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    # 탭으로 안전하게 분리 (3개 이하이면 skip)\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    \n",
    "    src, tgt = parts[0], parts[1]\n",
    "\n",
    "    # 정규화 + 토큰화\n",
    "    src_tokens = normalize_text_new(src).split()\n",
    "    tgt_tokens = normalize_text_new(tgt).split()\n",
    "\n",
    "    # 길이 필터 적용\n",
    "    if is_valid_length(src_tokens, MIN_LENGTH, MAX_LENGTH) and \\\n",
    "       is_valid_length(tgt_tokens, MIN_LENGTH, MAX_LENGTH):\n",
    "        X_r.append(src_tokens)\n",
    "        y_r.append(tgt_tokens)\n",
    "\n",
    "print(len(X_r), len(y_r))\n",
    "print(X_r[0], y_r[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e9b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(token_lists, min_freq=1, specials=None):\n",
    "    counter = Counter()\n",
    "    for tokens in token_lists:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    \n",
    "    vocab= []\n",
    "    for token, freq in counter.most_common():\n",
    "        if freq >= min_freq:\n",
    "            vocab.append(token)\n",
    "\n",
    "    \n",
    "    if specials is None:\n",
    "        specials = [\"<PAD>\", \"UNK\", \"<s>\", \"</s>\"]\n",
    "    vocab = specials + vocab\n",
    "\n",
    "    stoi = {word: idx for idx, word in enumerate(vocab)}\n",
    "    itos = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "    return vocab, stoi, itos   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddcb40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab, source2index, index2source = build_vocab(\n",
    "    X_r,\n",
    "    min_freq=1,\n",
    "    specials=[\"<PAD>\", \"UNK\", \"<s>\", \"</s>\"]\n",
    "    )\n",
    "\n",
    "target_vocab, target2index, index2target = build_vocab(\n",
    "    y_r,\n",
    "    min_freq=1,\n",
    "    specials=[\"<PAD>\", \"UNK\", \"<s>\", \"</s>\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, vocab, unk_token='<UNK>'):\n",
    "    return torch.tensor(\n",
    "        [vocab[token] if token in vocab else vocab[unk_token]\n",
    "         for token in tokens],\n",
    "         dtype=torch.long\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_example(src_tokens, tgt_tokens, src_vocab, tgt_vocab):\n",
    "    src_ids = tokens_to_indices(src_tokens + [\"</s>\"], src_vocab)\n",
    "    tgt_ids = tokens_to_indices(tgt_tokens + [\"</s>\"], tgt_vocab)\n",
    "    return src_ids, tgt_ids\n",
    "\n",
    "# modern way: 전체 데이터 전처리\n",
    "dataset = [prepare_example(so, ta, source2index, target2index) for so, ta in zip(X_r, y_r)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87cf202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "dataloader = get_dataloader(\n",
    "    train_data=dataset,   # 네가 만든 dataset 리스트\n",
    "    batch_size=32,\n",
    "    device=device,\n",
    "    x_pad_idx=source2index[\"<PAD>\"],\n",
    "    y_pad_idx=target2index[\"<PAD>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927909af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,       # vocab size\n",
    "        embedding_size: int,\n",
    "        hidden_size: int,\n",
    "        n_layers: int = 1,\n",
    "        bidirectional: bool = False,\n",
    "        dropout: float = 0.0,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Xavier 초기화\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if \"weight_ih\" in name or \"weight_hh\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def init_hidden(self, batch_size: int):\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return torch.zeros(\n",
    "            self.n_layers * self.n_directions,\n",
    "            batch_size,\n",
    "            self.hidden_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"\n",
    "        inputs: (batch, seq_len)  - token indices\n",
    "        input_lengths: list[int]  - 각 시퀀스 실제 길이\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        embedded = self.embedding(inputs)  # (batch, seq_len, embed_dim)\n",
    "\n",
    "        # 길이 기준으로 pack (정렬 안 되어 있어도 enforce_sorted=False로 처리)\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded,\n",
    "            input_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        # outputs: (batch, seq_len, hidden_size * n_directions)\n",
    "\n",
    "        # hidden: (num_layers * n_directions, batch, hidden_size)\n",
    "        # → 마지막 레이어만 사용\n",
    "        if self.bidirectional:\n",
    "            # (num_layers, n_directions, batch, hidden)\n",
    "            hidden = hidden.view(self.n_layers, self.n_directions, batch_size, self.hidden_size)\n",
    "            # 마지막 레이어: (n_directions, batch, hidden)\n",
    "            last_layer_hidden = hidden[-1]  # (2, batch, hidden)\n",
    "            # forward/backward concat: (batch, hidden*2)\n",
    "            last_layer_hidden = torch.cat(\n",
    "                [last_layer_hidden[0], last_layer_hidden[1]], dim=-1\n",
    "            )\n",
    "            # 디코더 초기 hidden으로 쓰기 쉽게 (1, batch, hidden*2) 형태로 리턴\n",
    "            dec_init_hidden = last_layer_hidden.unsqueeze(0)\n",
    "        else:\n",
    "            # unidirectional: hidden shape (num_layers, batch, hidden)\n",
    "            hidden = hidden.view(self.n_layers, batch_size, self.hidden_size)\n",
    "            last_layer_hidden = hidden[-1]  # (batch, hidden)\n",
    "            dec_init_hidden = last_layer_hidden.unsqueeze(0)  # (1, batch, hidden)\n",
    "\n",
    "        # outputs는 attention 쓸 때 전체 타임스텝마다 필요\n",
    "        return outputs, dec_init_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b2d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 1024])\n",
      "torch.Size([1, 32, 1024])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_size=len(source2index),\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    n_layers=3,\n",
    "    bidirectional=True,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "for x_batch, y_batch, x_len, y_len in dataloader:\n",
    "    enc_outputs, enc_hidden = encoder(x_batch, x_len)\n",
    "    print(enc_outputs.shape)  # (batch, seq_len, hidden*2)\n",
    "    print(enc_hidden.shape)   # (1, batch, hidden*2)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543100fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, device=\"cpu\", dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Luong-style attention: score(h_t, h_s) = h_s^T W h_t\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, prev_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        input_token: (B,) 현재 step에서 넣을 토큰 인덱스\n",
    "        prev_hidden: (1,B,H) 이전 디코더 hidden (혹은 encoder에서 온 hidden)\n",
    "        encoder_outputs: (B,T,H) 인코더 전체 출력\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_token).unsqueeze(1)  # (B,1,E)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, hidden = self.gru(embedded, prev_hidden)     # output: (B,1,H)\n",
    "\n",
    "        # Attention\n",
    "        # 1) encoder_outputs -> attn → (B,T,H)\n",
    "        energies = self.attn(encoder_outputs)                # (B,T,H)\n",
    "        # 2) energies · output^T\n",
    "        attn_scores = torch.bmm(energies, output.transpose(1,2)).squeeze(2)  # (B,T)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1).unsqueeze(1)            # (B,1,T)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)                   # (B,1,H)\n",
    "\n",
    "        # 3) [output, context] concat 후 vocab score\n",
    "        concat = torch.cat([output, context], dim=-1)        # (B,1,2H)\n",
    "        logits = self.out(concat.squeeze(1))                 # (B,vocab)\n",
    "\n",
    "        return logits, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009120d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 1) Encoder\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m enc_outputs, enc_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# enc_outputs: (B, src_T, H_enc)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# enc_hidden:  (1, B, H_dec_init)  # 이미 concat 된 상태라고 가정\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 2) Decoder 입력/타겟 준비\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#   y_batch = [<s>, y1, y2, ..., yN, </s>]\u001b[39;00m\n\u001b[0;32m     59\u001b[0m trg_input  \u001b[38;5;241m=\u001b[39m y_batch[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# B, T-1  (디코더 입력)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\NLP_Test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\NLP_Test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 72\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, inputs, input_lengths)\u001b[0m\n\u001b[0;32m     63\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(inputs)\n\u001b[0;32m     65\u001b[0m packed \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\n\u001b[0;32m     66\u001b[0m     embedded,\n\u001b[0;32m     67\u001b[0m     input_lengths,\n\u001b[0;32m     68\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     69\u001b[0m     enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(outputs, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional:\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\NLP_Test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\NLP_Test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\USER\\.conda\\envs\\NLP_Test\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1404\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1394\u001b[0m         hx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m   1402\u001b[0m     )\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1416\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 1e-3\n",
    "DECODER_LR_RATIO = 5.0\n",
    "TEACHER_FORCING_RATIO = 0.5  # 예: 절반은 정답, 절반은 모델 예측을 입력으로 사용\n",
    "\n",
    "# 1) 모델 생성\n",
    "encoder = Encoder(\n",
    "    input_size = len(source2index),\n",
    "    embedding_size= EMBEDDING_SIZE,\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    n_layers=3,\n",
    "    bidirectional=True,\n",
    "    device=device\n",
    "    ).to(device)\n",
    "\n",
    "decoder = AttnDecoder(\n",
    "    input_size = len(target2index),\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size = HIDDEN_SIZE * 2,\n",
    "    device= device\n",
    ").to(device)\n",
    "\n",
    "# 2) 손실 함수 & 옵티마이저\n",
    "PAD_IDX = target2index[\"<PAD>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=LR)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=LR * DECODER_LR_RATIO)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for x_batch, y_batch, x_len, y_len in dataloader:\n",
    "        # x_batch: (B, src_T)\n",
    "        # y_batch: (B, tgt_T)\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # 1) Encoder\n",
    "        enc_outputs, enc_hidden = encoder(x_batch, x_len)\n",
    "        # enc_outputs: (B, src_T, H_enc)\n",
    "        # enc_hidden:  (1, B, H_dec_init)  # 이미 concat 된 상태라고 가정\n",
    "\n",
    "        # 2) Decoder 입력/타겟 준비\n",
    "        #   y_batch = [<s>, y1, y2, ..., yN, </s>]\n",
    "        trg_input  = y_batch[:, :-1]  # B, T-1  (디코더 입력)\n",
    "        trg_target = y_batch[:, 1:]   # B, T-1  (정답 라벨)\n",
    "\n",
    "        batch_size, max_len = trg_input.size()\n",
    "        vocab_size = len(target2index)\n",
    "\n",
    "        # 디코더 출력 저장 텐서\n",
    "        all_logits = torch.zeros(batch_size, max_len, vocab_size, device=device)\n",
    "\n",
    "        # 초기 디코더 hidden = encoder에서 받은 hidden\n",
    "        dec_hidden = enc_hidden  # (1,B,H_dec)\n",
    "\n",
    "        # 첫 입력 토큰: <s>\n",
    "        input_token = trg_input[:, 0]  # B,\n",
    "\n",
    "        for t in range(max_len):\n",
    "            # 3) 현재 step 디코더 한 번 실행\n",
    "            logits, dec_hidden, attn_weights = decoder(\n",
    "                input_token,      # B,\n",
    "                dec_hidden,       # (1,B,H)\n",
    "                enc_outputs       # (B,src_T,H)\n",
    "            )\n",
    "            # logits: (B,vocab_size)\n",
    "\n",
    "            all_logits[:, t, :] = logits\n",
    "\n",
    "            # 4) teacher forcing 적용 여부\n",
    "            use_teacher = (torch.rand(1).item() < TEACHER_FORCING_RATIO)\n",
    "\n",
    "            if t+1 < max_len:  # 마지막 step 이후에는 필요 없음\n",
    "                if use_teacher:\n",
    "                    # 정답을 다음 step 입력으로\n",
    "                    input_token = trg_input[:, t+1]\n",
    "                else:\n",
    "                    # 모델이 방금 낸 예측을 다음 입력으로\n",
    "                    input_token = logits.argmax(dim=-1)\n",
    "\n",
    "        # 5) Loss 계산\n",
    "        # all_logits: (B, T-1, V) → (B*(T-1), V)\n",
    "        # trg_target: (B, T-1)    → (B*(T-1))\n",
    "        loss = criterion(\n",
    "            all_logits.view(batch_size * max_len, vocab_size),\n",
    "            trg_target.reshape(-1)\n",
    "        )\n",
    "\n",
    "        # 6) 역전파 + 업데이트\n",
    "        enc_optimizer.zero_grad()\n",
    "        dec_optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), 50.0)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 50.0)\n",
    "\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f\"[{epoch+1}/{EPOCHS}] mean_loss: {sum(epoch_losses)/len(epoch_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0012b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb3d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
