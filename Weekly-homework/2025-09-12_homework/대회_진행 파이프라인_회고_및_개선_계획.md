
---

# 대회 진행 파이프라인 회고 및 개선 계획

## 1. 원본 데이터 성능 평가

* **현재**: Hold-out 방식으로 기본 성능 평가.
* **깨달음**: 원본 데이터 그대로 baseline을 잡는 것은 중요하다. 이 baseline이 있어야 후속 개선의 성과를 비교할 수 있다.
* **추가 조언**: baseline 모델은 단순 모델(Linear Regression, Decision Tree, LightGBM 기본 설정 등)로 두세 개 만들어두면 더 탄탄한 기준선이 된다.

---

## 2. 목표 설정

* **현재**: 리더보드 점수 vs 인사이트 도출 사이에서 균형 고민.
* **깨달음**: 목표가 “성능 최적화”인지, “학습과 인사이트”인지 명확히 해야 이후 전략이 흔들리지 않는다.
* **추가 조언**: 대회 초반에는 **학습 목표**(EDA, 피처링, 모델 탐색)를 두고, 후반부에는 **점수 목표**(리더보드 최적화)로 전환하는 것도 좋은 전략이다.

---

## 3. 데이터 분석 (EDA)

* **현재**: 연도별 데이터 분포 확인 → 22\~23년도 데이터가 매우 적어 모델이 오래된 데이터 쪽에 치우친다는 점을 발견.
* **깨달음**: 맞추려는 문제(test set 시점)와 훈련 데이터 분포가 다르면 모델이 일반화에 실패할 수 있다.
* **추가 조언**:

  * 시계열적 데이터에서는 반드시 **train/test 시점 비교**를 하고, 불균형이 심할 경우 **가중치 부여**나 **최근 데이터 oversampling** 고려.
  * 단순 통계뿐 아니라 **타겟 분포 shift**도 확인하면 좋다. (예: 최근 집값이 과거보다 분포 자체가 다름)

---

## 4. 파생변수 생성 (Feature Engineering)

* **현재**: 한 번에 여러 파생변수를 추가 → 어떤 피처가 성능 개선에 기여했는지 추적 어려움.
* **깨달음**: 체계적인 버전 관리, 단계별 실험 기록이 필요.
* **개선 방향**: WandB/노션/CSV 등으로 실험별 버전을 남기고, “하나의 파생변수 → 성능 비교” 단계를 지켜야 한다.
* **추가 조언**:

  * Feature importance와 permutation importance를 비교해서, 실제 기여도를 추적.
  * domain knowledge 활용(부동산 → 금리, GDP, 거래량 lag 변수 등)도 체계적으로 기록.

---

## 5. 기록 관리

* **현재**: 기록을 소홀히 함 → 만든 피처와 이유를 잊어버림.
* **깨달음**: 기록 부재는 인사이트를 휘발시킨다.
* **개선 방향**: WandB, GitHub, Notion에 실험 기록을 남기기.
* **추가 조언**:

  * 최소한 **버전 번호 + 변경 사항 + 성능 지표**를 남기는 간단한 실험 로그 템플릿을 쓰면 된다.

---

## 6. 데이터 분할 전략

* **현재**: Hold-out, K-Fold, Stratified K-Fold, TimeSeriesSplit 비교.
* **깨달음**: 문제 특성(시계열 예측)과 맞지 않은 분할은 심각한 성능 저하를 유발.
* **추가 조언**:

  * 시계열 대회에서는 **GroupKFold (지역 단위)** + **TimeSeriesSplit**을 조합하는 것도 고려.
  * 데이터가 많으면 validation set을 여러 기간으로 나누어 **rolling validation**을 해보는 것도 일반화에 유리.

---

## 7. 모델 선택

* **현재**: RandomForest, LinearRegression은 사용했지만 CatBoost, XGBoost는 못해봄.
* **깨달음**: 다양한 모델 시도가 필요.
* **추가 조언**:

  * LightGBM, CatBoost, XGBoost는 필수.
  * 단순 모델도 baseline으로 계속 유지해야, 앙상블/스태킹에서 활용 가능.
  * AutoML 툴(H2O, FLAML, Optuna integration)로 빠르게 후보 탐색도 해볼 수 있다.

---

## 8. 모델 성능 평가

* **현재**: Hyperparameter tuning, feature importance 확인.
* **깨달음**: Importance가 크다고 항상 좋은 것은 아니다 → 과적합 가능성도 존재.
* **개선 방향**: 규제(regularization)와 feature selection 기법 활용.
* **추가 조언**:

  * SHAP, Permutation Importance 같이 **모델 불가지론적(feature-agnostic)** 방법도 같이 확인.
  * Hyperparameter는 Optuna, Hyperopt 같은 Bayesian optimization 툴을 활용.

---

## 9. 시각화

* **현재**: 데이터와 결과 해석에 시각화가 중요하다는 점 인식.
* **깨달음**: 시각화는 인사이트를 전달하는 강력한 도구.
* **추가 조언**:

  * 단순 분포/상관관계 외에도 \*\*Residual plot, Feature contribution plot(SHAP)\*\*까지 그리면 더 깊은 분석 가능.
  * 발표/보고서용 그래프는 색상·레이블 정리까지 신경쓰기.

---

# 전체 정리

* **보완하면 좋은 부분**:

  * baseline을 여러 모델로 잡아두기
  * validation 방식 다양화 (특히 시계열 rolling)
  * feature importance 해석 시 SHAP 같은 방법 병행
  * 기록 관리를 더 체계적으로 (버전/변경사항/성능 로그화)

---
